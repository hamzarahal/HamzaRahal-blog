[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Survival Analysis using R\n\n\n\n\n\n\nproject\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 5, 2024\n\n\nHamza Rahal\n\n\n\n\n\n\n\n\n\n\n\n\nDr. Semmelweis and the Discovery of Handwashing\n\n\n\n\n\n\nproject\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 9, 2024\n\n\nHamza Rahal\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Inequalities in Life Expectancy\n\n\n\n\n\n\nproject\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 4, 2024\n\n\nHamza Rahal\n\n\n\n\n\n\n\n\n\n\n\n\nHealth Survey Data Analysis of BMI\n\n\n\n\n\n\nproject\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 25, 2024\n\n\nHamza Rahal\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Your Heart Rate Is Telling You\n\n\n\n\n\n\nproject\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 12, 2024\n\n\nHamza Rahal\n\n\n\n\n\n\n\n\n\n\n\n\nClustering Heart Disease Patient Data\n\n\n\n\n\n\nproject\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 6, 2024\n\n\nHamza Rahal\n\n\n\n\n\n\n\n\n\n\n\n\nClassify Suspected Infection in Patients\n\n\n\n\n\n\nproject\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 5, 2024\n\n\nHamza Rahal\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/Project2/index.html",
    "href": "projects/Project2/index.html",
    "title": "Dr. Semmelweis and the Discovery of Handwashing",
    "section": "",
    "text": "This is Dr. Ignaz Semmelweis, a Hungarian physician born in 1818 and active at the Vienna General Hospital. If Dr. Semmelweis looks troubled it’s probably because he’s thinking about childbed fever: A deadly disease affecting women that just have given birth. He is thinking about it because in the early 1840s at the Vienna General Hospital as many as 10% of the women giving birth die from it. He is thinking about it because he knows the cause of childbed fever: It’s the contaminated hands of the doctors delivering the babies. And they won’t listen to him and wash their hands!\nIn this notebook, we’re going to reanalyze the data that made Semmelweis discover the importance of handwashing. Let’s start by looking at the data that made Semmelweis realize that something was wrong with the procedures at Vienna General Hospital.\n\n\nCode\n# Load in the tidyverse package\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\n# Read datasets/yearly_deaths_by_clinic.csv into yearly\nyearly &lt;- read_csv(\"yearly_deaths_by_clinic.csv\")\n\n\nRows: 12 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): clinic\ndbl (3): year, births, deaths\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n# Print out yearly\nyearly\n\n\n# A tibble: 12 × 4\n    year births deaths clinic  \n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n 1  1841   3036    237 clinic 1\n 2  1842   3287    518 clinic 1\n 3  1843   3060    274 clinic 1\n 4  1844   3157    260 clinic 1\n 5  1845   3492    241 clinic 1\n 6  1846   4010    459 clinic 1\n 7  1841   2442     86 clinic 2\n 8  1842   2659    202 clinic 2\n 9  1843   2739    164 clinic 2\n10  1844   2956     68 clinic 2\n11  1845   3241     66 clinic 2\n12  1846   3754    105 clinic 2"
  },
  {
    "objectID": "projects/Project2/index.html#meet-dr.-ignaz-semmelweis",
    "href": "projects/Project2/index.html#meet-dr.-ignaz-semmelweis",
    "title": "Dr. Semmelweis and the Discovery of Handwashing",
    "section": "",
    "text": "This is Dr. Ignaz Semmelweis, a Hungarian physician born in 1818 and active at the Vienna General Hospital. If Dr. Semmelweis looks troubled it’s probably because he’s thinking about childbed fever: A deadly disease affecting women that just have given birth. He is thinking about it because in the early 1840s at the Vienna General Hospital as many as 10% of the women giving birth die from it. He is thinking about it because he knows the cause of childbed fever: It’s the contaminated hands of the doctors delivering the babies. And they won’t listen to him and wash their hands!\nIn this notebook, we’re going to reanalyze the data that made Semmelweis discover the importance of handwashing. Let’s start by looking at the data that made Semmelweis realize that something was wrong with the procedures at Vienna General Hospital.\n\n\nCode\n# Load in the tidyverse package\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\n# Read datasets/yearly_deaths_by_clinic.csv into yearly\nyearly &lt;- read_csv(\"yearly_deaths_by_clinic.csv\")\n\n\nRows: 12 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): clinic\ndbl (3): year, births, deaths\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n# Print out yearly\nyearly\n\n\n# A tibble: 12 × 4\n    year births deaths clinic  \n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n 1  1841   3036    237 clinic 1\n 2  1842   3287    518 clinic 1\n 3  1843   3060    274 clinic 1\n 4  1844   3157    260 clinic 1\n 5  1845   3492    241 clinic 1\n 6  1846   4010    459 clinic 1\n 7  1841   2442     86 clinic 2\n 8  1842   2659    202 clinic 2\n 9  1843   2739    164 clinic 2\n10  1844   2956     68 clinic 2\n11  1845   3241     66 clinic 2\n12  1846   3754    105 clinic 2"
  },
  {
    "objectID": "projects/Project2/index.html#the-alarming-number-of-deaths",
    "href": "projects/Project2/index.html#the-alarming-number-of-deaths",
    "title": "Dr. Semmelweis and the Discovery of Handwashing",
    "section": "2. The alarming number of deaths",
    "text": "2. The alarming number of deaths\nThe table above shows the number of women giving birth at the two clinics at the Vienna General Hospital for the years 1841 to 1846. You’ll notice that giving birth was very dangerous; an alarming number of women died as the result of childbirth, most of them from childbed fever.\nWe see this more clearly if we look at the proportion of deaths out of the number of women giving birth.\n\n\nCode\n# Adding a new column to yearly with proportion of deaths per no. births\nyearly &lt;- yearly  %&gt;% mutate(proportion_deaths = deaths/births)\n\n# Print out yearly\nyearly\n\n\n# A tibble: 12 × 5\n    year births deaths clinic   proportion_deaths\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;\n 1  1841   3036    237 clinic 1            0.0781\n 2  1842   3287    518 clinic 1            0.158 \n 3  1843   3060    274 clinic 1            0.0895\n 4  1844   3157    260 clinic 1            0.0824\n 5  1845   3492    241 clinic 1            0.0690\n 6  1846   4010    459 clinic 1            0.114 \n 7  1841   2442     86 clinic 2            0.0352\n 8  1842   2659    202 clinic 2            0.0760\n 9  1843   2739    164 clinic 2            0.0599\n10  1844   2956     68 clinic 2            0.0230\n11  1845   3241     66 clinic 2            0.0204\n12  1846   3754    105 clinic 2            0.0280"
  },
  {
    "objectID": "projects/Project2/index.html#death-at-the-clinics",
    "href": "projects/Project2/index.html#death-at-the-clinics",
    "title": "Dr. Semmelweis and the Discovery of Handwashing",
    "section": "3. Death at the clinics",
    "text": "3. Death at the clinics\nIf we now plot the proportion of deaths at both clinic 1 and clinic 2 we’ll see a curious pattern…\n\n\nCode\n# Setting the size of plots in this notebook\noptions(repr.plot.width = 7, repr.plot.height = 4)\n\n# Plot yearly proportion of deaths at the two clinics\nggplot(yearly, aes(x = year, y = proportion_deaths, col = clinic)) + \n  geom_line()"
  },
  {
    "objectID": "projects/Project2/index.html#the-handwashing-begins",
    "href": "projects/Project2/index.html#the-handwashing-begins",
    "title": "Dr. Semmelweis and the Discovery of Handwashing",
    "section": "4. The handwashing begins",
    "text": "4. The handwashing begins\nWhy is the proportion of deaths constantly so much higher in Clinic 1? Semmelweis saw the same pattern and was puzzled and distressed. The only difference between the clinics was that many medical students served at Clinic 1, while mostly midwife students served at Clinic 2. While the midwives only tended to the women giving birth, the medical students also spent time in the autopsy rooms examining corpses.\nSemmelweis started to suspect that something on the corpses, spread from the hands of the medical students, caused childbed fever. So in a desperate attempt to stop the high mortality rates, he decreed: Wash your hands! This was an unorthodox and controversial request, nobody in Vienna knew about bacteria at this point in time.\nLet’s load in monthly data from Clinic 1 to see if the handwashing had any effect.\n\n\nCode\n# Read datasets/monthly_deaths.csv into monthly\nmonthly &lt;- read_csv(\"monthly_deaths.csv\")\n\n\nRows: 98 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (2): births, deaths\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n# Adding a new column with proportion of deaths per no. births\nmonthly &lt;- monthly  %&gt;% mutate(proportion_deaths = deaths/births)\n\n# Print out the first rows in monthly\nhead(monthly)\n\n\n# A tibble: 6 × 4\n  date       births deaths proportion_deaths\n  &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt;             &lt;dbl&gt;\n1 1841-01-01    254     37           0.146  \n2 1841-02-01    239     18           0.0753 \n3 1841-03-01    277     12           0.0433 \n4 1841-04-01    255      4           0.0157 \n5 1841-05-01    255      2           0.00784\n6 1841-06-01    200     10           0.05"
  },
  {
    "objectID": "projects/Project2/index.html#the-effect-of-handwashing",
    "href": "projects/Project2/index.html#the-effect-of-handwashing",
    "title": "Dr. Semmelweis and the Discovery of Handwashing",
    "section": "5. The effect of handwashing",
    "text": "5. The effect of handwashing\nWith the data loaded we can now look at the proportion of deaths over time. In the plot below we haven’t marked where obligatory handwashing started, but it reduced the proportion of deaths to such a degree that you should be able to spot it!\n\n\nCode\nggplot(monthly, aes(date, proportion_deaths)) +\n  geom_line() +\n  labs(x = \"Year\", y = \"Proportion Deaths\")"
  },
  {
    "objectID": "projects/Project2/index.html#the-effect-of-handwashing-highlighted",
    "href": "projects/Project2/index.html#the-effect-of-handwashing-highlighted",
    "title": "Dr. Semmelweis and the Discovery of Handwashing",
    "section": "6. The effect of handwashing highlighted",
    "text": "6. The effect of handwashing highlighted\nStarting from the summer of 1847 the proportion of deaths is drastically reduced and, yes, this was when Semmelweis made handwashing obligatory.\nThe effect of handwashing is made even more clear if we highlight this in the graph.\n\n\nCode\n# From this date handwashing was made mandatory\nhandwashing_start = as.Date('1847-06-01')\n\n# Add a TRUE/FALSE to monthly called handwashing_started\nmonthly &lt;- monthly %&gt;%\n  mutate(handwashing_started = date &gt;= handwashing_start)\n\n# Plot monthly proportion of deaths before and after handwashing\nggplot(monthly, aes(x = date, y = proportion_deaths, color = handwashing_started)) +\n  geom_line()"
  },
  {
    "objectID": "projects/Project2/index.html#more-handwashing-fewer-deaths",
    "href": "projects/Project2/index.html#more-handwashing-fewer-deaths",
    "title": "Dr. Semmelweis and the Discovery of Handwashing",
    "section": "7. More handwashing, fewer deaths?",
    "text": "7. More handwashing, fewer deaths?\nAgain, the graph shows that handwashing had a huge effect. How much did it reduce the monthly proportion of deaths on average?\n\n\nCode\n# Calculating the mean proportion of deaths \n# before and after handwashing.\n\nmonthly_summary &lt;- monthly %&gt;% \n  group_by(handwashing_started) %&gt;%\n  summarise(mean_proportion_deaths = mean(proportion_deaths))\n\n# Printing out the summary.\nmonthly_summary\n\n\n# A tibble: 2 × 2\n  handwashing_started mean_proportion_deaths\n  &lt;lgl&gt;                                &lt;dbl&gt;\n1 FALSE                               0.105 \n2 TRUE                                0.0211"
  },
  {
    "objectID": "projects/Project2/index.html#a-statistical-analysis-of-semmelweis-handwashing-data",
    "href": "projects/Project2/index.html#a-statistical-analysis-of-semmelweis-handwashing-data",
    "title": "Dr. Semmelweis and the Discovery of Handwashing",
    "section": "8. A statistical analysis of Semmelweis handwashing data",
    "text": "8. A statistical analysis of Semmelweis handwashing data\nIt reduced the proportion of deaths by around 8 percentage points! From 10% on average before handwashing to just 2% when handwashing was enforced (which is still a high number by modern standards). To get a feeling for the uncertainty around how much handwashing reduces mortalities we could look at a confidence interval (here calculated using a t-test).\n\n\nCode\n# Calculating a 95% Confidence intrerval using t.test \ntest_result &lt;- t.test( proportion_deaths ~ handwashing_started, data = monthly)\ntest_result\n\n\n\n    Welch Two Sample t-test\n\ndata:  proportion_deaths by handwashing_started\nt = 9.6101, df = 92.435, p-value = 1.445e-15\nalternative hypothesis: true difference in means between group FALSE and group TRUE is not equal to 0\n95 percent confidence interval:\n 0.06660662 0.10130659\nsample estimates:\nmean in group FALSE  mean in group TRUE \n         0.10504998          0.02109338"
  },
  {
    "objectID": "projects/Project2/index.html#the-fate-of-dr.-semmelweis",
    "href": "projects/Project2/index.html#the-fate-of-dr.-semmelweis",
    "title": "Dr. Semmelweis and the Discovery of Handwashing",
    "section": "9. The fate of Dr. Semmelweis",
    "text": "9. The fate of Dr. Semmelweis\nThat the doctors didn’t wash their hands increased the proportion of deaths by between 6.7 and 10 percentage points, according to a 95% confidence interval. All in all, it would seem that Semmelweis had solid evidence that handwashing was a simple but highly effective procedure that could save many lives.\nThe tragedy is that, despite the evidence, Semmelweis’ theory — that childbed fever was caused by some “substance” (what we today know as bacteria) from autopsy room corpses — was ridiculed by contemporary scientists. The medical community largely rejected his discovery and in 1849 he was forced to leave the Vienna General Hospital for good.\nOne reason for this was that statistics and statistical arguments were uncommon in medical science in the 1800s. Semmelweis only published his data as long tables of raw data, but he didn’t show any graphs nor confidence intervals. If he would have had access to the analysis we’ve just put together he might have been more successful in getting the Viennese doctors to wash their hands.\n\n\nCode\n# The data Semmelweis collected points to that:\ndoctors_should_wash_their_hands &lt;- TRUE"
  },
  {
    "objectID": "posts/post3/index.html",
    "href": "posts/post3/index.html",
    "title": "R and SAS: A Synergistic Approach for Clinical Programming",
    "section": "",
    "text": "In the evolving landscape of clinical programming, the debate between R and SAS continues to spark discussions among statistical programmers. However, instead of viewing these powerful tools as competitors, we should recognize the potential for them to complement each other in clinical research. Here’s why R and SAS should coexist and how statistical programmers can harness the strengths of both to enhance their work.\n1. R and SAS: Strengths and Synergies\n\nSAS: The Industry Standard\nSAS has long been the gold standard in clinical trials and regulatory submissions, with its robust data management capabilities, validated procedures, and a comprehensive suite of statistical tools. Its strong compliance with regulatory requirements makes it indispensable in the pharmaceutical industry.\nR: The Rising Star\nR, with its open-source nature, provides unparalleled flexibility, a vast repository of packages, and cutting-edge statistical techniques. It’s particularly strong in data visualization, machine learning, and complex statistical modeling, making it a favorite among data scientists and statisticians. By combining SAS’s reliability and regulatory acceptance with R’s flexibility and innovation, clinical programmers can deliver more comprehensive and sophisticated analyses.\n\n2. Why Coexistence is Key\n\nRegulatory Compliance with Innovation\nWhile SAS remains critical for regulatory submissions, incorporating R into the workflow allows for more exploratory and innovative analyses. Programmers can develop and test advanced models in R, then translate the final methods into SAS for submission, ensuring both innovation and compliance.\nEfficiency and Versatility\nSAS excels in data manipulation and report generation, while R shines in data exploration and visualization. Using SAS for large-scale data processing and R for detailed exploration allows programmers to optimize their workflow, making the entire process more efficient.\nBroader Skill Set for Programmers\nLearning both SAS and R equips statistical programmers with a versatile skill set that is highly valuable in the industry. Being proficient in both tools allows programmers to select the best tool for the task at hand, leading to better outcomes and greater career flexibility.\n\n3. Best Practices for Integrating R and SAS in Clinical Programming\n\nInteroperability\nSeamlessly integrating R and SAS in your workflow is possible with tools like SASPy (which allows running SAS code in a Python environment) and Rsas7bdat (for reading SAS data sets in R). This interoperability ensures that you can leverage the strengths of both languages without redundant effort.\nValidation and Reproducibility\nFor regulatory submissions, validation is critical. Use SAS for the final steps where strict adherence to regulatory standards is required. However, R can be used in earlier stages for exploratory analysis, with results cross-checked in SAS to ensure consistency and reproducibility.\nContinuous Learning and Adaptation\nAs the landscape of clinical programming evolves, so too should your skills. Regularly update your knowledge of both SAS and R, exploring new packages, procedures, and best practices to stay ahead of the curve.\n\nThe debate between R and SAS isn’t about choosing one over the other, but about integrating both to leverage their unique strengths. By fostering coexistence between R and SAS, statistical programmers can enhance their analytical capabilities, improve efficiency, and stay compliant with regulatory standards. In a field as dynamic and critical as clinical research, embracing the best of both worlds is not just beneficial—it’s essential."
  },
  {
    "objectID": "posts/post1/index.html",
    "href": "posts/post1/index.html",
    "title": "Essential Coding Best Practices for Clinical R programmer",
    "section": "",
    "text": "Almost of big pharmaceutical companies and CROs typically have many programmers, data managers and statisticians producing trial analysis and reporting programs. Each of these has their own style in programming. Unfortunately, such a large variety in program styles can lead to problems in the quality, readability, verifiability and maintainability of the program code. Establishing a central standard, or guideline, for good programming practices (GPP) is therefore a necessary first step for large companies.\nR programming is becoming a powerful tool for data analysis, statistical computing, and data visualization for pharma and clinical research industry. Whether you’re a seasoned data scientist or just starting your journey in clinical data analytics and programming, adhering to best practices in R is crucial for writing efficient, readable, and maintainable code.\nOnce the project design is ready, it’s the programmers’ job to develop the building blocks. While programming, it’s beneficial to follow certain conventions that increase the worth of the programs. These conventions are categorized into four criteria:\n Readability: Makes your programs easily understandable, increasing the programmer’s efficiency.\n Efficiency: Reduces the usage of resources like memory and CPU processing time, increasing the computer’s efficiency.\nReusability: Makes your programs reusable by separating frequently used logic from code and creating a separate program /user-defined function.\nRobustability: Makes your programs handle a wide variety of scenarios and does not crash. The program should be executable on a wider range of platforms.\nBelow, we explore key best practices to elevate your clinical R coding skills.\n1. Follow a Consistent Naming Convention\n\nVariable and Function Names: Use clear and descriptive names for variables and functions. Adopting a naming convention like snake_case (e.g., data_frame, calculate_mean) or camelCase (e.g., dataFrame, calculateMean) enhances readability. Consistency is key; choose one style and stick with it throughout your project.\nAvoid Abbreviations: While it may be tempting to use short abbreviations, they can lead to confusion. Instead, opt for fully descriptive names that convey the variable or function’s purpose.\n\n2. Write Modular Code\n\nFunction Use: Break down complex tasks into smaller, reusable functions. This modular approach not only simplifies your code but also makes it easier to test and debug. Each function should perform a single task and be named accordingly.\nScript Organization: Organize your scripts by logically grouping related functions and code blocks. Consider separating data loading, processing, and analysis into different scripts or sections within a script.\n\n3. Document Your Code\n\nComments: Use comments to explain the purpose of complex code blocks or functions. While R code can be self-explanatory with good naming conventions, comments provide valuable context, especially for future you or collaborators.\nRoxygen for Documentation: For functions, consider using Roxygen comments to create detailed documentation. Roxygen allows you to generate documentation files directly from your code, making it easier for others (and yourself) to understand and use your functions.\n\n4. Adopt the Tidyverse Style\n\nTidy Data Principles: Follow the principles of tidy data where each variable is a column, each observation is a row, and each type of observational unit is a table. This makes data manipulation and analysis more intuitive and aligns with the powerful functions provided by the tidyverse suite.\nPipe Operator (%&gt;%): Leverage the pipe operator to create clear and readable data transformation pipelines. The pipe operator enables a left-to-right flow of data through functions, reducing the need for nested function calls and making your code easier to follow.\n\n5. Error Handling and Debugging\n\nUse tryCatch for Robust Code: Implement tryCatch to handle errors gracefully. This ensures that your code can handle unexpected inputs or issues without crashing, allowing for better control over error messages and debugging.\nInteractive Debugging: Utilize R’s interactive debugging tools such as browser(), traceback(), and debug(). These tools allow you to step through your code line by line, inspect variables, and understand where and why errors occur.\n\n6. Optimize Performance\n\nVectorization: R is optimized for vectorized operations, meaning operations on entire vectors (arrays) are faster than loops. Whenever possible, replace loops with vectorized functions like apply, sapply, or the purrr package functions.\nProfiling and Benchmarking: Use system.time(), Rprof(), or the microbenchmark package to profile and benchmark your code. Identifying bottlenecks helps in optimizing performance-critical sections.\n\n7. Version Control\n\nUse Git: Track changes in your code with version control systems like Git. This not only helps in managing versions and collaborative work but also ensures that you can revert to earlier versions if something goes wrong.\nCommit Regularly: Make frequent commits with meaningful messages. This practice makes it easier to follow your development process and understand changes over time.\n\n8. Adopt a Linter\n\nLint Your Code: Use linters like lintr to automatically check your R code for syntax errors, potential bugs, and stylistic issues. Linters help enforce coding standards and improve code quality by catching issues early in the development process.\n\n9. Test Your Code\n\nAutomated Testing: Implement automated testing using frameworks like testthat. Writing tests ensures that your functions work as expected and helps prevent bugs from being introduced when making changes.\nTest Coverage: Aim for high test coverage, meaning most of your code is executed by tests. This provides greater confidence in the reliability of your code.\n\n10. Stay Updated and Continue Learning\n\nCommunity and Resources: The R community is vibrant and ever-evolving. Engage with it through forums, blogs, and attending conferences like useR! or RStudio::conf. Continuous learning through new packages, techniques, and best practices is vital in keeping your skills sharp.\n\nBy following these best practices, you’ll write R code that is not only functional but also elegant, efficient, and maintainable. Whether working on a solo project or collaborating with a team, these guidelines will help you produce high-quality R programs."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hamza Rahal",
    "section": "",
    "text": "I am a professional Statistical Programmer/Analyst with 10+ years of pharma industry experience in Statistical Programming and Clinical data analysis (using SAS/R) with study experience spanning the specification, production and validation of CDISC compliant datasets (SDTM & ADaM) along with TFLs. My experience covers Phase I-IV activities and multiple therapeutic areas as well as clinical data process improvement and standards implementation.\n\n\n\n\n\nI have been actively involved in training activities within the medical research field for a professional training center. My experience includes delivering comprehensive training on a variety of statistical software and conducted workshops, seminars, and one-on-one training sessions on SPSS, R, Stata, and SAS, power bi focusing on the specific needs of researchers and trainees from different fields.\n\n\n\n\n\n   \n\n\n\n\n   \n\n\n\nExtremely driven and goal orientated striving to push my team and company forward and adopting a lifelong learning mindset in the fast moving world of Data and AI, committed to stay on the cutting edge of the field."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "Hamza Rahal",
    "section": "",
    "text": "I am a professional Statistical Programmer/Analyst with 10+ years of pharma industry experience in Statistical Programming and Clinical data analysis (using SAS/R) with study experience spanning the specification, production and validation of CDISC compliant datasets (SDTM & ADaM) along with TFLs. My experience covers Phase I-IV activities and multiple therapeutic areas as well as clinical data process improvement and standards implementation.\n\n\n\n\n\nI have been actively involved in training activities within the medical research field for a professional training center. My experience includes delivering comprehensive training on a variety of statistical software and conducted workshops, seminars, and one-on-one training sessions on SPSS, R, Stata, and SAS, power bi focusing on the specific needs of researchers and trainees from different fields.\n\n\n\n\n\n   \n\n\n\n\n   \n\n\n\nExtremely driven and goal orientated striving to push my team and company forward and adopting a lifelong learning mindset in the fast moving world of Data and AI, committed to stay on the cutting edge of the field."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Posts",
    "section": "",
    "text": "The Importance of Agile/Scrum Methodology in Enhancing Statistical Programming and Analysis Efficiency in the Pharmaceutical Industry and Clinical Research\n\n\n\n\n\n\nthoughts\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\nHamza Rahal\n\n\n\n\n\n\n\n\n\n\n\n\nLeveraging Artificial Intelligence and Generative AI in Clinical Programming\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nAug 17, 2024\n\n\nHamza Rahal\n\n\n\n\n\n\n\n\n\n\n\n\nEssential Coding Best Practices for Clinical R programmer\n\n\n\n\n\n\nthoughts\n\n\n\n\n\n\n\n\n\nAug 15, 2024\n\n\nHamza Rahal\n\n\n\n\n\n\n\n\n\n\n\n\nQuality Assurance (QA) Importance and Standards in Clinical Statistical Programming\n\n\n\n\n\n\nthoughts\n\n\n\n\n\n\n\n\n\nJul 29, 2024\n\n\nHamza Rahal\n\n\n\n\n\n\n\n\n\n\n\n\nKey Benefits of R Shiny in Clinical Programming and Pharma industry\n\n\n\n\n\n\nthoughts\n\n\n\n\n\n\n\n\n\nJul 10, 2024\n\n\nHamza Rahal\n\n\n\n\n\n\n\n\n\n\n\n\nR and SAS: A Synergistic Approach for Clinical Programming\n\n\n\n\n\n\nthoughts\n\n\n\n\n\n\n\n\n\nJul 3, 2024\n\n\nHamza Rahal\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post2/index.html",
    "href": "posts/post2/index.html",
    "title": "Leveraging Artificial Intelligence and Generative AI in Clinical Programming",
    "section": "",
    "text": "Artificial Intelligence (AI) and Generative AI are revolutionizing various industries, including healthcare and clinical research. In clinical programming, where the accuracy, reproducibility, and speed of data analysis are crucial, these technologies can significantly enhance workflows and outcomes. Integrating AI and Generative AI with R programming offers new possibilities for automating complex tasks, improving decision-making, and generating novel insights from clinical data. Here’s how these technologies are being utilized and can be further explored in clinical programming using R.\n1. Enhancing Data Analysis and Interpretation\n\nPredictive Modeling: AI-driven predictive models can be built using R packages such as caret, xgboost, or randomForest. These models help in forecasting patient outcomes, identifying potential adverse events, and personalizing treatment plans based on patient data. Generative AI can also be employed to simulate patient outcomes under different treatment scenarios, aiding in risk-benefit analysis.\nAdvanced Statistical Methods: AI techniques like deep learning, implemented through R packages such as keras and tensorflow, can be used for more complex statistical analyses that traditional methods might not handle well. For instance, these methods can be applied to analyze large, high-dimensional datasets, like genomics or imaging data, leading to more accurate and nuanced interpretations.\n\n2. Automating Data Cleaning and Preprocessing\n\nAI-Powered Data Cleaning: Data cleaning is a critical yet time-consuming aspect of clinical programming. AI algorithms can automate much of this process by identifying and correcting data inconsistencies, imputing missing values, and standardizing data formats. Tools like DataRobot or custom R scripts utilizing AI libraries can be integrated into the workflow to streamline data preparation.\nGenerative Data Augmentation: In cases where clinical trial data is scarce, Generative AI models like GANs (Generative Adversarial Networks) can be used to create synthetic datasets that mimic the statistical properties of real-world data. This can be particularly useful for training AI models in clinical settings where real data may be limited or sensitive.\n\n3. Natural Language Processing (NLP) for Clinical Text Data\n\nAutomating Text Analysis: NLP, a subset of AI, is increasingly being used to analyze unstructured clinical data such as physician notes, medical records, and patient feedback. R packages like tm, text2vec, and spaCy can be used to extract valuable insights from text data, automate the identification of key medical terms, and even flag potential safety concerns.\nClinical Documentation: Generative AI models, such as GPT-based systems, can assist in generating clinical documentation, summarizing patient reports, and even drafting parts of clinical study protocols. This reduces the burden on clinical programmers and allows for faster report generation while maintaining accuracy and consistency.\n\n4. Improving Reproducibility and Compliance\n\nAutomated Reporting: R combined with AI can automate the generation of clinical trial reports. Generative AI can assist in writing sections of these reports, ensuring that they adhere to regulatory standards and reducing the time required for documentation. The use of templates and automated content generation can ensure consistency and compliance with regulatory requirements.\nAI-Driven Validation: AI can be used to validate data entry and processing steps, ensuring that they comply with pre-specified protocols. For instance, machine learning models can be trained to detect anomalies in data that might indicate errors or deviations from the protocol, allowing for early intervention.\n\n5. Clinical Trial Design and Simulation\n\nAI in Trial Design: AI can optimize clinical trial design by predicting the likely outcomes of different trial configurations. This includes patient recruitment strategies, determining the most appropriate endpoints, and selecting optimal dosing regimens. R can be used to build and validate these AI models, making trial design more efficient and cost-effective.\nGenerative AI for Simulation: Generative AI models can simulate clinical trial outcomes under various scenarios, helping researchers understand potential risks and benefits before the trial even begins. This approach can be integrated with R’s powerful simulation capabilities to create more robust trial designs.\n\n6. Enhancing Personalized Medicine\n\nAI for Precision Medicine: In personalized medicine, AI can analyze patient-specific data to tailor treatment plans. Using R, clinicians can build AI models that predict how individual patients will respond to specific treatments based on their genetic, environmental, and lifestyle factors. These models can help identify the most effective therapies, minimizing trial-and-error approaches in treatment.\nGenerative AI for Biomarker Discovery: Generative models can be used to discover new biomarkers by generating hypotheses and testing them against large datasets. This can accelerate the development of personalized treatment options and improve patient outcomes.\n\n7. Ethical Considerations and Regulatory Compliance\n\nEthical AI: When using AI in clinical programming, it’s crucial to address ethical considerations such as bias, transparency, and data privacy. AI models should be trained and validated on diverse datasets to avoid biases that could lead to unfair treatment recommendations. R’s flexibility allows for the inclusion of fairness checks and bias mitigation strategies during model development.\nRegulatory Alignment: Ensuring that AI applications in clinical programming comply with regulatory standards is vital. Models must be transparent, explainable, and validated in a way that meets FDA or EMA guidelines. Using R, programmers can document their AI workflows and provide detailed audit trails that demonstrate compliance.\n\n8. Challenges and Considerations\n\nData Quality: Ensuring data accuracy and completeness is crucial for AI model performance.\nModel Interpretability: Understanding how AI models arrive at their decisions is essential for regulatory compliance.\nEthical Implications: Addressing privacy, bias, and fairness concerns in AI applications.\nIntegration with Existing Systems: Seamlessly incorporating AI tools into clinical programming workflows.\nValidation and Verification: Rigorous testing to ensure the reliability of AI-generated outputs.\n\nIn conclusion, the integration of AI and Generative AI into clinical programming using R offers immense potential to enhance the efficiency, accuracy, and innovation in clinical research. As these technologies continue to develop, their role in automating complex tasks, improving decision-making, and personalizing patient care will likely expand, making them indispensable tools in the clinical programmer’s toolkit. However, as with any powerful tool, careful consideration of ethical, regulatory, and practical implications is essential to fully realize their benefits while minimizing risks."
  },
  {
    "objectID": "projects/Project1/index.html",
    "href": "projects/Project1/index.html",
    "title": "Visualizing Inequalities in Life Expectancy",
    "section": "",
    "text": "Life expectancy at birth is a measure of the average a living being is expected to live. It takes into account several demographic factors like gender, country, or year of birth.\nLife expectancy at birth can vary along time or between countries because of many causes: the evolution of medicine, the degree of development of countries, or the effect of armed conflicts. Life expectancy varies between gender, as well. The data shows that women live longer that men. Why? Several potential factors, including biological reasons and the theory that women tend to be more health conscious.\nLet’s create some plots to explore the inequalities about life expectancy at birth around the world. We will use a dataset from the United Nations Statistics Division, which is available here.\n\n\nCode\n# This sets plot images to a nice size\noptions(repr.plot.width = 6, repr.plot.height = 6)\n\n# Loading packages\nlibrary(dplyr)\n\n\n\nAttachement du package : 'dplyr'\n\n\nLes objets suivants sont masqués depuis 'package:stats':\n\n    filter, lag\n\n\nLes objets suivants sont masqués depuis 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(tidyr)\nlibrary(ggplot2)\n\n# Loading data\nlife_expectancy &lt;- read.csv(\"UNdata.csv\")\n\n# Taking a look at the first few rows\nhead(life_expectancy)\n\n\n  Country.or.Area Subgroup      Year\n1     Afghanistan   Female 2000-2005\n2     Afghanistan   Female 1995-2000\n3     Afghanistan   Female 1990-1995\n4     Afghanistan   Female 1985-1990\n5     Afghanistan     Male 2000-2005\n6     Afghanistan     Male 1995-2000\n                                                         Source  Unit Value\n1 UNPD_World Population Prospects_2006 (International estimate) Years    42\n2 UNPD_World Population Prospects_2006 (International estimate) Years    42\n3 UNPD_World Population Prospects_2006 (International estimate) Years    42\n4 UNPD_World Population Prospects_2006 (International estimate) Years    41\n5 UNPD_World Population Prospects_2006 (International estimate) Years    42\n6 UNPD_World Population Prospects_2006 (International estimate) Years    42\n  Value.Footnotes\n1              NA\n2              NA\n3              NA\n4              NA\n5              NA\n6              NA"
  },
  {
    "objectID": "projects/Project1/index.html#united-nations-life-expectancy-data",
    "href": "projects/Project1/index.html#united-nations-life-expectancy-data",
    "title": "Visualizing Inequalities in Life Expectancy",
    "section": "",
    "text": "Life expectancy at birth is a measure of the average a living being is expected to live. It takes into account several demographic factors like gender, country, or year of birth.\nLife expectancy at birth can vary along time or between countries because of many causes: the evolution of medicine, the degree of development of countries, or the effect of armed conflicts. Life expectancy varies between gender, as well. The data shows that women live longer that men. Why? Several potential factors, including biological reasons and the theory that women tend to be more health conscious.\nLet’s create some plots to explore the inequalities about life expectancy at birth around the world. We will use a dataset from the United Nations Statistics Division, which is available here.\n\n\nCode\n# This sets plot images to a nice size\noptions(repr.plot.width = 6, repr.plot.height = 6)\n\n# Loading packages\nlibrary(dplyr)\n\n\n\nAttachement du package : 'dplyr'\n\n\nLes objets suivants sont masqués depuis 'package:stats':\n\n    filter, lag\n\n\nLes objets suivants sont masqués depuis 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(tidyr)\nlibrary(ggplot2)\n\n# Loading data\nlife_expectancy &lt;- read.csv(\"UNdata.csv\")\n\n# Taking a look at the first few rows\nhead(life_expectancy)\n\n\n  Country.or.Area Subgroup      Year\n1     Afghanistan   Female 2000-2005\n2     Afghanistan   Female 1995-2000\n3     Afghanistan   Female 1990-1995\n4     Afghanistan   Female 1985-1990\n5     Afghanistan     Male 2000-2005\n6     Afghanistan     Male 1995-2000\n                                                         Source  Unit Value\n1 UNPD_World Population Prospects_2006 (International estimate) Years    42\n2 UNPD_World Population Prospects_2006 (International estimate) Years    42\n3 UNPD_World Population Prospects_2006 (International estimate) Years    42\n4 UNPD_World Population Prospects_2006 (International estimate) Years    41\n5 UNPD_World Population Prospects_2006 (International estimate) Years    42\n6 UNPD_World Population Prospects_2006 (International estimate) Years    42\n  Value.Footnotes\n1              NA\n2              NA\n3              NA\n4              NA\n5              NA\n6              NA"
  },
  {
    "objectID": "projects/Project1/index.html#life-expectancy-of-men-vs.-women-by-country",
    "href": "projects/Project1/index.html#life-expectancy-of-men-vs.-women-by-country",
    "title": "Visualizing Inequalities in Life Expectancy",
    "section": "2. Life expectancy of men vs. women by country",
    "text": "2. Life expectancy of men vs. women by country\nLet’s manipulate the data to make our exploration easier. We will build the dataset for our first plot in which we will represent the average life expectancy of men and women across countries for the last period recorded in our data (2000-2005).\n\n\nCode\n# Subsetting and reshaping the life expectancy data\nsubdata &lt;- life_expectancy  %&gt;% \n    filter(Year == \"2000-2005\") %&gt;%\n    select(Country.or.Area, Subgroup, Value) %&gt;%\n    spread(Subgroup, Value)\n\n# Taking a look at the first few rows\nhead(subdata)\n\n\n  Country.or.Area Female Male\n1     Afghanistan     42   42\n2         Albania     79   73\n3         Algeria     72   70\n4          Angola     43   39\n5       Argentina     78   71\n6         Armenia     75   68"
  },
  {
    "objectID": "projects/Project1/index.html#visualize-i",
    "href": "projects/Project1/index.html#visualize-i",
    "title": "Visualizing Inequalities in Life Expectancy",
    "section": "3. Visualize I",
    "text": "3. Visualize I\nA scatter plot is a useful way to visualize the relationship between two variables. It is a simple plot in which points are arranged on two axes, each of which represents one of those variables.\nLet’s create a scatter plot using ggplot2 to represent life expectancy of males (on the x-axis) against females (on the y-axis). We will create a straightforward plot in this task, without many details. We will take care of these kinds of things shortly.\n\n\nCode\n# Plotting male and female life expectancy\nggplot(subdata, aes(x = Male, y = Female)) +\n    geom_point()"
  },
  {
    "objectID": "projects/Project1/index.html#reference-lines-i",
    "href": "projects/Project1/index.html#reference-lines-i",
    "title": "Visualizing Inequalities in Life Expectancy",
    "section": "4. Reference lines I",
    "text": "4. Reference lines I\nA good plot must be easy to understand. There are many tools in ggplot2 to achieve this goal and we will explore some of them now. Starting from the previous plot, let’s set the same limits for both axes as well as place a diagonal line for reference. After doing this, the difference between men and women across countries will be easier to interpret.\nAfter completing this task, we will see how most of the points are arranged above the diagonal and how there is a significant dispersion among them. What does this all mean?\n\n\nCode\n# Adding an abline and changing the scale of axes of the previous plots\nggplot(subdata, aes(x = Male, y = Female)) +\n    geom_point() +\n    geom_abline(intercept = 0, slope = 1, linetype = 2) +\n    scale_x_continuous(limits=c(35,85))+\n    scale_y_continuous(limits=c(35,85))"
  },
  {
    "objectID": "projects/Project1/index.html#plot-titles-and-axis-labels",
    "href": "projects/Project1/index.html#plot-titles-and-axis-labels",
    "title": "Visualizing Inequalities in Life Expectancy",
    "section": "5. Plot titles and axis labels",
    "text": "5. Plot titles and axis labels\nA key point to make a plot understandable is placing clear labels on it. Let’s add titles, axis labels, and a caption to refer to the source of data. Let’s also change the appearance to make it clearer.\n\n\nCode\n# Adding labels to previous plot\nggplot(subdata, aes(x=Male, y=Female))+\n  geom_point(colour=\"white\", fill=\"chartreuse3\", shape=21, alpha=.55, size=5)+\n  geom_abline(intercept = 0, slope = 1, linetype=2)+\n  scale_x_continuous(limits=c(35,85))+\n  scale_y_continuous(limits=c(35,85))+\n  labs(title=\"Life Expectancy at Birth by Country\",\n       subtitle=\"Years. Period: 2000-2005. Average.\",\n       caption=\"Source: United Nations Statistics Division\",\n       x=\"Males\",\n       y=\"Females\")"
  },
  {
    "objectID": "projects/Project1/index.html#highlighting-remarkable-countries-i",
    "href": "projects/Project1/index.html#highlighting-remarkable-countries-i",
    "title": "Visualizing Inequalities in Life Expectancy",
    "section": "6. Highlighting remarkable countries I",
    "text": "6. Highlighting remarkable countries I\nNow, we will label some points of our plot with the name of its corresponding country. We want to draw attention to some special countries where the gap in life expectancy between men and women is significantly high. These will be the final touches on this first plot.\n\n\nCode\n# Subseting data to obtain countries of interest\ntop_male &lt;- subdata %&gt;% arrange(Male-Female) %&gt;% head(3)\ntop_female &lt;- subdata %&gt;% arrange(Female-Male) %&gt;% head(3)\n\n# Adding text to the previous plot to label countries of interest\nggplot(subdata, aes(x=Male, y=Female, label=Country.or.Area))+\n  geom_point(colour=\"white\", fill=\"chartreuse3\", shape=21, alpha=.55, size=5)+\n  geom_abline(intercept = 0, slope = 1, linetype=2)+\n  scale_x_continuous(limits=c(35,85))+\n  scale_y_continuous(limits=c(35,85))+\n  labs(title=\"Life Expectancy at Birth by Country\",\n       subtitle=\"Years. Period: 2000-2005. Average.\",\n       caption=\"Source: United Nations Statistics Division\",\n       x=\"Males\",\n       y=\"Females\") +\n  geom_text(data=top_male, size = 3) +\n  geom_text(data=top_female, size = 3) +\n  theme_bw()"
  },
  {
    "objectID": "projects/Project1/index.html#how-has-life-expectancy-by-gender-evolved",
    "href": "projects/Project1/index.html#how-has-life-expectancy-by-gender-evolved",
    "title": "Visualizing Inequalities in Life Expectancy",
    "section": "7. How has life expectancy by gender evolved?",
    "text": "7. How has life expectancy by gender evolved?\nSince our data contains historical information, let’s see now how life expectancy has evolved in recent years. Our second plot will represent the difference between men and women across countries between two periods: 2000-2005 and 1985-1990.\nLet’s start building a dataset called subdata2 for our second plot.\n\n\nCode\n# Subsetting, mutating and reshaping the life expectancy data\nsubdata2 &lt;- life_expectancy %&gt;% \n  filter(Year %in% c(\"1985-1990\", \"2000-2005\")) %&gt;% \n  mutate(Sub_Year=paste(Subgroup, Year, sep=\"_\")) %&gt;% \n  mutate(Sub_Year=gsub(\"-\", \"_\", Sub_Year)) %&gt;% \n  select(-Subgroup, -Year) %&gt;%\n  spread(Sub_Year, Value) %&gt;%\n  mutate(\n    diff_Female = Female_2000_2005 - Female_1985_1990,\n    diff_Male = Male_2000_2005 - Male_1985_1990\n  )\n\n# Taking a look at the first few rows\nhead(subdata2)\n\n\n  Country.or.Area                                                        Source\n1     Afghanistan UNPD_World Population Prospects_2006 (International estimate)\n2         Albania UNPD_World Population Prospects_2006 (International estimate)\n3         Algeria UNPD_World Population Prospects_2006 (International estimate)\n4          Angola UNPD_World Population Prospects_2006 (International estimate)\n5       Argentina UNPD_World Population Prospects_2006 (International estimate)\n6         Armenia UNPD_World Population Prospects_2006 (International estimate)\n   Unit Value.Footnotes Female_1985_1990 Female_2000_2005 Male_1985_1990\n1 Years              NA               41               42             41\n2 Years              NA               75               79             69\n3 Years              NA               67               72             65\n4 Years              NA               42               43             38\n5 Years              NA               75               78             68\n6 Years              NA               71               75             66\n  Male_2000_2005 diff_Female diff_Male\n1             42           1         1\n2             73           4         4\n3             70           5         5\n4             39           1         1\n5             71           3         3\n6             68           4         2"
  },
  {
    "objectID": "projects/Project1/index.html#visualize-ii",
    "href": "projects/Project1/index.html#visualize-ii",
    "title": "Visualizing Inequalities in Life Expectancy",
    "section": "8. Visualize II",
    "text": "8. Visualize II\nNow let’s create our second plot in which we will represent average life expectancy differences between “1985-1990” and “2000-2005” for men and women.\n\n\nCode\n# Doing a nice first version of the plot with abline, scaling axis and adding labels\nggplot(subdata2, aes(x=diff_Male, y=diff_Female, label=Country.or.Area))+\n  geom_point(colour=\"white\", fill=\"chartreuse3\", shape=21, alpha=.55, size=5)+\n  geom_abline(intercept = 0, slope = 1, linetype=2)+\n  scale_x_continuous(limits = c(-25, 25)) +\n  scale_y_continuous(limits = c(-25, 25)) +\n  labs(title=\"Life Expectancy at Birth by Country in Years\",\n       subtitle=\"Difference between 1985-1990 and 2000-2005. Average.\",\n       caption=\"Source: United Nations Statistics Division\",\n       x=\"Males\",\n       y=\"Females\")+\ntheme_bw()"
  },
  {
    "objectID": "projects/Project1/index.html#reference-lines-ii",
    "href": "projects/Project1/index.html#reference-lines-ii",
    "title": "Visualizing Inequalities in Life Expectancy",
    "section": "9. Reference lines II",
    "text": "9. Reference lines II\nAdding reference lines can make plots easier to understand. We already added a diagonal line to visualize differences between men and women more clearly. Now we will add two more lines to help to identify in which countries people increased or decreased their life expectancy in the period analyzed.\n\n\nCode\n# Adding an hline and vline to previous plots\nggplot(subdata2, aes(x=diff_Male, y=diff_Female, label=Country.or.Area))+\n  geom_point(colour=\"white\", fill=\"chartreuse3\", shape=21, alpha=.55, size=5)+\n  geom_abline(intercept = 0, slope = 1, linetype=2)+\n  scale_x_continuous(limits=c(-25,25))+\n  scale_y_continuous(limits=c(-25,25))+\n  geom_hline(yintercept = 0, linetype = 2) +\n  geom_vline(xintercept = 0, linetype = 2) +\n  labs(title=\"Life Expectancy at Birth by Country\",\n       subtitle=\"Years. Difference between 1985-1990 and 2000-2005. Average.\",\n       caption=\"Source: United Nations Statistics Division\",\n       x=\"Males\",\n       y=\"Females\")+\ntheme_bw()"
  },
  {
    "objectID": "projects/Project1/index.html#highlighting-remarkable-countries-ii",
    "href": "projects/Project1/index.html#highlighting-remarkable-countries-ii",
    "title": "Visualizing Inequalities in Life Expectancy",
    "section": "10. Highlighting remarkable countries II",
    "text": "10. Highlighting remarkable countries II\nAs we did in the first plot, let’s label some points. Concretely, we will point those three where the aggregated average life expectancy for men and women increased most and those three where decreased most in the period.\n\n\nCode\n# Subseting data to obtain countries of interest\ntop &lt;- subdata2 %&gt;% arrange(diff_Male+diff_Female) %&gt;% head(3)\nbottom &lt;- subdata2 %&gt;% arrange(-(diff_Male+diff_Female)) %&gt;% head(3)\n\n# Adding text to the previous plot to label countries of interest\nggplot(subdata2, aes(x=diff_Male, y=diff_Female, label=Country.or.Area), guide=FALSE)+\n  geom_point(colour=\"white\", fill=\"chartreuse3\", shape=21, alpha=.55, size=5)+\n  geom_abline(intercept = 0, slope = 1, linetype=2)+\n  scale_x_continuous(limits=c(-25,25))+\n  scale_y_continuous(limits=c(-25,25))+\n  geom_hline(yintercept=0, linetype=2)+\n  geom_vline(xintercept=0, linetype=2)+\n  labs(title=\"Life Expectancy at Birth by Country\",\n       subtitle=\"Years. Difference between 1985-1990 and 2000-2005. Average.\",\n       caption=\"Source: United Nations Statistics Division\",\n       x=\"Males\",\n       y=\"Females\")+\n  geom_text(data=top, size=3)+\n  geom_text(data=bottom, size=3)+\n  theme_bw()"
  },
  {
    "objectID": "projects/Project3/index.html",
    "href": "projects/Project3/index.html",
    "title": "Health Survey Data Analysis of BMI",
    "section": "",
    "text": "We’ve all taken a survey at some point, but do you ever wonder what happens to your answers? Surveys are given to a carefully selected sample of people with the goal of generalizing the results to a much larger population.\nThe National Health and Nutrition Examination Survey (NHANES) data is a complex survey of tens of thousands of people designed to assess the health and nutritional status of adults and children in the United States. The NHANES data includes many measurements related to overall health, physical activity, diet, psychological health, socioeconomic factors and more.\nDepending on the sampling design, each person has a sampling weight that quantifies how many people in the larger population their data is meant to represent. In this notebook, we’ll apply survey methods that use sampling weights to estimate and model relationships between measurements.\nWe are going to focus on a common health indicator, Body Mass Index (BMI kg/m2), and how it is related to physical activity. We’ll visualize the data and use survey-weighted regression to test for associations.\n\n\nCode\n# Load the NHANES and dplyr packages\nlibrary(NHANES)\nlibrary(dplyr)\n\n\n\nAttachement du package : 'dplyr'\n\n\nLes objets suivants sont masqués depuis 'package:stats':\n\n    filter, lag\n\n\nLes objets suivants sont masqués depuis 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\n# Load the NHANESraw data\ndata(\"NHANESraw\")\n\n# Take a glimpse at the contents\nglimpse(NHANESraw)\n\n\nRows: 20,293\nColumns: 78\n$ ID               &lt;int&gt; 51624, 51625, 51626, 51627, 51628, 51629, 51630, 5163…\n$ SurveyYr         &lt;fct&gt; 2009_10, 2009_10, 2009_10, 2009_10, 2009_10, 2009_10,…\n$ Gender           &lt;fct&gt; male, male, male, male, female, male, female, female,…\n$ Age              &lt;int&gt; 34, 4, 16, 10, 60, 26, 49, 1, 10, 80, 10, 80, 4, 35, …\n$ AgeMonths        &lt;int&gt; 409, 49, 202, 131, 722, 313, 596, 12, 124, NA, 121, N…\n$ Race1            &lt;fct&gt; White, Other, Black, Black, Black, Mexican, White, Wh…\n$ Race3            &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Education        &lt;fct&gt; High School, NA, NA, NA, High School, 9 - 11th Grade,…\n$ MaritalStatus    &lt;fct&gt; Married, NA, NA, NA, Widowed, Married, LivePartner, N…\n$ HHIncome         &lt;fct&gt; 25000-34999, 20000-24999, 45000-54999, 20000-24999, 1…\n$ HHIncomeMid      &lt;int&gt; 30000, 22500, 50000, 22500, 12500, 30000, 40000, 4000…\n$ Poverty          &lt;dbl&gt; 1.36, 1.07, 2.27, 0.81, 0.69, 1.01, 1.91, 1.36, 2.68,…\n$ HomeRooms        &lt;int&gt; 6, 9, 5, 6, 6, 4, 5, 5, 7, 4, 5, 5, 7, NA, 6, 6, 5, 6…\n$ HomeOwn          &lt;fct&gt; Own, Own, Own, Rent, Rent, Rent, Rent, Rent, Own, Own…\n$ Work             &lt;fct&gt; NotWorking, NA, NotWorking, NA, NotWorking, Working, …\n$ Weight           &lt;dbl&gt; 87.4, 17.0, 72.3, 39.8, 116.8, 97.6, 86.7, 9.4, 26.0,…\n$ Length           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, 75.7, NA, NA, NA, NA, NA,…\n$ HeadCirc         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Height           &lt;dbl&gt; 164.7, 105.4, 181.3, 147.8, 166.0, 173.0, 168.4, NA, …\n$ BMI              &lt;dbl&gt; 32.22, 15.30, 22.00, 18.22, 42.39, 32.61, 30.57, NA, …\n$ BMICatUnder20yrs &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ BMI_WHO          &lt;fct&gt; 30.0_plus, 12.0_18.5, 18.5_to_24.9, 12.0_18.5, 30.0_p…\n$ Pulse            &lt;int&gt; 70, NA, 68, 68, 72, 72, 86, NA, 70, 88, 84, 54, NA, N…\n$ BPSysAve         &lt;int&gt; 113, NA, 109, 93, 150, 104, 112, NA, 108, 139, 94, 12…\n$ BPDiaAve         &lt;int&gt; 85, NA, 59, 41, 68, 49, 75, NA, 53, 43, 45, 60, NA, N…\n$ BPSys1           &lt;int&gt; 114, NA, 112, 92, 154, 102, 118, NA, 106, 142, 94, 12…\n$ BPDia1           &lt;int&gt; 88, NA, 62, 36, 70, 50, 82, NA, 60, 62, 38, 62, NA, N…\n$ BPSys2           &lt;int&gt; 114, NA, 114, 94, 150, 104, 108, NA, 106, 140, 92, 12…\n$ BPDia2           &lt;int&gt; 88, NA, 60, 44, 68, 48, 74, NA, 50, 46, 40, 62, NA, N…\n$ BPSys3           &lt;int&gt; 112, NA, 104, 92, 150, 104, 116, NA, 110, 138, 96, 11…\n$ BPDia3           &lt;int&gt; 82, NA, 58, 38, 68, 50, 76, NA, 56, 40, 50, 58, NA, N…\n$ Testosterone     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ DirectChol       &lt;dbl&gt; 1.29, NA, 1.55, 1.89, 1.16, 1.16, 1.16, NA, 1.58, 1.9…\n$ TotChol          &lt;dbl&gt; 3.49, NA, 4.97, 4.16, 5.22, 4.14, 6.70, NA, 4.14, 4.7…\n$ UrineVol1        &lt;int&gt; 352, NA, 281, 139, 30, 202, 77, NA, 39, 128, 109, 38,…\n$ UrineFlow1       &lt;dbl&gt; NA, NA, 0.415, 1.078, 0.476, 0.563, 0.094, NA, 0.300,…\n$ UrineVol2        &lt;int&gt; NA, NA, NA, NA, 246, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ UrineFlow2       &lt;dbl&gt; NA, NA, NA, NA, 2.51, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Diabetes         &lt;fct&gt; No, No, No, No, Yes, No, No, No, No, No, No, Yes, No,…\n$ DiabetesAge      &lt;int&gt; NA, NA, NA, NA, 56, NA, NA, NA, NA, NA, NA, 70, NA, N…\n$ HealthGen        &lt;fct&gt; Good, NA, Vgood, NA, Fair, Good, Good, NA, NA, Excell…\n$ DaysPhysHlthBad  &lt;int&gt; 0, NA, 2, NA, 20, 2, 0, NA, NA, 0, NA, 0, NA, NA, NA,…\n$ DaysMentHlthBad  &lt;int&gt; 15, NA, 0, NA, 25, 14, 10, NA, NA, 0, NA, 0, NA, NA, …\n$ LittleInterest   &lt;fct&gt; Most, NA, NA, NA, Most, None, Several, NA, NA, None, …\n$ Depressed        &lt;fct&gt; Several, NA, NA, NA, Most, Most, Several, NA, NA, Non…\n$ nPregnancies     &lt;int&gt; NA, NA, NA, NA, 1, NA, 2, NA, NA, NA, NA, NA, NA, NA,…\n$ nBabies          &lt;int&gt; NA, NA, NA, NA, 1, NA, 2, NA, NA, NA, NA, NA, NA, NA,…\n$ Age1stBaby       &lt;int&gt; NA, NA, NA, NA, NA, NA, 27, NA, NA, NA, NA, NA, NA, N…\n$ SleepHrsNight    &lt;int&gt; 4, NA, 8, NA, 4, 4, 8, NA, NA, 6, NA, 9, NA, 7, NA, N…\n$ SleepTrouble     &lt;fct&gt; Yes, NA, No, NA, No, No, Yes, NA, NA, No, NA, No, NA,…\n$ PhysActive       &lt;fct&gt; No, NA, Yes, NA, No, Yes, No, NA, NA, Yes, NA, No, NA…\n$ PhysActiveDays   &lt;int&gt; NA, NA, 5, NA, NA, 2, NA, NA, NA, 4, NA, NA, NA, NA, …\n$ TVHrsDay         &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CompHrsDay       &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ TVHrsDayChild    &lt;int&gt; NA, 4, NA, 1, NA, NA, NA, NA, 1, NA, 3, NA, 2, NA, 5,…\n$ CompHrsDayChild  &lt;int&gt; NA, 1, NA, 1, NA, NA, NA, NA, 0, NA, 0, NA, 1, NA, 0,…\n$ Alcohol12PlusYr  &lt;fct&gt; Yes, NA, NA, NA, No, Yes, Yes, NA, NA, Yes, NA, No, N…\n$ AlcoholDay       &lt;int&gt; NA, NA, NA, NA, NA, 19, 2, NA, NA, 1, NA, NA, NA, NA,…\n$ AlcoholYear      &lt;int&gt; 0, NA, NA, NA, 0, 48, 20, NA, NA, 52, NA, 0, NA, NA, …\n$ SmokeNow         &lt;fct&gt; No, NA, NA, NA, Yes, No, Yes, NA, NA, No, NA, No, NA,…\n$ Smoke100         &lt;fct&gt; Yes, NA, NA, NA, Yes, Yes, Yes, NA, NA, Yes, NA, Yes,…\n$ SmokeAge         &lt;int&gt; 18, NA, NA, NA, 16, 15, 38, NA, NA, 16, NA, 21, NA, N…\n$ Marijuana        &lt;fct&gt; Yes, NA, NA, NA, NA, Yes, Yes, NA, NA, NA, NA, NA, NA…\n$ AgeFirstMarij    &lt;int&gt; 17, NA, NA, NA, NA, 10, 18, NA, NA, NA, NA, NA, NA, N…\n$ RegularMarij     &lt;fct&gt; No, NA, NA, NA, NA, Yes, No, NA, NA, NA, NA, NA, NA, …\n$ AgeRegMarij      &lt;int&gt; NA, NA, NA, NA, NA, 12, NA, NA, NA, NA, NA, NA, NA, N…\n$ HardDrugs        &lt;fct&gt; Yes, NA, NA, NA, No, Yes, Yes, NA, NA, NA, NA, NA, NA…\n$ SexEver          &lt;fct&gt; Yes, NA, NA, NA, Yes, Yes, Yes, NA, NA, NA, NA, NA, N…\n$ SexAge           &lt;int&gt; 16, NA, NA, NA, 15, 9, 12, NA, NA, NA, NA, NA, NA, NA…\n$ SexNumPartnLife  &lt;int&gt; 8, NA, NA, NA, 4, 10, 10, NA, NA, NA, NA, NA, NA, NA,…\n$ SexNumPartYear   &lt;int&gt; 1, NA, NA, NA, NA, 1, 1, NA, NA, NA, NA, NA, NA, NA, …\n$ SameSex          &lt;fct&gt; No, NA, NA, NA, No, No, Yes, NA, NA, NA, NA, NA, NA, …\n$ SexOrientation   &lt;fct&gt; Heterosexual, NA, NA, NA, NA, Heterosexual, Heterosex…\n$ WTINT2YR         &lt;dbl&gt; 80100.544, 53901.104, 13953.078, 11664.899, 20090.339…\n$ WTMEC2YR         &lt;dbl&gt; 81528.772, 56995.035, 14509.279, 12041.635, 21000.339…\n$ SDMVPSU          &lt;int&gt; 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1,…\n$ SDMVSTRA         &lt;int&gt; 83, 79, 84, 86, 75, 88, 85, 86, 88, 77, 86, 79, 84, 7…\n$ PregnantNow      &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, U…"
  },
  {
    "objectID": "projects/Project3/index.html#survey-of-bmi-and-physical-activity",
    "href": "projects/Project3/index.html#survey-of-bmi-and-physical-activity",
    "title": "Health Survey Data Analysis of BMI",
    "section": "",
    "text": "We’ve all taken a survey at some point, but do you ever wonder what happens to your answers? Surveys are given to a carefully selected sample of people with the goal of generalizing the results to a much larger population.\nThe National Health and Nutrition Examination Survey (NHANES) data is a complex survey of tens of thousands of people designed to assess the health and nutritional status of adults and children in the United States. The NHANES data includes many measurements related to overall health, physical activity, diet, psychological health, socioeconomic factors and more.\nDepending on the sampling design, each person has a sampling weight that quantifies how many people in the larger population their data is meant to represent. In this notebook, we’ll apply survey methods that use sampling weights to estimate and model relationships between measurements.\nWe are going to focus on a common health indicator, Body Mass Index (BMI kg/m2), and how it is related to physical activity. We’ll visualize the data and use survey-weighted regression to test for associations.\n\n\nCode\n# Load the NHANES and dplyr packages\nlibrary(NHANES)\nlibrary(dplyr)\n\n\n\nAttachement du package : 'dplyr'\n\n\nLes objets suivants sont masqués depuis 'package:stats':\n\n    filter, lag\n\n\nLes objets suivants sont masqués depuis 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\n# Load the NHANESraw data\ndata(\"NHANESraw\")\n\n# Take a glimpse at the contents\nglimpse(NHANESraw)\n\n\nRows: 20,293\nColumns: 78\n$ ID               &lt;int&gt; 51624, 51625, 51626, 51627, 51628, 51629, 51630, 5163…\n$ SurveyYr         &lt;fct&gt; 2009_10, 2009_10, 2009_10, 2009_10, 2009_10, 2009_10,…\n$ Gender           &lt;fct&gt; male, male, male, male, female, male, female, female,…\n$ Age              &lt;int&gt; 34, 4, 16, 10, 60, 26, 49, 1, 10, 80, 10, 80, 4, 35, …\n$ AgeMonths        &lt;int&gt; 409, 49, 202, 131, 722, 313, 596, 12, 124, NA, 121, N…\n$ Race1            &lt;fct&gt; White, Other, Black, Black, Black, Mexican, White, Wh…\n$ Race3            &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Education        &lt;fct&gt; High School, NA, NA, NA, High School, 9 - 11th Grade,…\n$ MaritalStatus    &lt;fct&gt; Married, NA, NA, NA, Widowed, Married, LivePartner, N…\n$ HHIncome         &lt;fct&gt; 25000-34999, 20000-24999, 45000-54999, 20000-24999, 1…\n$ HHIncomeMid      &lt;int&gt; 30000, 22500, 50000, 22500, 12500, 30000, 40000, 4000…\n$ Poverty          &lt;dbl&gt; 1.36, 1.07, 2.27, 0.81, 0.69, 1.01, 1.91, 1.36, 2.68,…\n$ HomeRooms        &lt;int&gt; 6, 9, 5, 6, 6, 4, 5, 5, 7, 4, 5, 5, 7, NA, 6, 6, 5, 6…\n$ HomeOwn          &lt;fct&gt; Own, Own, Own, Rent, Rent, Rent, Rent, Rent, Own, Own…\n$ Work             &lt;fct&gt; NotWorking, NA, NotWorking, NA, NotWorking, Working, …\n$ Weight           &lt;dbl&gt; 87.4, 17.0, 72.3, 39.8, 116.8, 97.6, 86.7, 9.4, 26.0,…\n$ Length           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, 75.7, NA, NA, NA, NA, NA,…\n$ HeadCirc         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Height           &lt;dbl&gt; 164.7, 105.4, 181.3, 147.8, 166.0, 173.0, 168.4, NA, …\n$ BMI              &lt;dbl&gt; 32.22, 15.30, 22.00, 18.22, 42.39, 32.61, 30.57, NA, …\n$ BMICatUnder20yrs &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ BMI_WHO          &lt;fct&gt; 30.0_plus, 12.0_18.5, 18.5_to_24.9, 12.0_18.5, 30.0_p…\n$ Pulse            &lt;int&gt; 70, NA, 68, 68, 72, 72, 86, NA, 70, 88, 84, 54, NA, N…\n$ BPSysAve         &lt;int&gt; 113, NA, 109, 93, 150, 104, 112, NA, 108, 139, 94, 12…\n$ BPDiaAve         &lt;int&gt; 85, NA, 59, 41, 68, 49, 75, NA, 53, 43, 45, 60, NA, N…\n$ BPSys1           &lt;int&gt; 114, NA, 112, 92, 154, 102, 118, NA, 106, 142, 94, 12…\n$ BPDia1           &lt;int&gt; 88, NA, 62, 36, 70, 50, 82, NA, 60, 62, 38, 62, NA, N…\n$ BPSys2           &lt;int&gt; 114, NA, 114, 94, 150, 104, 108, NA, 106, 140, 92, 12…\n$ BPDia2           &lt;int&gt; 88, NA, 60, 44, 68, 48, 74, NA, 50, 46, 40, 62, NA, N…\n$ BPSys3           &lt;int&gt; 112, NA, 104, 92, 150, 104, 116, NA, 110, 138, 96, 11…\n$ BPDia3           &lt;int&gt; 82, NA, 58, 38, 68, 50, 76, NA, 56, 40, 50, 58, NA, N…\n$ Testosterone     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ DirectChol       &lt;dbl&gt; 1.29, NA, 1.55, 1.89, 1.16, 1.16, 1.16, NA, 1.58, 1.9…\n$ TotChol          &lt;dbl&gt; 3.49, NA, 4.97, 4.16, 5.22, 4.14, 6.70, NA, 4.14, 4.7…\n$ UrineVol1        &lt;int&gt; 352, NA, 281, 139, 30, 202, 77, NA, 39, 128, 109, 38,…\n$ UrineFlow1       &lt;dbl&gt; NA, NA, 0.415, 1.078, 0.476, 0.563, 0.094, NA, 0.300,…\n$ UrineVol2        &lt;int&gt; NA, NA, NA, NA, 246, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ UrineFlow2       &lt;dbl&gt; NA, NA, NA, NA, 2.51, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Diabetes         &lt;fct&gt; No, No, No, No, Yes, No, No, No, No, No, No, Yes, No,…\n$ DiabetesAge      &lt;int&gt; NA, NA, NA, NA, 56, NA, NA, NA, NA, NA, NA, 70, NA, N…\n$ HealthGen        &lt;fct&gt; Good, NA, Vgood, NA, Fair, Good, Good, NA, NA, Excell…\n$ DaysPhysHlthBad  &lt;int&gt; 0, NA, 2, NA, 20, 2, 0, NA, NA, 0, NA, 0, NA, NA, NA,…\n$ DaysMentHlthBad  &lt;int&gt; 15, NA, 0, NA, 25, 14, 10, NA, NA, 0, NA, 0, NA, NA, …\n$ LittleInterest   &lt;fct&gt; Most, NA, NA, NA, Most, None, Several, NA, NA, None, …\n$ Depressed        &lt;fct&gt; Several, NA, NA, NA, Most, Most, Several, NA, NA, Non…\n$ nPregnancies     &lt;int&gt; NA, NA, NA, NA, 1, NA, 2, NA, NA, NA, NA, NA, NA, NA,…\n$ nBabies          &lt;int&gt; NA, NA, NA, NA, 1, NA, 2, NA, NA, NA, NA, NA, NA, NA,…\n$ Age1stBaby       &lt;int&gt; NA, NA, NA, NA, NA, NA, 27, NA, NA, NA, NA, NA, NA, N…\n$ SleepHrsNight    &lt;int&gt; 4, NA, 8, NA, 4, 4, 8, NA, NA, 6, NA, 9, NA, 7, NA, N…\n$ SleepTrouble     &lt;fct&gt; Yes, NA, No, NA, No, No, Yes, NA, NA, No, NA, No, NA,…\n$ PhysActive       &lt;fct&gt; No, NA, Yes, NA, No, Yes, No, NA, NA, Yes, NA, No, NA…\n$ PhysActiveDays   &lt;int&gt; NA, NA, 5, NA, NA, 2, NA, NA, NA, 4, NA, NA, NA, NA, …\n$ TVHrsDay         &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CompHrsDay       &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ TVHrsDayChild    &lt;int&gt; NA, 4, NA, 1, NA, NA, NA, NA, 1, NA, 3, NA, 2, NA, 5,…\n$ CompHrsDayChild  &lt;int&gt; NA, 1, NA, 1, NA, NA, NA, NA, 0, NA, 0, NA, 1, NA, 0,…\n$ Alcohol12PlusYr  &lt;fct&gt; Yes, NA, NA, NA, No, Yes, Yes, NA, NA, Yes, NA, No, N…\n$ AlcoholDay       &lt;int&gt; NA, NA, NA, NA, NA, 19, 2, NA, NA, 1, NA, NA, NA, NA,…\n$ AlcoholYear      &lt;int&gt; 0, NA, NA, NA, 0, 48, 20, NA, NA, 52, NA, 0, NA, NA, …\n$ SmokeNow         &lt;fct&gt; No, NA, NA, NA, Yes, No, Yes, NA, NA, No, NA, No, NA,…\n$ Smoke100         &lt;fct&gt; Yes, NA, NA, NA, Yes, Yes, Yes, NA, NA, Yes, NA, Yes,…\n$ SmokeAge         &lt;int&gt; 18, NA, NA, NA, 16, 15, 38, NA, NA, 16, NA, 21, NA, N…\n$ Marijuana        &lt;fct&gt; Yes, NA, NA, NA, NA, Yes, Yes, NA, NA, NA, NA, NA, NA…\n$ AgeFirstMarij    &lt;int&gt; 17, NA, NA, NA, NA, 10, 18, NA, NA, NA, NA, NA, NA, N…\n$ RegularMarij     &lt;fct&gt; No, NA, NA, NA, NA, Yes, No, NA, NA, NA, NA, NA, NA, …\n$ AgeRegMarij      &lt;int&gt; NA, NA, NA, NA, NA, 12, NA, NA, NA, NA, NA, NA, NA, N…\n$ HardDrugs        &lt;fct&gt; Yes, NA, NA, NA, No, Yes, Yes, NA, NA, NA, NA, NA, NA…\n$ SexEver          &lt;fct&gt; Yes, NA, NA, NA, Yes, Yes, Yes, NA, NA, NA, NA, NA, N…\n$ SexAge           &lt;int&gt; 16, NA, NA, NA, 15, 9, 12, NA, NA, NA, NA, NA, NA, NA…\n$ SexNumPartnLife  &lt;int&gt; 8, NA, NA, NA, 4, 10, 10, NA, NA, NA, NA, NA, NA, NA,…\n$ SexNumPartYear   &lt;int&gt; 1, NA, NA, NA, NA, 1, 1, NA, NA, NA, NA, NA, NA, NA, …\n$ SameSex          &lt;fct&gt; No, NA, NA, NA, No, No, Yes, NA, NA, NA, NA, NA, NA, …\n$ SexOrientation   &lt;fct&gt; Heterosexual, NA, NA, NA, NA, Heterosexual, Heterosex…\n$ WTINT2YR         &lt;dbl&gt; 80100.544, 53901.104, 13953.078, 11664.899, 20090.339…\n$ WTMEC2YR         &lt;dbl&gt; 81528.772, 56995.035, 14509.279, 12041.635, 21000.339…\n$ SDMVPSU          &lt;int&gt; 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1,…\n$ SDMVSTRA         &lt;int&gt; 83, 79, 84, 86, 75, 88, 85, 86, 88, 77, 86, 79, 84, 7…\n$ PregnantNow      &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, U…"
  },
  {
    "objectID": "projects/Project3/index.html#visualize-survey-weight-and-strata-variables",
    "href": "projects/Project3/index.html#visualize-survey-weight-and-strata-variables",
    "title": "Health Survey Data Analysis of BMI",
    "section": "2. Visualize survey weight and strata variables",
    "text": "2. Visualize survey weight and strata variables\nWe see from glimpse() that the NHANESraw data has many health measurement variables. It also contains a sampling weight variable WTMEC2YR.\nSince NHANESraw data spans 4 years (2009–2012) and the sampling weights are based on 2 years of data, we first need to create a weight variable that scales the sample across the full 4 years. We will divide the 2-year weight in half so that in total, the weights sum to the total US population.\nThe NHANES data has oversampled some geographic regions and specific minority groups. By examining the distribution of sampling weights for each race, we can see that Whites are undersampled and have higher weights while oversampled Black, Mexican, Hispanic people have lower weights since each sampled person in these minority groups represents fewer US people.\n\n\nCode\n# Load the ggplot2 package\nlibrary(ggplot2)\n\n# Use mutate to create a 4-year weight variable and call it WTMEC4YR\nNHANESraw &lt;- NHANESraw %&gt;%\n                mutate(WTMEC4YR = WTMEC2YR / 2) \n\n# Calculate the sum of this weight variable\nNHANESraw %&gt;% summarize(total_WTMEC4YR = sum(WTMEC4YR))\n\n\n# A tibble: 1 × 1\n  total_WTMEC4YR\n           &lt;dbl&gt;\n1     304267200.\n\n\nCode\n# Plot the sample weights using boxplots, with Race1 on the x-axis\nggplot(NHANESraw, aes(x=Race1, y=WTMEC4YR))+\ngeom_boxplot()"
  },
  {
    "objectID": "projects/Project3/index.html#specify-the-survey-design",
    "href": "projects/Project3/index.html#specify-the-survey-design",
    "title": "Health Survey Data Analysis of BMI",
    "section": "3. Specify the survey design",
    "text": "3. Specify the survey design\nWe will now use the survey package to specify the complex survey design that we will use in later analyses. The NHANESraw data contains a strata variable SDMVSTRA, and a cluster id variable (also known as a primary sampling unit, PSU), SDMVPSU, that accounts for design effects of clustering. These clusters (PSUs) are nested within strata.\n\n\nCode\n# Load the survey package\nlibrary(survey)\n\n\nLe chargement a nécessité le package : grid\n\n\nLe chargement a nécessité le package : Matrix\n\n\nLe chargement a nécessité le package : survival\n\n\n\nAttachement du package : 'survey'\n\n\nL'objet suivant est masqué depuis 'package:graphics':\n\n    dotchart\n\n\nCode\n# Specify the survey design\nnhanes_design &lt;- svydesign(\n    data = NHANESraw,\n    strata = ~SDMVSTRA,\n    id = ~SDMVPSU,\n    nest = TRUE,\n    weights = ~WTMEC4YR)\n\n# Print a summary of this design\nsummary(nhanes_design)\n\n\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (62) clusters.\nsvydesign(data = NHANESraw, strata = ~SDMVSTRA, id = ~SDMVPSU, \n    nest = TRUE, weights = ~WTMEC4YR)\nProbabilities:\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n8.986e-06 5.664e-05 1.054e-04       Inf 1.721e-04       Inf \nStratum Sizes: \n            75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91\nobs        803 785 823 829 696 751 696 724 713 683 592 946 598 647 251 862 998\ndesign.PSU   2   2   2   2   2   2   2   2   2   2   2   3   2   2   2   3   3\nactual.PSU   2   2   2   2   2   2   2   2   2   2   2   3   2   2   2   3   3\n            92  93  94  95  96  97  98  99 100 101 102 103\nobs        875 602 688 722 676 608 708 682 700 715 624 296\ndesign.PSU   3   2   2   2   2   2   2   2   2   2   2   2\nactual.PSU   3   2   2   2   2   2   2   2   2   2   2   2\nData variables:\n [1] \"ID\"               \"SurveyYr\"         \"Gender\"           \"Age\"             \n [5] \"AgeMonths\"        \"Race1\"            \"Race3\"            \"Education\"       \n [9] \"MaritalStatus\"    \"HHIncome\"         \"HHIncomeMid\"      \"Poverty\"         \n[13] \"HomeRooms\"        \"HomeOwn\"          \"Work\"             \"Weight\"          \n[17] \"Length\"           \"HeadCirc\"         \"Height\"           \"BMI\"             \n[21] \"BMICatUnder20yrs\" \"BMI_WHO\"          \"Pulse\"            \"BPSysAve\"        \n[25] \"BPDiaAve\"         \"BPSys1\"           \"BPDia1\"           \"BPSys2\"          \n[29] \"BPDia2\"           \"BPSys3\"           \"BPDia3\"           \"Testosterone\"    \n[33] \"DirectChol\"       \"TotChol\"          \"UrineVol1\"        \"UrineFlow1\"      \n[37] \"UrineVol2\"        \"UrineFlow2\"       \"Diabetes\"         \"DiabetesAge\"     \n[41] \"HealthGen\"        \"DaysPhysHlthBad\"  \"DaysMentHlthBad\"  \"LittleInterest\"  \n[45] \"Depressed\"        \"nPregnancies\"     \"nBabies\"          \"Age1stBaby\"      \n[49] \"SleepHrsNight\"    \"SleepTrouble\"     \"PhysActive\"       \"PhysActiveDays\"  \n[53] \"TVHrsDay\"         \"CompHrsDay\"       \"TVHrsDayChild\"    \"CompHrsDayChild\" \n[57] \"Alcohol12PlusYr\"  \"AlcoholDay\"       \"AlcoholYear\"      \"SmokeNow\"        \n[61] \"Smoke100\"         \"SmokeAge\"         \"Marijuana\"        \"AgeFirstMarij\"   \n[65] \"RegularMarij\"     \"AgeRegMarij\"      \"HardDrugs\"        \"SexEver\"         \n[69] \"SexAge\"           \"SexNumPartnLife\"  \"SexNumPartYear\"   \"SameSex\"         \n[73] \"SexOrientation\"   \"WTINT2YR\"         \"WTMEC2YR\"         \"SDMVPSU\"         \n[77] \"SDMVSTRA\"         \"PregnantNow\"      \"WTMEC4YR\""
  },
  {
    "objectID": "projects/Project3/index.html#subset-the-data",
    "href": "projects/Project3/index.html#subset-the-data",
    "title": "Health Survey Data Analysis of BMI",
    "section": "4. Subset the data",
    "text": "4. Subset the data\nAnalysis of survey data requires careful consideration of the sampling design and weights at every step. Something as simple as filtering the data becomes complicated when weights are involved.\nWhen we wish to examine a subset of the data (i.e. the subpopulation of adult Hispanics with diabetes, or pregnant women), we must explicitly specify this in the design. We cannot simply remove that subset of the data through filtering the raw data because the survey weights will no longer be correct and will not add up to the full US population.\nBMI categories are different for children and young adults younger than 20 so we will subset the data to only analyze adults of at least 20 years of age.\n\n\nCode\n# Select adults of Age &gt;= 20 with subset\nnhanes_adult &lt;- nhanes_design%&gt;%\n                subset(Age &gt;=20)\n\n# Print a summary of this subset\nsummary(nhanes_adult)\n\n\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (62) clusters.\nsubset(., Age &gt;= 20)\nProbabilities:\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n8.986e-06 4.303e-05 8.107e-05       Inf 1.240e-04       Inf \nStratum Sizes: \n            75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91\nobs        471 490 526 500 410 464 447 400 411 395 357 512 327 355 153 509 560\ndesign.PSU   2   2   2   2   2   2   2   2   2   2   2   3   2   2   2   3   3\nactual.PSU   2   2   2   2   2   2   2   2   2   2   2   3   2   2   2   3   3\n            92  93  94  95  96  97  98  99 100 101 102 103\nobs        483 376 368 454 362 315 414 409 377 460 308 165\ndesign.PSU   3   2   2   2   2   2   2   2   2   2   2   2\nactual.PSU   3   2   2   2   2   2   2   2   2   2   2   2\nData variables:\n [1] \"ID\"               \"SurveyYr\"         \"Gender\"           \"Age\"             \n [5] \"AgeMonths\"        \"Race1\"            \"Race3\"            \"Education\"       \n [9] \"MaritalStatus\"    \"HHIncome\"         \"HHIncomeMid\"      \"Poverty\"         \n[13] \"HomeRooms\"        \"HomeOwn\"          \"Work\"             \"Weight\"          \n[17] \"Length\"           \"HeadCirc\"         \"Height\"           \"BMI\"             \n[21] \"BMICatUnder20yrs\" \"BMI_WHO\"          \"Pulse\"            \"BPSysAve\"        \n[25] \"BPDiaAve\"         \"BPSys1\"           \"BPDia1\"           \"BPSys2\"          \n[29] \"BPDia2\"           \"BPSys3\"           \"BPDia3\"           \"Testosterone\"    \n[33] \"DirectChol\"       \"TotChol\"          \"UrineVol1\"        \"UrineFlow1\"      \n[37] \"UrineVol2\"        \"UrineFlow2\"       \"Diabetes\"         \"DiabetesAge\"     \n[41] \"HealthGen\"        \"DaysPhysHlthBad\"  \"DaysMentHlthBad\"  \"LittleInterest\"  \n[45] \"Depressed\"        \"nPregnancies\"     \"nBabies\"          \"Age1stBaby\"      \n[49] \"SleepHrsNight\"    \"SleepTrouble\"     \"PhysActive\"       \"PhysActiveDays\"  \n[53] \"TVHrsDay\"         \"CompHrsDay\"       \"TVHrsDayChild\"    \"CompHrsDayChild\" \n[57] \"Alcohol12PlusYr\"  \"AlcoholDay\"       \"AlcoholYear\"      \"SmokeNow\"        \n[61] \"Smoke100\"         \"SmokeAge\"         \"Marijuana\"        \"AgeFirstMarij\"   \n[65] \"RegularMarij\"     \"AgeRegMarij\"      \"HardDrugs\"        \"SexEver\"         \n[69] \"SexAge\"           \"SexNumPartnLife\"  \"SexNumPartYear\"   \"SameSex\"         \n[73] \"SexOrientation\"   \"WTINT2YR\"         \"WTMEC2YR\"         \"SDMVPSU\"         \n[77] \"SDMVSTRA\"         \"PregnantNow\"      \"WTMEC4YR\"        \n\n\nCode\n# Compare the number of observations in the full data to the adult data\nnrow(nhanes_adult)\n\n\n[1] 11778\n\n\nCode\nnrow(nhanes_design)\n\n\n[1] 20293"
  },
  {
    "objectID": "projects/Project3/index.html#visualizing-bmi",
    "href": "projects/Project3/index.html#visualizing-bmi",
    "title": "Health Survey Data Analysis of BMI",
    "section": "5. Visualizing BMI",
    "text": "5. Visualizing BMI\nWe let svydesign() do its magic, but how does this help us learn about the full US population? With survey methods, we can use the sampling weights to estimate the true distributions of measurements within the entire population. This works for many statistics such as means, proportions, and standard deviations.\nWe’ll use survey methods to estimate average BMI in the US adult population and also to draw a weighted histogram of the distribution.\n\n\nCode\n# Calculate the mean BMI in NHANESraw\nbmi_mean_raw &lt;- NHANESraw %&gt;% \n    filter(Age &gt;= 20) %&gt;%\n    summarize(avg.BMI = mean(BMI, na.rm=TRUE))\nbmi_mean_raw\n\n\n# A tibble: 1 × 1\n  avg.BMI\n    &lt;dbl&gt;\n1    29.0\n\n\nCode\n# Calculate the survey-weighted mean BMI of US adults\nbmi_mean &lt;- svymean(~BMI, design = nhanes_adult, na.rm = TRUE)\nbmi_mean\n\n\n      mean     SE\nBMI 28.734 0.1235\n\n\nCode\n# Draw a weighted histogram of BMI in the US population\nNHANESraw %&gt;% \n  filter(Age &gt;= 20) %&gt;%\n    ggplot(mapping=aes(x=BMI, weight=WTMEC4YR)) + \n    geom_histogram()+\n    geom_vline(xintercept = coef(bmi_mean), color=\"red\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 547 rows containing non-finite outside the scale range\n(`stat_bin()`)."
  },
  {
    "objectID": "projects/Project3/index.html#is-bmi-lower-in-physically-active-people",
    "href": "projects/Project3/index.html#is-bmi-lower-in-physically-active-people",
    "title": "Health Survey Data Analysis of BMI",
    "section": "6. Is BMI lower in physically active people?",
    "text": "6. Is BMI lower in physically active people?\nThe distribution of BMI looks to be about what we might expect with most people under 40 kg/m2 and a slight positive skewness because a few people have much higher BMI. Now to the question of interest: does the distribution of BMI differ between people who are physically active versus those who are not physically active? We can visually compare BMI with a boxplot as well as formally test for a difference in mean BMI.\n\n\nCode\n# Load the broom library\nlibrary(broom)\n\n# Make a boxplot of BMI stratified by physically active status\nNHANESraw %&gt;% \n  filter(Age&gt;=20) %&gt;%\n    ggplot(mapping=aes(x=PhysActive, y= BMI, weight=WTMEC4YR))+\n    geom_boxplot()\n\n\nWarning: Removed 547 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\nWarning: Computation failed in `stat_boxplot()`.\nCaused by error in `loadNamespace()`:\n! aucun package nommé 'quantreg' n'est trouvé\n\n\n\n\n\n\n\n\n\nCode\n# Conduct a t-test comparing mean BMI between physically active status\nsurvey_ttest &lt;- svyttest(BMI~PhysActive, design = nhanes_adult)\n\n# Use broom to show the tidy results\ntidy(survey_ttest)\n\n\n# A tibble: 1 × 8\n  estimate statistic  p.value parameter conf.low conf.high method    alternative\n     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;      \n1    -1.85     -9.72 4.56e-11        32    -2.23     -1.46 Design-b… two.sided"
  },
  {
    "objectID": "projects/Project3/index.html#could-there-be-confounding-by-smoking-part-1",
    "href": "projects/Project3/index.html#could-there-be-confounding-by-smoking-part-1",
    "title": "Health Survey Data Analysis of BMI",
    "section": "7. Could there be confounding by smoking? (part 1)",
    "text": "7. Could there be confounding by smoking? (part 1)\nThe relationship between physical activity and BMI is likely not so simple as “if you exercise you will lower your BMI.” In fact, many other lifestyle or demographic variables could be confounding this relationship. One such variable could be smoking status. If someone smokes, is he or she more or less likely to be physically active? Are smokers more likely to have higher or lower BMI? We can examine these relationships in the survey data. Note that many people chose not to answer the smoking question, so we reduce our sample size when looking at this data.\nFirst, let’s look at the relationship between smoking and physical activity.\n\n\nCode\n# Estimate the proportion who are physically active by current smoking status\nphys_by_smoke &lt;- svyby(~PhysActive, by = ~SmokeNow, \n                       FUN = svymean, \n                       design = nhanes_adult, \n                       keep.names = FALSE)\n\n# Print the table\nphys_by_smoke\n\n\n  SmokeNow PhysActiveNo PhysActiveYes se.PhysActiveNo se.PhysActiveYes\n1       No    0.4566990     0.5433010      0.01738054       0.01738054\n2      Yes    0.5885421     0.4114579      0.01163246       0.01163246\n\n\nCode\n# Plot the proportions\nggplot(data = phys_by_smoke, aes(SmokeNow, PhysActiveYes, fill = SmokeNow)) +\n geom_col()+\n    ylab(\"Proportion Physically Active\")"
  },
  {
    "objectID": "projects/Project3/index.html#could-there-be-confounding-by-smoking-part-2",
    "href": "projects/Project3/index.html#could-there-be-confounding-by-smoking-part-2",
    "title": "Health Survey Data Analysis of BMI",
    "section": "8. Could there be confounding by smoking? (part 2)",
    "text": "8. Could there be confounding by smoking? (part 2)\nNow let’s examine the relationship between smoking with BMI.\n\n\nCode\n# Estimate mean BMI by current smoking status\nBMI_by_smoke &lt;- svyby(~BMI, by = ~SmokeNow, \n                       FUN = svymean, \n                       design = nhanes_adult, \n                       na.rm = TRUE)\nBMI_by_smoke\n\n\n    SmokeNow      BMI        se\nNo        No 29.25734 0.1915138\nYes      Yes 27.74873 0.1652377\n\n\nCode\n# Plot the distribution of BMI by current smoking status\nNHANESraw %&gt;% \n  filter(Age&gt;=20, !is.na(SmokeNow)) %&gt;% \n    ggplot(mapping=aes(x=SmokeNow, y= BMI, weight=WTMEC4YR))+\n    geom_boxplot()\n\n\nWarning: Removed 244 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\nWarning: Computation failed in `stat_boxplot()`.\nCaused by error in `loadNamespace()`:\n! aucun package nommé 'quantreg' n'est trouvé"
  },
  {
    "objectID": "projects/Project3/index.html#add-smoking-in-the-mix",
    "href": "projects/Project3/index.html#add-smoking-in-the-mix",
    "title": "Health Survey Data Analysis of BMI",
    "section": "9. Add smoking in the mix",
    "text": "9. Add smoking in the mix\nWe saw that people who smoke are less likely to be physically active and have a higher BMI on average. We also saw that people who are physically active have a lower BMI on average. How do these seemingly conflicting associations work together? To get a better sense of what’s going on, we can compare BMI by physical activity stratified by smoking status.\n\n\nCode\n# Plot the distribution of BMI by smoking and physical activity status\nNHANESraw %&gt;% \n  filter(Age&gt;=20) %&gt;%\n    ggplot(mapping=aes(x=SmokeNow, y= BMI, weight=WTMEC4YR, color=PhysActive))+\n    geom_boxplot()\n\n\nWarning: Removed 547 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\nWarning: Computation failed in `stat_boxplot()`.\nCaused by error in `loadNamespace()`:\n! aucun package nommé 'quantreg' n'est trouvé"
  },
  {
    "objectID": "projects/Project3/index.html#incorporate-possible-confounding-in-the-model",
    "href": "projects/Project3/index.html#incorporate-possible-confounding-in-the-model",
    "title": "Health Survey Data Analysis of BMI",
    "section": "10. Incorporate possible confounding in the model",
    "text": "10. Incorporate possible confounding in the model\nIn the above plot, we see that people who are physically active tend to have lower BMI no matter their smoking status, and this is true even if they didn’t answer the question. However, we also see that smokers have lower BMI in general. Also, looking closely we see the difference in BMI comparing physically active people to non-physically active people is slightly smaller in smokers than in non-smokers.\nPreviously, we used a simple t-test to compare mean BMI in physically active people and non-physically active people. In order to adjust for smoking status, as well as other possible confounders or predictors of BMI, we can use a linear regression model with multiple independent variables. When using survey data, we use a weighted linear regression method which is a special case of generalized linear models (GLMs).\n\n\nCode\n# Fit a multiple regression model\nmod1 &lt;- svyglm(BMI ~ SmokeNow * PhysActive, design = nhanes_adult)\n\n# Tidy the model results\ntidy_mod1 &lt;- tidy(mod1)\ntidy_mod1\n\n\n# A tibble: 4 × 5\n  term                      estimate std.error statistic  p.value\n  &lt;chr&gt;                        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                  30.5      0.210    146.   2.62e-44\n2 SmokeNowYes                  -2.24     0.267     -8.40 2.26e- 9\n3 PhysActiveYes                -2.35     0.236     -9.97 4.96e-11\n4 SmokeNowYes:PhysActiveYes     1.00     0.344      2.92 6.52e- 3\n\n\nCode\n# Calculate expected mean difference in BMI for activity within non-smokers\ndiff_non_smoke &lt;- tidy_mod1 %&gt;% \n    filter(term == \"PhysActiveYes\") %&gt;% \n    select(estimate)\ndiff_non_smoke\n\n\n# A tibble: 1 × 1\n  estimate\n     &lt;dbl&gt;\n1    -2.35\n\n\nCode\n# Calculate expected mean difference in BMI for activity within smokers\ndiff_smoke &lt;- tidy_mod1 %&gt;% \n   filter(term %in% c('PhysActiveYes','SmokeNowYes:PhysActiveYes')) %&gt;% \n    summarize(estimate = sum(estimate))\ndiff_smoke\n\n\n# A tibble: 1 × 1\n  estimate\n     &lt;dbl&gt;\n1    -1.35"
  },
  {
    "objectID": "projects/Project3/index.html#what-does-it-all-mean",
    "href": "projects/Project3/index.html#what-does-it-all-mean",
    "title": "Health Survey Data Analysis of BMI",
    "section": "11. What does it all mean?",
    "text": "11. What does it all mean?\nWe fit a linear regression model where the association of physical activity with BMI could vary by smoking status. The interaction between physical activity and smoking has a small p-value, which suggests the association does vary by smoking status. The difference between physically active and non-physically active people is larger in magnitude in the non-smoker population.\nWe should check the model fit and technical assumptions of our regression model. Then, we can conclude that physically active people tend to have lower BMI, as do smokers. Although they have similar effect sizes, we probably wouldn’t want to recommend smoking along with exercise!\nIn order to determine whether physical activity causes lower BMI, we would need to use causal inference methods or a randomized control study. We can adjust for other possible confounders in our regression model to determine if physical activity is still associated with BMI, but we fall short of confirming that physical activity itself can lower one’s BMI.\n\n\nCode\n# Adjust mod1 for other possible confounders\nmod2 &lt;- svyglm(BMI ~ PhysActive*SmokeNow + Race1 + Alcohol12PlusYr + Gender, \n               design = nhanes_adult)\n\n# Tidy the output\ntidy(mod2)\n\n\n# A tibble: 10 × 5\n   term                      estimate std.error statistic  p.value\n   &lt;chr&gt;                        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                 33.2       0.316   105.    1.75e-33\n 2 PhysActiveYes               -2.11      0.273    -7.75  5.56e- 8\n 3 SmokeNowYes                 -2.23      0.303    -7.34  1.40e- 7\n 4 Race1Hispanic               -1.47      0.420    -3.49  1.88e- 3\n 5 Race1Mexican                -0.191     0.464    -0.412 6.84e- 1\n 6 Race1White                  -2.08      0.320    -6.49  1.04e- 6\n 7 Race1Other                  -3.11      0.620    -5.01  4.09e- 5\n 8 Alcohol12PlusYrYes          -0.855     0.358    -2.39  2.50e- 2\n 9 Gendermale                  -0.256     0.230    -1.11  2.78e- 1\n10 PhysActiveYes:SmokeNowYes    0.737     0.387     1.90  6.92e- 2"
  },
  {
    "objectID": "projects/Project4/index.html",
    "href": "projects/Project4/index.html",
    "title": "What Your Heart Rate Is Telling You",
    "section": "",
    "text": "Millions of people are getting some sort of heart disease every year and heart disease is the biggest killer of both men and women in the United States and around the world. Statistical analysis has identified many risk factors associated with heart disease such as age, blood pressure, total cholesterol, diabetes, hypertension, family history of heart disease, obesity, lack of physical exercise, etc. In this notebook, we’re going to run statistical testings and regression models using the Cleveland heart disease dataset to assess one particular factor – maximum heart rate one can achieve during exercise and how it is associated with a higher likelihood of getting heart disease.\n\n\nCode\n# Read datasets Cleveland_hd.csv into hd_data\nhd_data &lt;- read.csv(\"Cleveland_hd.csv\")\n\n# take a look at the first 5 rows of hd_data\nhead(hd_data, n=5)\n\n\n  age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal\n1  63   1  1      145  233   1       2     150     0     2.3     3  0    6\n2  67   1  4      160  286   0       2     108     1     1.5     2  3    3\n3  67   1  4      120  229   0       2     129     1     2.6     2  2    7\n4  37   1  3      130  250   0       0     187     0     3.5     3  0    3\n5  41   0  2      130  204   0       2     172     0     1.4     1  0    3\n  class\n1     0\n2     2\n3     1\n4     0\n5     0"
  },
  {
    "objectID": "projects/Project4/index.html#heart-disease-and-potential-risk-factors",
    "href": "projects/Project4/index.html#heart-disease-and-potential-risk-factors",
    "title": "What Your Heart Rate Is Telling You",
    "section": "",
    "text": "Millions of people are getting some sort of heart disease every year and heart disease is the biggest killer of both men and women in the United States and around the world. Statistical analysis has identified many risk factors associated with heart disease such as age, blood pressure, total cholesterol, diabetes, hypertension, family history of heart disease, obesity, lack of physical exercise, etc. In this notebook, we’re going to run statistical testings and regression models using the Cleveland heart disease dataset to assess one particular factor – maximum heart rate one can achieve during exercise and how it is associated with a higher likelihood of getting heart disease.\n\n\nCode\n# Read datasets Cleveland_hd.csv into hd_data\nhd_data &lt;- read.csv(\"Cleveland_hd.csv\")\n\n# take a look at the first 5 rows of hd_data\nhead(hd_data, n=5)\n\n\n  age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal\n1  63   1  1      145  233   1       2     150     0     2.3     3  0    6\n2  67   1  4      160  286   0       2     108     1     1.5     2  3    3\n3  67   1  4      120  229   0       2     129     1     2.6     2  2    7\n4  37   1  3      130  250   0       0     187     0     3.5     3  0    3\n5  41   0  2      130  204   0       2     172     0     1.4     1  0    3\n  class\n1     0\n2     2\n3     1\n4     0\n5     0"
  },
  {
    "objectID": "projects/Project4/index.html#converting-diagnosis-class-into-outcome-variable",
    "href": "projects/Project4/index.html#converting-diagnosis-class-into-outcome-variable",
    "title": "What Your Heart Rate Is Telling You",
    "section": "2. Converting diagnosis class into outcome variable",
    "text": "2. Converting diagnosis class into outcome variable\nWe noticed that the outcome variable class has more than two levels. According to the codebook, any non-zero values can be coded as an “event.” Let’s create a new variable called hd to represent a binary 1/0 outcome.\nThere are a few other categorical/discrete variables in the dataset. Let’s also convert sex into ‘factor’ type for next step analysis. Otherwise, R will treat them as continuous by default.\n\n\nCode\n# load the tidyverse package\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\n# Use the 'mutate' function from dplyr to recode our data\nhd_data %&gt;% mutate(hd = ifelse(class &gt; 0, 1, 0))-&gt; hd_data\n\n# recode sex using mutate function and save as hd_data\nhd_data %&gt;% mutate(sex = factor(sex, levels = 0:1, labels = c(\"Female\",\"Male\")))-&gt; hd_data"
  },
  {
    "objectID": "projects/Project4/index.html#identifying-important-clinical-variables",
    "href": "projects/Project4/index.html#identifying-important-clinical-variables",
    "title": "What Your Heart Rate Is Telling You",
    "section": "3. Identifying important clinical variables",
    "text": "3. Identifying important clinical variables\nNow, let’s use statistical tests to see which ones are related to heart disease. We can explore the associations for each variable in the dataset. Depending on the type of the data (i.e., continuous or categorical), we use t-test or chi-squared test to calculate the p-values.\nRecall, t-test is used to determine whether there is a significant difference between the means of two groups (e.g., is the mean age from group A different from the mean age from group B?). A chi-squared test for independence compares two variables in a contingency table to see if they are related. In a more general sense, it tests to see whether distributions of categorical variables differ from each another.\n\n\nCode\n# Does sex have an effect? Sex is a binary variable in this dataset,\n# so the appropriate test is chi-squared test\n#hd_sex &lt;- chisq.test(hd_datah)\n\n# Does age have an effect? Age is continuous, so we use a t-test\n#hd_age &lt;- t.test(hd_datahd)\n\n# What about thalach? Thalach is continuous, so we use a t-test\n#hd_heartrate &lt;- t.test(hd_datahd)\n\n# Print the results to see if p&lt;0.05.\n#print(hd_sex)\n#print(hd_age)\n#print(hd_heartrate)"
  },
  {
    "objectID": "projects/Project4/index.html#explore-the-associations-graphically-i",
    "href": "projects/Project4/index.html#explore-the-associations-graphically-i",
    "title": "What Your Heart Rate Is Telling You",
    "section": "4. Explore the associations graphically (i)",
    "text": "4. Explore the associations graphically (i)\nA good picture is worth a thousand words. In addition to p-values from statistical tests, we can plot the age, sex, and maximum heart rate distributions with respect to our outcome variable. This will give us a sense of both the direction and magnitude of the relationship.\nFirst, let’s plot age using a boxplot since it is a continuous variable.\n\n\nCode\n# Recode hd to be labelled\nhd_data %&gt;% mutate(hd_labelled = ifelse(hd == 1, \"Disease\", \"No Disease\")) -&gt; hd_data\n\n# age vs hd\nggplot(data = hd_data, aes(x = hd_labelled,y = age)) + geom_boxplot()"
  },
  {
    "objectID": "projects/Project4/index.html#explore-the-associations-graphically-ii",
    "href": "projects/Project4/index.html#explore-the-associations-graphically-ii",
    "title": "What Your Heart Rate Is Telling You",
    "section": "5. Explore the associations graphically (ii)",
    "text": "5. Explore the associations graphically (ii)\nNext, let’s plot sex using a barplot since it is a binary variable in this dataset.\n\n\nCode\n# sex vs hd\nggplot(data = hd_data,aes(hd_labelled, fill=sex)) + geom_bar(positio=\"fill\") + ylab(\"Sex %\")"
  },
  {
    "objectID": "projects/Project4/index.html#explore-the-associations-graphically-iii",
    "href": "projects/Project4/index.html#explore-the-associations-graphically-iii",
    "title": "What Your Heart Rate Is Telling You",
    "section": "6. Explore the associations graphically (iii)",
    "text": "6. Explore the associations graphically (iii)\nAnd finally, let’s plot thalach using a boxplot since it is a continuous variable.\n\n\nCode\n# max heart rate vs hd\nggplot(data = hd_data,aes(hd_labelled,thalach)) + geom_boxplot()"
  },
  {
    "objectID": "projects/Project4/index.html#putting-all-three-variables-in-one-model",
    "href": "projects/Project4/index.html#putting-all-three-variables-in-one-model",
    "title": "What Your Heart Rate Is Telling You",
    "section": "7. Putting all three variables in one model",
    "text": "7. Putting all three variables in one model\nThe plots and the statistical tests both confirmed that all the three variables are highly significantly associated with our outcome (p&lt;0.001 for all tests).\nIn general, we want to use multiple logistic regression when we have one binary and two or more predicting variables. The binary variable is the dependent (Y) variable; we are studying the effect that the independent (X) variables have on the probability of obtaining a particular value of the dependent variable. For example, we might want to know the effect that maximum heart rate, age, and sex have on the probability that a person will have a heart disease in the next year. The model will also tell us what the remaining effect of maximum heart rate is after we control or adjust for the effects from the other two effectors.\nThe glm() command is designed to perform generalized linear models (regressions) on binary outcome data, count data, probability data, proportion data, and many other data types. In our case, the outcome is binary following a binomial distribution.\n\n\nCode\n# use glm function from base R and specify the family argument as binomial\nmodel &lt;- glm(data = hd_data, hd~age+sex+thalach, family=\"binomial\")\n\n# extract the model summary\nsummary(model)\n\n\n\nCall:\nglm(formula = hd ~ age + sex + thalach, family = \"binomial\", \n    data = hd_data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.111610   1.607466   1.936   0.0529 .  \nage          0.031886   0.016440   1.940   0.0524 .  \nsexMale      1.491902   0.307193   4.857 1.19e-06 ***\nthalach     -0.040541   0.007073  -5.732 9.93e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 417.98  on 302  degrees of freedom\nResidual deviance: 332.85  on 299  degrees of freedom\nAIC: 340.85\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "projects/Project5/index.html",
    "href": "projects/Project5/index.html",
    "title": "Clustering Heart Disease Patient Data",
    "section": "",
    "text": "Clustering algorithms are used to group together items that are similar to one another. There are many industries where it would be beneficial and insightful to use an unsupervised learning algorithm - retailers want to group similar customers for targeted ad campaigns, biologists want to find plants that share similar characteristics, and more. We are going to explore if it would be appropriate to use some clustering algorithms to group medical patients.\nWe are going to look at anonymized patients who have been diagnosed with heart disease. Patients with similar characteristics might respond to the same treatments, and doctors would benefit from learning about the outcomes of patients similar to those they are treating. The data we are analyzing comes from the V.A. Medical Center in Long Beach, CA. For more information, see here.\nBefore beginning a project, it is important to get an idea of what the patient data looks like. In addition, the clustering algorithms used below require that the data be numeric, so it is necessary to ensure the patient data doesn’t need any transformations. You will also be brushing up on your base R skills for some analysis.\n\n\nCode\n# loading the data\nheart_disease = read.csv(\"heart_disease_patients.csv\")\n\n# print the first ten rows of the data set\nhead(heart_disease, 10)\n\n\n   id age sex cp trestbps chol fbs restecg thalach exang oldpeak slope\n1   1  63   1  1      145  233   1       2     150     0     2.3     3\n2   2  67   1  4      160  286   0       2     108     1     1.5     2\n3   3  67   1  4      120  229   0       2     129     1     2.6     2\n4   4  37   1  3      130  250   0       0     187     0     3.5     3\n5   5  41   0  2      130  204   0       2     172     0     1.4     1\n6   6  56   1  2      120  236   0       0     178     0     0.8     1\n7   7  62   0  4      140  268   0       2     160     0     3.6     3\n8   8  57   0  4      120  354   0       0     163     1     0.6     1\n9   9  63   1  4      130  254   0       2     147     0     1.4     2\n10 10  53   1  4      140  203   1       2     155     1     3.1     3\n\n\nCode\n# check that only numeric variables\nlapply(heart_disease, class)\n\n\n$id\n[1] \"integer\"\n\n$age\n[1] \"integer\"\n\n$sex\n[1] \"integer\"\n\n$cp\n[1] \"integer\"\n\n$trestbps\n[1] \"integer\"\n\n$chol\n[1] \"integer\"\n\n$fbs\n[1] \"integer\"\n\n$restecg\n[1] \"integer\"\n\n$thalach\n[1] \"integer\"\n\n$exang\n[1] \"integer\"\n\n$oldpeak\n[1] \"numeric\"\n\n$slope\n[1] \"integer\""
  },
  {
    "objectID": "projects/Project5/index.html#targeting-treatment-for-heart-disease-patients",
    "href": "projects/Project5/index.html#targeting-treatment-for-heart-disease-patients",
    "title": "Clustering Heart Disease Patient Data",
    "section": "",
    "text": "Clustering algorithms are used to group together items that are similar to one another. There are many industries where it would be beneficial and insightful to use an unsupervised learning algorithm - retailers want to group similar customers for targeted ad campaigns, biologists want to find plants that share similar characteristics, and more. We are going to explore if it would be appropriate to use some clustering algorithms to group medical patients.\nWe are going to look at anonymized patients who have been diagnosed with heart disease. Patients with similar characteristics might respond to the same treatments, and doctors would benefit from learning about the outcomes of patients similar to those they are treating. The data we are analyzing comes from the V.A. Medical Center in Long Beach, CA. For more information, see here.\nBefore beginning a project, it is important to get an idea of what the patient data looks like. In addition, the clustering algorithms used below require that the data be numeric, so it is necessary to ensure the patient data doesn’t need any transformations. You will also be brushing up on your base R skills for some analysis.\n\n\nCode\n# loading the data\nheart_disease = read.csv(\"heart_disease_patients.csv\")\n\n# print the first ten rows of the data set\nhead(heart_disease, 10)\n\n\n   id age sex cp trestbps chol fbs restecg thalach exang oldpeak slope\n1   1  63   1  1      145  233   1       2     150     0     2.3     3\n2   2  67   1  4      160  286   0       2     108     1     1.5     2\n3   3  67   1  4      120  229   0       2     129     1     2.6     2\n4   4  37   1  3      130  250   0       0     187     0     3.5     3\n5   5  41   0  2      130  204   0       2     172     0     1.4     1\n6   6  56   1  2      120  236   0       0     178     0     0.8     1\n7   7  62   0  4      140  268   0       2     160     0     3.6     3\n8   8  57   0  4      120  354   0       0     163     1     0.6     1\n9   9  63   1  4      130  254   0       2     147     0     1.4     2\n10 10  53   1  4      140  203   1       2     155     1     3.1     3\n\n\nCode\n# check that only numeric variables\nlapply(heart_disease, class)\n\n\n$id\n[1] \"integer\"\n\n$age\n[1] \"integer\"\n\n$sex\n[1] \"integer\"\n\n$cp\n[1] \"integer\"\n\n$trestbps\n[1] \"integer\"\n\n$chol\n[1] \"integer\"\n\n$fbs\n[1] \"integer\"\n\n$restecg\n[1] \"integer\"\n\n$thalach\n[1] \"integer\"\n\n$exang\n[1] \"integer\"\n\n$oldpeak\n[1] \"numeric\"\n\n$slope\n[1] \"integer\""
  },
  {
    "objectID": "projects/Project5/index.html#quantifying-patient-differences",
    "href": "projects/Project5/index.html#quantifying-patient-differences",
    "title": "Clustering Heart Disease Patient Data",
    "section": "2. Quantifying patient differences",
    "text": "2. Quantifying patient differences\nIt is important to conduct some exploratory data analysis to familiarize ourselves with the data before clustering. This will help us learn more about the variables and make an informed decision about whether we should scale the data. Because k-means and hierarchical clustering measures similarity between points using a distance formula, it can place extra emphasis on certain variables that have a larger scale and thus larger differences between points.\nExploratory data analysis helps us to understand the characteristics of the patients in the data. We need to get an idea of the value ranges of the variables and their distributions. This will also be helpful when we evaluate the clusters of patients from the algorithms. Are there more patients of one gender? What might an outlier look like?\n\n\nCode\n# evidence that the data should be scaled?\nsummary(heart_disease)\n\n\n       id             age             sex               cp       \n Min.   :  1.0   Min.   :29.00   Min.   :0.0000   Min.   :1.000  \n 1st Qu.: 76.5   1st Qu.:48.00   1st Qu.:0.0000   1st Qu.:3.000  \n Median :152.0   Median :56.00   Median :1.0000   Median :3.000  \n Mean   :152.0   Mean   :54.44   Mean   :0.6799   Mean   :3.158  \n 3rd Qu.:227.5   3rd Qu.:61.00   3rd Qu.:1.0000   3rd Qu.:4.000  \n Max.   :303.0   Max.   :77.00   Max.   :1.0000   Max.   :4.000  \n    trestbps          chol            fbs            restecg      \n Min.   : 94.0   Min.   :126.0   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:120.0   1st Qu.:211.0   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :130.0   Median :241.0   Median :0.0000   Median :1.0000  \n Mean   :131.7   Mean   :246.7   Mean   :0.1485   Mean   :0.9901  \n 3rd Qu.:140.0   3rd Qu.:275.0   3rd Qu.:0.0000   3rd Qu.:2.0000  \n Max.   :200.0   Max.   :564.0   Max.   :1.0000   Max.   :2.0000  \n    thalach          exang           oldpeak         slope      \n Min.   : 71.0   Min.   :0.0000   Min.   :0.00   Min.   :1.000  \n 1st Qu.:133.5   1st Qu.:0.0000   1st Qu.:0.00   1st Qu.:1.000  \n Median :153.0   Median :0.0000   Median :0.80   Median :2.000  \n Mean   :149.6   Mean   :0.3267   Mean   :1.04   Mean   :1.601  \n 3rd Qu.:166.0   3rd Qu.:1.0000   3rd Qu.:1.60   3rd Qu.:2.000  \n Max.   :202.0   Max.   :1.0000   Max.   :6.20   Max.   :3.000  \n\n\nCode\n# remove id\nheart_disease = heart_disease[ , !(names(heart_disease) %in% c('id'))]\n\n# scaling data and saving as a data frame\nscaled = scale(heart_disease)\n\n# what does data look like now?\nsummary(scaled)\n\n\n      age               sex                cp             trestbps       \n Min.   :-2.8145   Min.   :-1.4549   Min.   :-2.2481   Min.   :-2.14149  \n 1st Qu.:-0.7124   1st Qu.:-1.4549   1st Qu.:-0.1650   1st Qu.:-0.66420  \n Median : 0.1727   Median : 0.6851   Median :-0.1650   Median :-0.09601  \n Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.00000  \n 3rd Qu.: 0.7259   3rd Qu.: 0.6851   3rd Qu.: 0.8765   3rd Qu.: 0.47218  \n Max.   : 2.4961   Max.   : 0.6851   Max.   : 0.8765   Max.   : 3.88132  \n      chol              fbs             restecg             thalach       \n Min.   :-2.3310   Min.   :-0.4169   Min.   :-0.995103   Min.   :-3.4364  \n 1st Qu.:-0.6894   1st Qu.:-0.4169   1st Qu.:-0.995103   1st Qu.:-0.7041  \n Median :-0.1100   Median :-0.4169   Median : 0.009951   Median : 0.1483  \n Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.000000   Mean   : 0.0000  \n 3rd Qu.: 0.5467   3rd Qu.:-0.4169   3rd Qu.: 1.015005   3rd Qu.: 0.7166  \n Max.   : 6.1283   Max.   : 2.3905   Max.   : 1.015005   Max.   : 2.2904  \n     exang            oldpeak            slope        \n Min.   :-0.6955   Min.   :-0.8954   Min.   :-0.9747  \n 1st Qu.:-0.6955   1st Qu.:-0.8954   1st Qu.:-0.9747  \n Median :-0.6955   Median :-0.2064   Median : 0.6480  \n Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 1.4331   3rd Qu.: 0.4827   3rd Qu.: 0.6480  \n Max.   : 1.4331   Max.   : 4.4445   Max.   : 2.2708"
  },
  {
    "objectID": "projects/Project5/index.html#lets-start-grouping-patients",
    "href": "projects/Project5/index.html#lets-start-grouping-patients",
    "title": "Clustering Heart Disease Patient Data",
    "section": "3. Let’s start grouping patients",
    "text": "3. Let’s start grouping patients\nOnce we’ve figured out if we need to modify the data and made any necessary changes, we can now start the clustering process. For the k-means algorithm, it is necessary to select the number of clusters in advance.\nIt is also important to make sure that your results are reproducible when conducting a statistical analysis. This means that when someone runs your code on the same data, they will get the same results as you reported. Therefore, if you’re conducting an analysis that has a random aspect, it is necessary to set a seed to ensure reproducibility.\nReproducibility is especially important since doctors will potentially be using our results to treat patients. It is vital that another analyst can see where the groups come from and be able to verify the results.\n\n\nCode\n# set the seed so that results are reproducible\nseed_val = 10\nset.seed(seed_val)\n\n# select a number of clusters\nk = 5\n\n# run the k-means algorithms\nfirst_clust = kmeans(scaled, centers = k, nstart = 1)\n\n# how many patients are in each group\nfirst_clust$size\n\n\n[1] 66 43 88 61 45"
  },
  {
    "objectID": "projects/Project5/index.html#another-round-of-k-means",
    "href": "projects/Project5/index.html#another-round-of-k-means",
    "title": "Clustering Heart Disease Patient Data",
    "section": "4. Another round of k-means",
    "text": "4. Another round of k-means\nBecause the k-means algorithm initially selects the cluster centers by randomly selecting points, different iterations of the algorithm can result in different clusters being created. If the algorithm is truly grouping together similar observations (as opposed to clustering noise), then cluster assignments will be somewhat robust between different iterations of the algorithm.\nWith regards to the heart disease data, this would mean that the same patients would be grouped together even when the algorithm is initialized at different random points. If patients are not in similar clusters with various algorithm runs, then the clustering method isn’t picking up on meaningful relationships between patients.\nWe’re going to explore how the patients are grouped together with another iteration of the k-means algorithm. We will then be able to compare the resulting groups of patients.\n\n\nCode\n# set the seed\nseed_val = 38\nset.seed(seed_val)\n\n# run the k-means algorithms\nk = 5\nsecond_clust = kmeans(scaled, k, nstart=1)\n\n# how many patients are in each group\nsecond_clust$size\n\n\n[1] 65 43 61 46 88"
  },
  {
    "objectID": "projects/Project5/index.html#comparing-patient-clusters",
    "href": "projects/Project5/index.html#comparing-patient-clusters",
    "title": "Clustering Heart Disease Patient Data",
    "section": "5. Comparing patient clusters",
    "text": "5. Comparing patient clusters\nIt is important that the clusters resulting from the k-means algorithm are stable. Even though the algorithm begins by randomly initializing the cluster centers, if the k-means algorithm is the right choice for the data, then different initializations of the algorithm will result in similar clusters.\nThe clusters from different iterations may not be exactly the same, but the clusters should be roughly the same size and have similar distributions of variables. If there is a lot of change in clusters between different iterations of the algorithm, then k-means clustering is not a good choice for the data.\nIt is not possible to validate that the clusters obtained from an algorithm are ground truth are accurate since there is no true labeling for patients. Thus, it is necessary to examine how the clusters change between different iterations of the algorithm. We’re going to use some visualizations to get an idea of the cluster stabilities. That way we can see how certain patient characteristics may have been used to group patients together.\nI\n\n\nCode\n# adding cluster assignments to the data\nheart_disease['first_clust'] = first_clust$cluster\nheart_disease['second_clust'] = second_clust$cluster\n\n# load ggplot2\nlibrary(ggplot2)\n\n# creating the plots of age and chol for the first clustering algorithm\nplot_one = ggplot(heart_disease, aes(x=age, y=chol, color=as.factor(first_clust))) + geom_point()\nplot_one \n\n\n\n\n\n\n\n\n\nCode\n# creating the plots of age and chol for the second clustering algorithm\nplot_two = ggplot(heart_disease, aes(x=age, y=chol, color=as.factor(second_clust))) + geom_point()\nplot_two"
  },
  {
    "objectID": "projects/Project5/index.html#hierarchical-clustering-another-clustering-approach",
    "href": "projects/Project5/index.html#hierarchical-clustering-another-clustering-approach",
    "title": "Clustering Heart Disease Patient Data",
    "section": "6. Hierarchical clustering: another clustering approach",
    "text": "6. Hierarchical clustering: another clustering approach\nAn alternative to k-means clustering is hierarchical clustering. This method works well when the data has a nested structure. It is possible that the data from heart disease patients follows this type of structure. For example, if men are more likely to exhibit certain characteristics, those characteristics might be nested inside the gender variable. Hierarchical clustering also does not require the number of clusters to be selected prior to running the algorithm.\nClusters can be selected by using the dendrogram. The dendrogram allows one to see how similar observations are to one another and are useful in selecting the number of clusters to group the data. It is now time for us to see how hierarchical clustering groups the data.\n\n\nCode\n# executing hierarchical clustering with complete linkage\nhier_clust_1 = hclust(dist(scaled), method= 'complete')\n\n# printing the dendrogram\nplot(hier_clust_1)\n\n\n\n\n\n\n\n\n\nCode\n# getting cluster assignments based on number of selected clusters\nhc_1_assign &lt;- cutree(hier_clust_1, 5)"
  },
  {
    "objectID": "projects/Project5/index.html#hierarchical-clustering-round-two",
    "href": "projects/Project5/index.html#hierarchical-clustering-round-two",
    "title": "Clustering Heart Disease Patient Data",
    "section": "7. Hierarchical clustering round two",
    "text": "7. Hierarchical clustering round two\nIn hierarchical clustering, there are multiple ways to measure the dissimilarity between clusters of observations. Complete linkage records the largest dissimilarity between any two points in the two clusters being compared. On the other hand, single linkage is the smallest dissimilarity between any two points in the clusters. Different linkages will result in different clusters being formed.\nWe want to explore different algorithms to group our heart disease patients. The best way to measure dissimilarity between patients could be to look at the smallest difference between patients and minimize that difference when grouping together clusters. It is always a good idea to explore different dissimilarity measures. Let’s implement hierarchical clustering using a new linkage function.\n\n\nCode\n# executing hierarchical clustering with complete linkage\nhier_clust_2 = hclust(dist(scaled), method='single')\n\n# printing the dendrogram\nplot(hier_clust_2)\n\n\n\n\n\n\n\n\n\nCode\n# getting cluster assignments based on number of selected clusters\nhc_2_assign &lt;- cutree(hier_clust_2,5)"
  },
  {
    "objectID": "projects/Project5/index.html#comparing-clustering-results",
    "href": "projects/Project5/index.html#comparing-clustering-results",
    "title": "Clustering Heart Disease Patient Data",
    "section": "8. Comparing clustering results",
    "text": "8. Comparing clustering results\nThe doctors are interested in grouping similar patients together in order to determine appropriate treatments. Therefore, they want to have clusters with more than a few patients to see different treatment options. While it is possible for a patient to be in a cluster by themselves, this means that the treatment they received might not be recommended for someone else in the group.\nAs with the k-means algorithm, the way to evaluate the clusters is to investigate which patients are being grouped together. Are there patterns evident in the cluster assignments or do they seem to be groups of noise? We’re going to examine the clusters resulting from the two hierarchical algorithms.\n\n\nCode\n# adding assignments of chosen hierarchical linkage\nheart_disease['hc_clust'] = hc_1_assign\n\n# remove 'sex', 'first_clust', and 'second_clust' variables\nhd_simple = heart_disease[, !(names(heart_disease) %in% c('sex', 'first_clust', 'second_clust'))]\n\n# getting mean and standard deviation summary statistics\nclust_summary = do.call(data.frame, aggregate(. ~hc_clust, data = hd_simple, function(x) c(avg = mean(x), sd = sd(x))))\nclust_summary\n\n\n  hc_clust  age.avg   age.sd   cp.avg     cp.sd trestbps.avg trestbps.sd\n1        1 51.41667 8.540979 2.783333 0.9470625     129.1389    15.93800\n2        2 58.11111 7.754246 3.763889 0.6165112     130.0417    13.90657\n3        3 61.00000 3.908034 3.916667 0.2886751     168.5000    17.45904\n4        4 59.00000 9.203580 3.571429 0.8501112     134.7714    18.64070\n5        5 64.75000 2.061553 3.250000 0.5000000     138.7500    18.42779\n  chol.avg  chol.sd   fbs.avg    fbs.sd restecg.avg restecg.sd thalach.avg\n1 239.8722 42.29228 0.1222222 0.3284559   0.8444444  0.9905826    161.5722\n2 253.2222 49.74476 0.1805556 0.3873488   1.4027778  0.9140488    135.5417\n3 284.9167 53.00336 0.3333333 0.4923660   1.2500000  0.9653073    147.7500\n4 233.8571 49.67136 0.1428571 0.3550358   0.6857143  0.9321521    116.8857\n5 433.7500 89.93470 0.2500000 0.5000000   2.0000000  0.0000000    156.2500\n  thalach.sd  exang.avg  exang.sd oldpeak.avg oldpeak.sd slope.avg  slope.sd\n1  15.779214 0.07777778 0.2685686    0.555000  0.7847196  1.388889 0.5730336\n2  17.991342 0.81944444 0.3873488    1.451389  1.0804268  1.750000 0.5240686\n3  13.157266 0.75000000 0.4522670    2.316667  1.4708274  2.166667 0.5773503\n4  17.842071 0.48571429 0.5070926    2.240000  1.3856831  2.200000 0.4058397\n5   3.774917 0.00000000 0.0000000    1.100000  0.3829708  1.500000 0.5773503"
  },
  {
    "objectID": "projects/Project5/index.html#visualizing-the-cluster-contents",
    "href": "projects/Project5/index.html#visualizing-the-cluster-contents",
    "title": "Clustering Heart Disease Patient Data",
    "section": "9. Visualizing the cluster contents",
    "text": "9. Visualizing the cluster contents\nIn addition to looking at the distributions of variables in each of the hierarchical clustering run, we will make visualizations to evaluate the algorithms. Even though the data has more than two dimensions, we can get an idea of how the data clusters by looking at a scatterplot of two variables. We want to look for patterns that appear in the data and see what patients get clustered together.\n\n\nCode\n# plotting age and chol\nplot_one = ggplot(hd_simple, aes(x=age, y=chol, color=as.factor(hc_clust))) + geom_point()\nplot_one \n\n\n\n\n\n\n\n\n\nCode\n# plotting oldpeak and trestbps\nplot_two = ggplot(hd_simple, aes(oldpeak, trestbps, color=as.factor(hc_clust))) + geom_point()\nplot_two"
  },
  {
    "objectID": "projects/Project5/index.html#conclusion",
    "href": "projects/Project5/index.html#conclusion",
    "title": "Clustering Heart Disease Patient Data",
    "section": "10. Conclusion",
    "text": "10. Conclusion\nNow that we’ve tried out multiple clustering algorithms, it is necessary to determine if we think any of them will work for clustering our patients. For the k-means algorithm, it is imperative that similar clusters are produced for each iteration of the algorithm. We want to make sure that the algorithm is clustering signal as opposed to noise.\nFor the sake of the doctors, we also want to have multiple patients in each group so they can compare treatments. We only did some preliminary work to explore the performance of the algorithms. It is necessary to create more visualizations and explore how the algorithms group other variables. Based on the above analysis, are there any algorithms that you would want to investigate further to group patients? Remember that it is important the k-mean algorithm seems stable when running multiple iterations."
  },
  {
    "objectID": "projects/Project6/index.html",
    "href": "projects/Project6/index.html",
    "title": "Classify Suspected Infection in Patients",
    "section": "",
    "text": "Sepsis is a deadly syndrome where a patient has a severe infection that causes organ failure. The sooner septic patients are treated, the more likely they are to survive, but sepsis can be challenging to recognize. It may be possible to use hospital data to develop machine learning models that could flag patients who are likely to be septic. However, before we develop predictive algorithms, we need a reliable method to determine patients who are septic. One component of sepsis is a severe infection.\nIn this project, we will use two weeks of hospital electronic health record (EHR) data to find out which patients had a severe infection according to four criteria. We will look into the data to see if a doctor ordered a blood test to look for bacteria (a blood culture) and gave the patient a series of intervenous antibiotics.\nLet’s get started!\n\n\nCode\n# Load packages\nlibrary(data.table)\n\n# Read in the data\nantibioticDT &lt;- fread('antibioticDT.csv')\n\n# Look at the first 30 rows\nhead(antibioticDT, 30)\n\n\n    patient_id day_given antibiotic_type  route\n         &lt;int&gt;     &lt;int&gt;          &lt;char&gt; &lt;char&gt;\n 1:          1         2   ciprofloxacin     IV\n 2:          1         4   ciprofloxacin     IV\n 3:          1         6   ciprofloxacin     IV\n 4:          1         7     doxycycline     IV\n 5:          1         9     doxycycline     IV\n 6:          1        15      penicillin     IV\n 7:          1        16     doxycycline     IV\n 8:          1        18   ciprofloxacin     IV\n 9:          8         1     doxycycline     PO\n10:          8         2      penicillin     IV\n11:          8         3     doxycycline     IV\n12:          8         6     doxycycline     PO\n13:          8         8      penicillin     PO\n14:          8        12      penicillin     IV\n15:          9         8     doxycycline     IV\n16:          9        12     doxycycline     PO\n17:         12         4     doxycycline     PO\n18:         12         9     doxycycline     IV\n19:         16         1     doxycycline     IV\n20:         16         4     amoxicillin     IV\n21:         19         3     doxycycline     PO\n22:         19         5     amoxicillin     IV\n23:         19         6   ciprofloxacin     IV\n24:         19        10     doxycycline     IV\n25:         19        12      penicillin     IV\n26:         23         1     doxycycline     IV\n27:         23         1      penicillin     IV\n28:         23         3     amoxicillin     IV\n29:         23         3   ciprofloxacin     IV\n30:         23         3     doxycycline     IV\n    patient_id day_given antibiotic_type  route"
  },
  {
    "objectID": "projects/Project6/index.html#this-patient-may-have-sepsis",
    "href": "projects/Project6/index.html#this-patient-may-have-sepsis",
    "title": "Classify Suspected Infection in Patients",
    "section": "",
    "text": "Sepsis is a deadly syndrome where a patient has a severe infection that causes organ failure. The sooner septic patients are treated, the more likely they are to survive, but sepsis can be challenging to recognize. It may be possible to use hospital data to develop machine learning models that could flag patients who are likely to be septic. However, before we develop predictive algorithms, we need a reliable method to determine patients who are septic. One component of sepsis is a severe infection.\nIn this project, we will use two weeks of hospital electronic health record (EHR) data to find out which patients had a severe infection according to four criteria. We will look into the data to see if a doctor ordered a blood test to look for bacteria (a blood culture) and gave the patient a series of intervenous antibiotics.\nLet’s get started!\n\n\nCode\n# Load packages\nlibrary(data.table)\n\n# Read in the data\nantibioticDT &lt;- fread('antibioticDT.csv')\n\n# Look at the first 30 rows\nhead(antibioticDT, 30)\n\n\n    patient_id day_given antibiotic_type  route\n         &lt;int&gt;     &lt;int&gt;          &lt;char&gt; &lt;char&gt;\n 1:          1         2   ciprofloxacin     IV\n 2:          1         4   ciprofloxacin     IV\n 3:          1         6   ciprofloxacin     IV\n 4:          1         7     doxycycline     IV\n 5:          1         9     doxycycline     IV\n 6:          1        15      penicillin     IV\n 7:          1        16     doxycycline     IV\n 8:          1        18   ciprofloxacin     IV\n 9:          8         1     doxycycline     PO\n10:          8         2      penicillin     IV\n11:          8         3     doxycycline     IV\n12:          8         6     doxycycline     PO\n13:          8         8      penicillin     PO\n14:          8        12      penicillin     IV\n15:          9         8     doxycycline     IV\n16:          9        12     doxycycline     PO\n17:         12         4     doxycycline     PO\n18:         12         9     doxycycline     IV\n19:         16         1     doxycycline     IV\n20:         16         4     amoxicillin     IV\n21:         19         3     doxycycline     PO\n22:         19         5     amoxicillin     IV\n23:         19         6   ciprofloxacin     IV\n24:         19        10     doxycycline     IV\n25:         19        12      penicillin     IV\n26:         23         1     doxycycline     IV\n27:         23         1      penicillin     IV\n28:         23         3     amoxicillin     IV\n29:         23         3   ciprofloxacin     IV\n30:         23         3     doxycycline     IV\n    patient_id day_given antibiotic_type  route"
  },
  {
    "objectID": "projects/Project6/index.html#which-antibiotics-are-new",
    "href": "projects/Project6/index.html#which-antibiotics-are-new",
    "title": "Classify Suspected Infection in Patients",
    "section": "2. Which antibiotics are “new”?",
    "text": "2. Which antibiotics are “new”?\nThese data represent all drugs administered in a hospital over two weeks. Each row represents one time a patient was given an antibiotic. The variables include the patient identification number, the day the drug was administered, the name of the antibiotic, and how it was administered. For example, patient “8” received doxycycline by mouth on the first day of their stay.\nWe will identify patients with a serious infection using the following criteria.\nCriteria for Suspected Infection*\n\nThe patient receives antibiotics for a sequence of four days, with gaps of one day allowed.\nThe sequence must start with a new antibiotic, defined as an antibiotic type that was not given in the previous two days.\nThe sequence must start within two days of a blood culture.\nThere must be at least one intervenous (I.V.) antibiotic within the +/-2 day window.\n\nLet’s start with the second item by finding which rows represent “new antibiotics”. We will determine if each antibiotic was given to the patient in the prior two days. We’ll visualize this task by looking at the data sorted by id, antibiotic type, and day.\n\n\nCode\n# Sort the data and examine the first 40 rows\nsetorder(x = antibioticDT, patient_id, antibiotic_type, day_given)\nantibioticDT[1:40]\n\n\n    patient_id day_given antibiotic_type  route\n         &lt;int&gt;     &lt;int&gt;          &lt;char&gt; &lt;char&gt;\n 1:          1         2   ciprofloxacin     IV\n 2:          1         4   ciprofloxacin     IV\n 3:          1         6   ciprofloxacin     IV\n 4:          1        18   ciprofloxacin     IV\n 5:          1         7     doxycycline     IV\n 6:          1         9     doxycycline     IV\n 7:          1        16     doxycycline     IV\n 8:          1        15      penicillin     IV\n 9:          8         1     doxycycline     PO\n10:          8         3     doxycycline     IV\n11:          8         6     doxycycline     PO\n12:          8         2      penicillin     IV\n13:          8         8      penicillin     PO\n14:          8        12      penicillin     IV\n15:          9         8     doxycycline     IV\n16:          9        12     doxycycline     PO\n17:         12         4     doxycycline     PO\n18:         12         9     doxycycline     IV\n19:         16         4     amoxicillin     IV\n20:         16         1     doxycycline     IV\n21:         19         5     amoxicillin     IV\n22:         19         6   ciprofloxacin     IV\n23:         19         3     doxycycline     PO\n24:         19        10     doxycycline     IV\n25:         19        12      penicillin     IV\n26:         23         3     amoxicillin     IV\n27:         23         8     amoxicillin     IV\n28:         23        10     amoxicillin     PO\n29:         23         3   ciprofloxacin     IV\n30:         23         5   ciprofloxacin     PO\n31:         23        16   ciprofloxacin     IV\n32:         23         1     doxycycline     IV\n33:         23         3     doxycycline     IV\n34:         23         4     doxycycline     IV\n35:         23         5     doxycycline     IV\n36:         23         6     doxycycline     IV\n37:         23         6     doxycycline     PO\n38:         23         9     doxycycline     PO\n39:         23        10     doxycycline     IV\n40:         23        11     doxycycline     PO\n    patient_id day_given antibiotic_type  route\n\n\nCode\n# Use shift to calculate the last day a particular drug was administered\nantibioticDT[ , last_administration_day := shift(day_given, n = 1, type = \"lag\"), \n  by = .(patient_id, antibiotic_type)]\n\n# Calculate the number of days since the drug was last administered\nantibioticDT[ , days_since_last_admin := day_given - last_administration_day]\n\n# Create antibiotic_new with an initial value of one, then reset it to zero as needed\nantibioticDT[, antibiotic_new := 1]\nantibioticDT[days_since_last_admin &lt;= 2, antibiotic_new := 0]\nantibioticDT[1:40]\n\n\n    patient_id day_given antibiotic_type  route last_administration_day\n         &lt;int&gt;     &lt;int&gt;          &lt;char&gt; &lt;char&gt;                   &lt;int&gt;\n 1:          1         2   ciprofloxacin     IV                      NA\n 2:          1         4   ciprofloxacin     IV                       2\n 3:          1         6   ciprofloxacin     IV                       4\n 4:          1        18   ciprofloxacin     IV                       6\n 5:          1         7     doxycycline     IV                      NA\n 6:          1         9     doxycycline     IV                       7\n 7:          1        16     doxycycline     IV                       9\n 8:          1        15      penicillin     IV                      NA\n 9:          8         1     doxycycline     PO                      NA\n10:          8         3     doxycycline     IV                       1\n11:          8         6     doxycycline     PO                       3\n12:          8         2      penicillin     IV                      NA\n13:          8         8      penicillin     PO                       2\n14:          8        12      penicillin     IV                       8\n15:          9         8     doxycycline     IV                      NA\n16:          9        12     doxycycline     PO                       8\n17:         12         4     doxycycline     PO                      NA\n18:         12         9     doxycycline     IV                       4\n19:         16         4     amoxicillin     IV                      NA\n20:         16         1     doxycycline     IV                      NA\n21:         19         5     amoxicillin     IV                      NA\n22:         19         6   ciprofloxacin     IV                      NA\n23:         19         3     doxycycline     PO                      NA\n24:         19        10     doxycycline     IV                       3\n25:         19        12      penicillin     IV                      NA\n26:         23         3     amoxicillin     IV                      NA\n27:         23         8     amoxicillin     IV                       3\n28:         23        10     amoxicillin     PO                       8\n29:         23         3   ciprofloxacin     IV                      NA\n30:         23         5   ciprofloxacin     PO                       3\n31:         23        16   ciprofloxacin     IV                       5\n32:         23         1     doxycycline     IV                      NA\n33:         23         3     doxycycline     IV                       1\n34:         23         4     doxycycline     IV                       3\n35:         23         5     doxycycline     IV                       4\n36:         23         6     doxycycline     IV                       5\n37:         23         6     doxycycline     PO                       6\n38:         23         9     doxycycline     PO                       6\n39:         23        10     doxycycline     IV                       9\n40:         23        11     doxycycline     PO                      10\n    patient_id day_given antibiotic_type  route last_administration_day\n    days_since_last_admin antibiotic_new\n                    &lt;int&gt;          &lt;num&gt;\n 1:                    NA              1\n 2:                     2              0\n 3:                     2              0\n 4:                    12              1\n 5:                    NA              1\n 6:                     2              0\n 7:                     7              1\n 8:                    NA              1\n 9:                    NA              1\n10:                     2              0\n11:                     3              1\n12:                    NA              1\n13:                     6              1\n14:                     4              1\n15:                    NA              1\n16:                     4              1\n17:                    NA              1\n18:                     5              1\n19:                    NA              1\n20:                    NA              1\n21:                    NA              1\n22:                    NA              1\n23:                    NA              1\n24:                     7              1\n25:                    NA              1\n26:                    NA              1\n27:                     5              1\n28:                     2              0\n29:                    NA              1\n30:                     2              0\n31:                    11              1\n32:                    NA              1\n33:                     2              0\n34:                     1              0\n35:                     1              0\n36:                     1              0\n37:                     0              0\n38:                     3              1\n39:                     1              0\n40:                     1              0\n    days_since_last_admin antibiotic_new"
  },
  {
    "objectID": "projects/Project6/index.html#looking-at-the-blood-culture-data",
    "href": "projects/Project6/index.html#looking-at-the-blood-culture-data",
    "title": "Classify Suspected Infection in Patients",
    "section": "3. Looking at the blood culture data",
    "text": "3. Looking at the blood culture data\nNow let’s look at blood culture data from the same two-week period in this hospital. These data are in blood_cultureDT.csv. Let’s start by reading it into the workspace and having a look at a few rows.\nEach row represents one blood culture and gives the patient’s id and the day the blood culture test occurred. For example, patient “8” had a blood culture on the second day of their hospitalization and again on the thirteenth day. Notice that some patients from the antibiotic dataset are not in this dataset and vice versa. Some patients are in neither because they received neither antibiotics nor a blood culture.\n\n\nCode\n# Read in blood_cultureDT.csv\nblood_cultureDT &lt;- fread(\"blood_cultureDT.csv\")\n\n# Print the first 30 rows\nblood_cultureDT[1:30]\n\n\n    patient_id blood_culture_day\n         &lt;int&gt;             &lt;int&gt;\n 1:          1                 3\n 2:          1                13\n 3:          8                 2\n 4:          8                13\n 5:         23                 3\n 6:         39                10\n 7:         45                 4\n 8:         45                 9\n 9:         45                11\n10:         51                 3\n11:         51                 6\n12:         59                 2\n13:         64                 1\n14:         66                 9\n15:         66                10\n16:         69                 2\n17:         69                 6\n18:         69                 7\n19:         69                11\n20:         69                16\n21:         76                 1\n22:         77                 3\n23:         79                 5\n24:         79                11\n25:         79                12\n26:         80                 3\n27:         80                12\n28:         81                 2\n29:        112                 6\n30:        115                 2\n    patient_id blood_culture_day"
  },
  {
    "objectID": "projects/Project6/index.html#combine-the-antibiotic-data-and-the-blood-culture-data",
    "href": "projects/Project6/index.html#combine-the-antibiotic-data-and-the-blood-culture-data",
    "title": "Classify Suspected Infection in Patients",
    "section": "4. Combine the antibiotic data and the blood culture data",
    "text": "4. Combine the antibiotic data and the blood culture data\nTo find which antibiotics were given close to a blood culture test, we need to combine the drug administration data with the blood culture data. We’ll keep only patients that are still candidates for infection—only those in both data sets.\nA challenge with the data is that some patients had blood cultures on several different days. For each of those days, we will see if there is a sequence of antibiotic days close to them. To accomplish this, in the merge we will match each blood culture to each antibiotic day.\nAfter sorting the data following the merge, you will see that each patient’s antibiotic sequence repeats for each blood culture day. This repetition allows us to look at each blood culture day and check if it is associated with a qualifying sequence of antibiotics.\n\n\nCode\n# Merge antibioticDT with blood_cultureDT\ncombinedDT &lt;- merge(antibioticDT, blood_cultureDT, by = \"patient_id\", all = FALSE)\n\n# Sort by patient_id, blood_culture_day, day_given, and antibiotic_type\nsetorder(combinedDT, patient_id, blood_culture_day, day_given, antibiotic_type)\n\n# Print and examine the first 30 rows\ncombinedDT[1:30]\n\n\nKey: &lt;patient_id&gt;\n    patient_id day_given antibiotic_type  route last_administration_day\n         &lt;int&gt;     &lt;int&gt;          &lt;char&gt; &lt;char&gt;                   &lt;int&gt;\n 1:          1         2   ciprofloxacin     IV                      NA\n 2:          1         4   ciprofloxacin     IV                       2\n 3:          1         6   ciprofloxacin     IV                       4\n 4:          1         7     doxycycline     IV                      NA\n 5:          1         9     doxycycline     IV                       7\n 6:          1        15      penicillin     IV                      NA\n 7:          1        16     doxycycline     IV                       9\n 8:          1        18   ciprofloxacin     IV                       6\n 9:          1         2   ciprofloxacin     IV                      NA\n10:          1         4   ciprofloxacin     IV                       2\n11:          1         6   ciprofloxacin     IV                       4\n12:          1         7     doxycycline     IV                      NA\n13:          1         9     doxycycline     IV                       7\n14:          1        15      penicillin     IV                      NA\n15:          1        16     doxycycline     IV                       9\n16:          1        18   ciprofloxacin     IV                       6\n17:          8         1     doxycycline     PO                      NA\n18:          8         2      penicillin     IV                      NA\n19:          8         3     doxycycline     IV                       1\n20:          8         6     doxycycline     PO                       3\n21:          8         8      penicillin     PO                       2\n22:          8        12      penicillin     IV                       8\n23:          8         1     doxycycline     PO                      NA\n24:          8         2      penicillin     IV                      NA\n25:          8         3     doxycycline     IV                       1\n26:          8         6     doxycycline     PO                       3\n27:          8         8      penicillin     PO                       2\n28:          8        12      penicillin     IV                       8\n29:         23         1     doxycycline     IV                      NA\n30:         23         1      penicillin     IV                      NA\n    patient_id day_given antibiotic_type  route last_administration_day\n    days_since_last_admin antibiotic_new blood_culture_day\n                    &lt;int&gt;          &lt;num&gt;             &lt;int&gt;\n 1:                    NA              1                 3\n 2:                     2              0                 3\n 3:                     2              0                 3\n 4:                    NA              1                 3\n 5:                     2              0                 3\n 6:                    NA              1                 3\n 7:                     7              1                 3\n 8:                    12              1                 3\n 9:                    NA              1                13\n10:                     2              0                13\n11:                     2              0                13\n12:                    NA              1                13\n13:                     2              0                13\n14:                    NA              1                13\n15:                     7              1                13\n16:                    12              1                13\n17:                    NA              1                 2\n18:                    NA              1                 2\n19:                     2              0                 2\n20:                     3              1                 2\n21:                     6              1                 2\n22:                     4              1                 2\n23:                    NA              1                13\n24:                    NA              1                13\n25:                     2              0                13\n26:                     3              1                13\n27:                     6              1                13\n28:                     4              1                13\n29:                    NA              1                 3\n30:                    NA              1                 3\n    days_since_last_admin antibiotic_new blood_culture_day"
  },
  {
    "objectID": "projects/Project6/index.html#determine-whether-each-row-is-in-window",
    "href": "projects/Project6/index.html#determine-whether-each-row-is-in-window",
    "title": "Classify Suspected Infection in Patients",
    "section": "5. Determine whether each row is in-window",
    "text": "5. Determine whether each row is in-window\nNow that we have the antibiotic and blood culture data combined, we can test each drug administration against each blood culture to see if it’s “in the window.”\n\n\nCode\n# Make a new variable called drug_in_bcx_window\ncombinedDT[ , \n  drug_in_bcx_window := \n           as.numeric(\n               day_given - blood_culture_day &lt;= 2 \n               & \n               day_given - blood_culture_day &gt;= -2)]\ncombinedDT[1:5]\n\n\nKey: &lt;patient_id&gt;\n   patient_id day_given antibiotic_type  route last_administration_day\n        &lt;int&gt;     &lt;int&gt;          &lt;char&gt; &lt;char&gt;                   &lt;int&gt;\n1:          1         2   ciprofloxacin     IV                      NA\n2:          1         4   ciprofloxacin     IV                       2\n3:          1         6   ciprofloxacin     IV                       4\n4:          1         7     doxycycline     IV                      NA\n5:          1         9     doxycycline     IV                       7\n   days_since_last_admin antibiotic_new blood_culture_day drug_in_bcx_window\n                   &lt;int&gt;          &lt;num&gt;             &lt;int&gt;              &lt;num&gt;\n1:                    NA              1                 3                  1\n2:                     2              0                 3                  1\n3:                     2              0                 3                  0\n4:                    NA              1                 3                  0\n5:                     2              0                 3                  0"
  },
  {
    "objectID": "projects/Project6/index.html#check-the-i.v.-requirement",
    "href": "projects/Project6/index.html#check-the-i.v.-requirement",
    "title": "Classify Suspected Infection in Patients",
    "section": "6. Check the I.V. requirement",
    "text": "6. Check the I.V. requirement\nNow let’s look at the fourth item in the criteria.\nCriteria for Suspected Infection*\n\nThe patient receives antibiotics for a sequence of four days, with gaps of one day allowed.\nThe sequence must start with a new antibiotic, defined as an antibiotic type that was not given in the previous two days.\nThe sequence must start within two days of a blood culture.\nThere must be at least one intervenous (I.V.) antibiotic within the +/-2 day window.\n\n\nCode\n# Create a variable indicating if there was at least one I.V. drug given in the window\ncombinedDT[ , \n  any_iv_in_bcx_window := as.numeric(any(route == 'IV' & drug_in_bcx_window == 1)),\n  by = .(patient_id, blood_culture_day)]\n# Exclude rows in which the blood_culture_day does not have any I.V. drugs in window \ncombinedDT &lt;- combinedDT[any_iv_in_bcx_window == 1]\ncombinedDT[1:5]\n\n\nKey: &lt;patient_id&gt;\n   patient_id day_given antibiotic_type  route last_administration_day\n        &lt;int&gt;     &lt;int&gt;          &lt;char&gt; &lt;char&gt;                   &lt;int&gt;\n1:          1         2   ciprofloxacin     IV                      NA\n2:          1         4   ciprofloxacin     IV                       2\n3:          1         6   ciprofloxacin     IV                       4\n4:          1         7     doxycycline     IV                      NA\n5:          1         9     doxycycline     IV                       7\n   days_since_last_admin antibiotic_new blood_culture_day drug_in_bcx_window\n                   &lt;int&gt;          &lt;num&gt;             &lt;int&gt;              &lt;num&gt;\n1:                    NA              1                 3                  1\n2:                     2              0                 3                  1\n3:                     2              0                 3                  0\n4:                    NA              1                 3                  0\n5:                     2              0                 3                  0\n   any_iv_in_bcx_window\n                  &lt;num&gt;\n1:                    1\n2:                    1\n3:                    1\n4:                    1\n5:                    1"
  },
  {
    "objectID": "projects/Project6/index.html#find-the-first-day-of-possible-sequences",
    "href": "projects/Project6/index.html#find-the-first-day-of-possible-sequences",
    "title": "Classify Suspected Infection in Patients",
    "section": "7. Find the first day of possible sequences",
    "text": "7. Find the first day of possible sequences\nWe’re getting close! Let’s review the criteria again.\nCriteria for Suspected Infection*\n\nThe patient receives antibiotics for a sequence of four days, with gaps of one day allowed.\nThe sequence must start with a new antibiotic, defined as an antibiotic type that was not given in the previous two days.\nThe sequence must start within two days of a blood culture.\nThere must be at least one intervenous (I.V.) antibiotic within the +/-2 day window.\n\nLet’s assess the first criterion by finding the first day of possible 4-day qualifying sequences.\n\n\nCode\n# Create a new variable called day_of_first_new_abx_in_window\ncombinedDT[,\n          day_of_first_new_abx_in_window := \n           day_given[antibiotic_new == 1 & drug_in_bcx_window == 1][1],\n           by = .(patient_id, blood_culture_day)\n          ]\n\n# Remove rows where the day is before this first qualifying day\ncombinedDT &lt;- combinedDT[day_given &gt;= day_of_first_new_abx_in_window]\ncombinedDT[1:5]\n\n\nKey: &lt;patient_id&gt;\n   patient_id day_given antibiotic_type  route last_administration_day\n        &lt;int&gt;     &lt;int&gt;          &lt;char&gt; &lt;char&gt;                   &lt;int&gt;\n1:          1         2   ciprofloxacin     IV                      NA\n2:          1         4   ciprofloxacin     IV                       2\n3:          1         6   ciprofloxacin     IV                       4\n4:          1         7     doxycycline     IV                      NA\n5:          1         9     doxycycline     IV                       7\n   days_since_last_admin antibiotic_new blood_culture_day drug_in_bcx_window\n                   &lt;int&gt;          &lt;num&gt;             &lt;int&gt;              &lt;num&gt;\n1:                    NA              1                 3                  1\n2:                     2              0                 3                  1\n3:                     2              0                 3                  0\n4:                    NA              1                 3                  0\n5:                     2              0                 3                  0\n   any_iv_in_bcx_window day_of_first_new_abx_in_window\n                  &lt;num&gt;                          &lt;int&gt;\n1:                    1                              2\n2:                    1                              2\n3:                    1                              2\n4:                    1                              2\n5:                    1                              2"
  },
  {
    "objectID": "projects/Project6/index.html#simplify-the-data",
    "href": "projects/Project6/index.html#simplify-the-data",
    "title": "Classify Suspected Infection in Patients",
    "section": "8. Simplify the data",
    "text": "8. Simplify the data\nThe first criterion is: The patient receives antibiotics for a sequence of four days, with gaps of one day allowed.\nWe’ve pinned down the first day of possible sequences in the previous task. Now we have to check for four-day sequences. We don’t need the drug type (name); we need the days the antibiotics were administered.\n\n\nCode\n# Create a new data.table containing only patient_id, blood_culture_day, and day_given\nsimplified_data &lt;- combinedDT[, .(patient_id, blood_culture_day, day_given)]\n\n# Remove duplicate rows\nsimplified_data &lt;- unique(simplified_data)\nsimplified_data[1:5]\n\n\nKey: &lt;patient_id&gt;\n   patient_id blood_culture_day day_given\n        &lt;int&gt;             &lt;int&gt;     &lt;int&gt;\n1:          1                 3         2\n2:          1                 3         4\n3:          1                 3         6\n4:          1                 3         7\n5:          1                 3         9"
  },
  {
    "objectID": "projects/Project6/index.html#extract-first-four-rows-for-each-blood-culture",
    "href": "projects/Project6/index.html#extract-first-four-rows-for-each-blood-culture",
    "title": "Classify Suspected Infection in Patients",
    "section": "9. Extract first four rows for each blood culture",
    "text": "9. Extract first four rows for each blood culture\nTo check for four-day sequences, let’s pull out the first four days (rows) for each patient/blood culture combination. Some patients will have less than four antibiotic days. We’ll remove them first.\n\n\nCode\n# Count the antibiotic days within each patient/blood culture day combination\nsimplified_data[, num_antibiotic_days := .N, by = .(patient_id, blood_culture_day)]\n\n# Remove blood culture days with less than four rows \nsimplified_data &lt;- simplified_data[num_antibiotic_days &gt;= 4]\n\n# Select the first four days for each blood culture\nfirst_four_days &lt;- simplified_data[, .SD[1:4], by = .(patient_id, blood_culture_day)]\nfirst_four_days[1:5]\n\n\n   patient_id blood_culture_day day_given num_antibiotic_days\n        &lt;int&gt;             &lt;int&gt;     &lt;int&gt;               &lt;int&gt;\n1:          1                 3         2                   8\n2:          1                 3         4                   8\n3:          1                 3         6                   8\n4:          1                 3         7                   8\n5:          8                 2         1                   6"
  },
  {
    "objectID": "projects/Project6/index.html#consecutive-sequence",
    "href": "projects/Project6/index.html#consecutive-sequence",
    "title": "Classify Suspected Infection in Patients",
    "section": "10. Consecutive sequence",
    "text": "10. Consecutive sequence\nNow we need to check whether each four-day sequence qualifies by having no gaps of more than one day.\n\n\nCode\n# Make the indicator for consecutive sequence\nfirst_four_days[,\n               four_in_seq := as.numeric(max(diff(day_given)) &lt; 3), by = .(patient_id, blood_culture_day)\n               ]"
  },
  {
    "objectID": "projects/Project6/index.html#select-the-patients-who-meet-criteria",
    "href": "projects/Project6/index.html#select-the-patients-who-meet-criteria",
    "title": "Classify Suspected Infection in Patients",
    "section": "11. Select the patients who meet criteria",
    "text": "11. Select the patients who meet criteria\nA patient would meet the criteria if any of their blood cultures were accompanied by a qualifying sequence of antibiotics. Now that we’ve determined which each blood culture qualify let’s select the patients who meet the criteria.\n\n\nCode\n# Select the rows which have four_in_seq equal to 1\nsuspected_infection &lt;- first_four_days[four_in_seq == 1]\n\n# Retain only the patient_id column\nsuspected_infection &lt;- suspected_infection[, .(patient_id)]\n\n# Remove duplicates\nsuspected_infection &lt;- unique(suspected_infection)\n\n# Make an infection indicator\nsuspected_infection[, infection := 1]\nsuspected_infection[1:5]\n\n\n   patient_id infection\n        &lt;int&gt;     &lt;num&gt;\n1:          1         1\n2:         23         1\n3:         64         1\n4:         76         1\n5:        164         1"
  },
  {
    "objectID": "projects/Project6/index.html#find-the-prevalence-of-sepsis",
    "href": "projects/Project6/index.html#find-the-prevalence-of-sepsis",
    "title": "Classify Suspected Infection in Patients",
    "section": "12. Find the prevalence of sepsis",
    "text": "12. Find the prevalence of sepsis\nIn this project, we used two EHR datasets to flag patients who were suspected of having a serious infection. We also got a data.table workout!\nSo far, we’ve been looking at records of all antibiotics administered and blood cultures that occurred over two weeks at a particular hospital. However, not all patients who were hospitalized over this period are represented in combinedDT because not all of them took antibiotics or had blood culture tests. We have to read in and merge the rest of the patient information to see what percentage of patients at the hospital might have had a serious infection.\n\n\nCode\n# Read in \"all_patients.csv\"\nall_patientsDT &lt;- fread(\"all_patients.csv\")\n\n# Merge this with the infection flag data\nall_patientsDT &lt;- merge(all_patientsDT, suspected_infection, all = TRUE)\n\n# Set any missing values of the infection flag to 0\nall_patientsDT &lt;- all_patientsDT[is.na(infection), infection := 0]\nall_patientsDT[1:10]\n\n\nKey: &lt;patient_id&gt;\n    patient_id infection\n         &lt;int&gt;     &lt;num&gt;\n 1:          1         1\n 2:          5         0\n 3:          8         0\n 4:          9         0\n 5:         12         0\n 6:         16         0\n 7:         19         0\n 8:         23         1\n 9:         25         0\n10:         39         0\n\n\nCode\n# Calculate the percentage of patients who met the criteria for presumed infection\nans  &lt;- 100* all_patientsDT[, mean(infection)]\nans\n\n\n[1] 14.94382"
  },
  {
    "objectID": "posts/post4/index.html",
    "href": "posts/post4/index.html",
    "title": "Key Benefits of R Shiny in Clinical Programming and Pharma industry",
    "section": "",
    "text": "R Shiny, a framework for building interactive web applications with R, has become an indispensable tool for clinical programmers, researchers, and pharmaceutical companies. By combining the statistical capabilities of R with the interactivity of web applications, Shiny enables users to create dynamic, user-friendly interfaces that facilitate data exploration, analysis, and communication.\n\nInteractive Data Visualization:\n\nDynamic plots and charts: Create interactive visualizations that allow users to explore data in real-time, adjust parameters, and uncover hidden patterns.\nCustomizable dashboards: Develop tailored dashboards that present key metrics and insights in a visually appealing and informative format.\nEnhanced collaboration: Facilitate collaboration among team members by providing a shared platform for exploring and discussing data.\n\nData Exploration and Analysis:\n\nUser-friendly interfaces: Build intuitive interfaces that simplify complex data analysis tasks, making it accessible to users with varying technical backgrounds.\nInteractive data exploration: Allow users to filter, sort, and search data, enabling them to delve deeper into specific areas of interest.\nReal-time updates: Ensure that visualizations and analyses are updated automatically as data changes, providing a dynamic and up-to-date view of the information.\n\nClinical Trial Management:\n\nPatient recruitment and tracking: Develop interactive tools for managing patient recruitment, tracking enrollment progress, and monitoring key metrics.\nData monitoring and reporting: Create dashboards for real-time data monitoring, identifying potential safety signals, and generating customized reports.\nRegulatory compliance: Ensure adherence to regulatory standards by providing a platform for documenting and tracking clinical trial activities.\n\nPharmacovigilance:\n\nAdverse event reporting: Build interactive tools for capturing, reporting, and analyzing adverse events, facilitating timely identification and investigation of safety signals.\nRisk assessment and mitigation: Develop dashboards for assessing risk factors, monitoring safety profiles, and implementing risk mitigation strategies.\nRegulatory reporting: Generate regulatory-compliant reports, such as periodic safety update reports (PSURs), using Shiny’s data visualization and reporting capabilities.\n\n\nExample Use Cases:\n\nInteractive safety monitoring dashboard: Visualize adverse event rates, severity, and temporal trends over time.\nClinical trial recruitment tracking app: Monitor enrollment progress, identify recruitment bottlenecks, and optimize recruitment strategies.\nDrug safety risk assessment tool: Assess drug-drug interaction risks, identify potential safety signals, and implement mitigation measures.\nPharmaceutical sales analysis dashboard: Analyze sales trends, market share, and customer behavior to inform marketing and sales strategies.\n\nR Shiny offers a powerful and versatile platform for clinical programming, clinical research, and the pharmaceutical industry. By leveraging its capabilities for interactive data visualization, data exploration, clinical trial management, and pharmacovigilance, researchers and clinicians can gain valuable insights, improve decision-making, and ultimately accelerate drug development and improve patient outcomes."
  },
  {
    "objectID": "Services/index.html",
    "href": "Services/index.html",
    "title": "Services",
    "section": "",
    "text": "test services"
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Services",
    "section": "",
    "text": "Statistical Programming\n\nSAS Programming: Develop and maintain SAS programs for statistical analysis, dataset creation, and data manipulation.\nR Programming: Use R for statistical modeling, visualizations, and data handling in clinical trials.\nPython: Provide programming support using Python for advanced data science applications, machine learning models, or automation in data processing.\nCustom Reporting Tools: Develop custom scripts to automate the generation of Tables, Listings, and Figures (TLFs) for clinical trial reports.\n\n\n\nQuality Control and Validation\n\nDouble Programming: Perform independent programming to validate datasets and analysis results, ensuring data integrity.\nValidation of Statistical Code: Conduct validation checks on SAS, R, or Python code for accuracy, consistency, and reproducibility.\n\n\n\nClinical Data Visualization\n\nGraphs and Plots: Create advanced data visualizations (e.g., Kaplan-Meier curves, forest plots, waterfall plots, spaghetti plots) to represent clinical trial data.\nInteractive Dashboards: Develop interactive dashboards (using tools like Shiny in R or Power BI) for clinical teams to explore data in real-time.\n\n\n\nRegulatory Submission Support\n\nSubmission Datasets: Prepare datasets and documentation in accordance with regulatory standards (e.g., CDISC-compliant datasets) for submissions to the FDA, EMA, or other regulatory agencies.\nIntegrated Summary of Safety (ISS) / Integrated Summary of Efficacy (ISE): Create datasets and perform analyses required for the ISS and ISE sections of the New Drug Application (NDA) or Biologics License Application (BLA).\nReviewer’s Guide and Define.XML: Prepare essential documentation such as the Reviewer’s Guide, Define.XML, and annotated CRFs (Case Report Forms) for regulatory submissions.\n\n\n\nTraining and Support\n\nProgramming Workshops: Provide training sessions or workshops in SAS, R, Python, or CDISC standards for clinical and statistical teams.\nOngoing Statistical Support: Offer ongoing support for clinical research teams during the trial lifecycle, including troubleshooting, re-analysis, and data interpretation."
  },
  {
    "objectID": "posts/post5/index.html",
    "href": "posts/post5/index.html",
    "title": "Quality Assurance (QA) Importance and Standards in Clinical Statistical Programming",
    "section": "",
    "text": "In clinical research, Quality Assurance (QA) is an essential component that ensures the accuracy, consistency, and reliability of data used for decision-making in drug development and regulatory submissions. With clinical trials becoming more complex and data-driven, the role of QA in clinical statistical programming has never been more critical. This article explores the importance of QA in clinical programming and outlines key industry standards that help safeguard the integrity of statistical analysis in clinical trials.\n1. Ensuring Data Accuracy\nClinical trials generate vast amounts of data, ranging from patient demographics to medical histories and lab results. Statistical programming plays a crucial role in transforming this raw data into meaningful, interpretable results that inform decisions about the safety and efficacy of new drugs.\nWithout effective QA processes, errors in data processing, transformations, or analysis could lead to misleading results, potentially affecting decisions about a treatment’s viability. For instance, an error in programming could result in an incorrect conclusion about the effectiveness of a drug, leading to unnecessary delays in bringing a life-saving therapy to market or, worse, posing a risk to patient safety.\nQA measures ensure that every step of data handling is thoroughly checked and validated, from data cleaning to the final statistical output. By enforcing strict QA protocols, clinical programmers can prevent common errors such as:\n\nInconsistent data merging or transformation.\nInaccurate variable definitions.\nCoding mistakes in statistical methods or procedures.\n\n2. Regulatory Compliance\nClinical trials are tightly regulated by agencies such as the U.S. Food and Drug Administration (FDA), European Medicines Agency (EMA), and other global regulatory bodies. These organizations demand that clinical data be presented in a clear, accurate, and reproducible manner to ensure that the conclusions drawn are scientifically sound.\nQuality Assurance in statistical programming helps ensure that all data and analysis processes comply with international standards, such as:\n\nICH E9: Guidance on statistical principles for clinical trials.\nICH E3: Provides standards for clinical study reports, which include detailed requirements for how statistical methods and results are presented.\nFDA’s CDISC (Clinical Data Interchange Standards Consortium) standards: Specifies how data should be structured for regulatory submissions.\n\nFailure to meet these regulatory requirements can result in delays or even rejection of a drug application. QA frameworks make sure that all clinical trial data is compliant with these standards, mitigating the risk of submission failures and ensuring that the data is presented clearly and correctly.\n3. Data Integrity and Reproducibility\nData integrity refers to the accuracy and consistency of data throughout its lifecycle. In clinical trials, where multiple analyses may be run across different datasets and time points, it is crucial that the programming outputs are reproducible and reliable.\nQA processes help ensure that statistical programs are fully documented, version-controlled, and executed in a manner that allows other researchers or auditors to reproduce the same results using the same inputs. This is especially important during regulatory inspections, where reproducibility is often a requirement. Programs and outputs that are inconsistent or difficult to replicate raise concerns about the reliability of the findings.\nKey components of ensuring data integrity include:\n\nVersion control: Tracking changes to programming code, datasets, and outputs to maintain consistency and prevent errors.\nAuditing: Regularly reviewing programming logs and code to identify and rectify discrepancies\n\n4. Risk Mitigation\nErrors in statistical programming can lead to significant downstream consequences, including incorrect conclusions, rework, or even rejection of a trial’s results by regulatory bodies. QA in clinical statistical programming plays a pivotal role in risk mitigation by identifying potential issues early in the process.\nQuality Assurance activities, such as code reviews, validation of datasets, and double programming (where the same analysis is independently coded by different programmers), can significantly reduce the likelihood of critical errors going undetected. This not only ensures the accuracy of the data but also improves the efficiency of the trial by minimizing the need for rework.\n5. Efficient Collaboration and Communication\nQA practices encourage standardization, which facilitates clear communication and collaboration between different teams involved in a clinical trial, such as biostatisticians, data managers, and programmers. Standardized procedures for documenting code, validating datasets, and reporting results help all stakeholders understand the structure and logic of the analysis.\nBy ensuring that all team members follow the same standards and procedures, QA creates a transparent and efficient workflow, reducing the time needed for review and revisions. This ultimately speeds up the submission process and increases the likelihood of successful regulatory approval."
  },
  {
    "objectID": "posts/post6/index.html",
    "href": "posts/post6/index.html",
    "title": "The Importance of Agile/Scrum Methodology in Enhancing Statistical Programming and Analysis Efficiency in the Pharmaceutical Industry and Clinical Research",
    "section": "",
    "text": "The pharmaceutical industry is continuously evolving, driven by the need for faster drug development, stringent regulatory demands, and the push for innovation in medical research. Clinical research, which serves as the backbone of drug discovery, relies heavily on statistical programming and data analysis. Historically, traditional, linear project management methodologies, such as Waterfall, have been used to manage clinical trials. However, the complexities and dynamic nature of clinical trials require a more adaptable and efficient approach.\nEnter Agile/Scrum methodology—a project management approach traditionally associated with software development but now gaining popularity in statistical programming and clinical research. Agile methodology, and its subset Scrum, offers a flexible, iterative, and collaborative approach to managing tasks and processes. In this article, we explore the importance of Agile/Scrum methodology in statistical programming for the pharmaceutical industry and how it enhances the efficiency of data analysis in clinical research.\n1. Adapting to Change: Clinical Trials are Dynamic\nOne of the core principles of the Agile methodology is adaptability to change. In clinical trials, study designs, protocols, and regulatory requirements often change during the course of the trial. Traditional project management models, which follow a linear, step-by-step process, may struggle to accommodate these changes without significant disruptions or delays. Agile, however, thrives in environments where change is expected.\n\nAgile in Statistical Programming: By breaking down large, complex tasks (such as creating SDTM and ADaM datasets, producing analysis results, or generating tables, listings, and figures (TLFs)) into smaller, manageable sprints, Agile allows statistical programming teams to respond quickly to changes. If a change in a clinical trial protocol affects a variable, programmers can address it in the next sprint without having to rewrite the entire analysis plan.\nExample: Midway through a clinical trial, the trial sponsor may request changes to the study’s endpoints or statistical analysis plan (SAP). Under Agile, these changes can be absorbed into the next sprint, with teams working collaboratively to update the relevant code, datasets, and outputs.\n\n2. Faster Delivery of Results: Iterative Approach\nIn clinical research, time is of the essence. Every day that a trial is delayed can mean longer time to market for life-saving drugs and increased costs for pharmaceutical companies. Agile’s iterative development cycles—known as sprints—help accelerate the delivery of valuable outcomes at regular intervals. Each sprint focuses on delivering a subset of the work, allowing for early data insights and faster feedback loops.\n\nAgile in Statistical Programming: By organizing tasks into sprints, statistical programming teams can deliver interim datasets, preliminary analyses, or draft outputs at the end of each sprint, allowing sponsors and stakeholders to review and provide feedback early. This reduces the risk of significant rework at the end of the project and ensures that any issues are caught and addressed sooner.\nExample: In a six-month clinical trial, an Agile-driven statistical programming team might deliver preliminary summary tables and listings after each two-week sprint, giving biostatisticians and data managers early insight into the data and enabling them to make adjustments as needed before the final analysis.\n\n3. Cross-Functional Collaboration: Breaking Down Silos\nIn traditional clinical trial management, teams often work in silos, with statisticians, data managers, programmers, and regulatory affairs teams working sequentially rather than collaboratively. This can lead to delays in communication, misinterpretation of requirements, and inefficiencies. Agile, particularly Scrum, promotes cross-functional collaboration, breaking down these silos and ensuring that teams work together throughout the process.\n\nScrum Teams in Clinical Research: A Scrum team typically includes professionals from diverse functions—statisticians, clinical data managers, programmers, and project managers—all working toward a common goal. By conducting regular stand-up meetings (daily 15-minute check-ins), the entire team stays aligned, and potential roadblocks are identified early.\nExample: In a clinical trial, rather than statisticians developing an analysis plan in isolation and handing it off to programmers at the end, both statisticians and programmers work together from the outset. During each sprint, they continuously communicate to ensure that the analysis plan is clearly understood, and the necessary datasets and outputs are generated as planned.\n\n4. Transparency and Continuous Feedback\nThe pharmaceutical industry is heavily regulated, with stringent requirements for traceability, transparency, and accountability. Agile/Scrum’s emphasis on continuous feedback and transparency can significantly enhance compliance in clinical research.\n\nRegular Reviews: Agile encourages regular reviews of progress, with stakeholders and team members providing feedback at the end of each sprint. For statistical programming, this means that interim analyses, validation checks, and data cleaning tasks are reviewed and approved continuously rather than at the end of the trial.\nExample: After each sprint, a statistical programmer might present the completed set of tables, listings, and figures to the sponsor for feedback. Any necessary modifications or corrections are integrated into the next sprint cycle. This ensures that final submissions are of the highest quality, with fewer surprises or errors at the end of the project.\n\n5. Prioritizing High-Value Tasks\nIn Agile/Scrum methodology, tasks are prioritized based on their value to the customer—in the case of clinical research, this means the sponsor or regulatory bodies. By focusing on delivering the most critical datasets and analyses first, teams can ensure that high-priority tasks are completed early, reducing risk and improving efficiency.\n\nAgile Prioritization in Clinical Trials: In clinical trials, some analyses (such as safety and efficacy analyses) are more critical than others. Using Agile, teams can prioritize the development of these outputs first, ensuring that stakeholders have access to the most important results as early as possible.\nExample: In a late-phase clinical trial, a team using Scrum might prioritize generating adverse event summaries and primary endpoint analyses before working on less critical secondary analyses. This ensures that regulatory bodies can review critical safety and efficacy data at interim points without delay.\n\n6. Improved Risk Management\nAgile methodology’s iterative nature allows for continuous testing and validation, making it easier to identify and address risks as they emerge, rather than after the trial is complete. Statistical programmers, in particular, benefit from this approach, as they can run early validation checks, identify inconsistencies, and adjust the programming code during each sprint.\n\nExample: During an Agile sprint, a clinical data manager may notice that a variable in the dataset is not correctly aligned with the statistical analysis plan. The team can immediately address the issue in the next sprint, rather than discovering the problem during the final validation stages, which could delay the trial submission.\n\n7. Agile Validation and Compliance in Clinical Trials\nValidation is a critical part of clinical research and statistical programming. For regulatory submissions, the data and analyses must be thoroughly validated to ensure accuracy and compliance with standards such as CDISC, SDTM, and ADaM. In a traditional model, validation often occurs late in the project lifecycle, potentially leading to costly rework.\nAgile promotes continuous validation throughout the lifecycle of the trial. By integrating validation checks into each sprint, statistical programming teams can ensure that datasets and outputs meet compliance requirements early and often. This reduces the risk of errors being discovered at the end of the trial, ensuring that the final submission is accurate, complete, and compliant.\n\nExample: A team using Scrum might perform QC (Quality Control) and validation on key analysis datasets and outputs at the end of each sprint. Any discrepancies are addressed during the following sprint, minimizing the risk of major issues during the final submission phase.\n\n8. Increased Team Morale and Ownership\nAgile fosters an environment of team ownership and accountability, empowering team members to make decisions and contribute actively to the project’s success. For statistical programmers, this means they are not just executors of a predefined plan but are active participants in shaping the project’s outcome.\nIn Agile, programmers, statisticians, and data managers take collective ownership of the deliverables for each sprint. This increases team morale, engagement, and overall productivity, leading to more efficient statistical programming and analysis processes.\nThe pharmaceutical industry is increasingly embracing Agile/Scrum methodology as a way to enhance efficiency, adaptability, and collaboration in clinical trials. For statistical programming teams, Agile offers a more responsive, transparent, and collaborative approach, ensuring that datasets, analyses, and outputs are delivered accurately, efficiently, and on time.\nIn an industry where every day counts in the development of new treatments and therapies, Agile’s ability to adapt to change, accelerate deliverables, and improve team dynamics makes it a valuable methodology for the future of clinical research and statistical programming"
  },
  {
    "objectID": "projects/Project7/index.html",
    "href": "projects/Project7/index.html",
    "title": "Survival Analysis using R",
    "section": "",
    "text": "In the class on essential statistics we covered basic categorical data analysis – comparing proportions (risks, rates, etc) between different groups using a chi-square or fisher exact test, or logistic regression. For example, we looked at how the diabetes rate differed between males and females. In this kind of analysis you implicitly assume that the rates are constant over the period of the study, or as defined by the different groups you defined.\nBut, in longitudinal studies where you track samples or subjects from one time point (e.g., entry into a study, diagnosis, start of a treatment) until you observe some outcome event (e.g., death, onset of disease, relapse), it doesn’t make sense to assume the rates are constant. For example: the risk of death after heart surgery is highest immediately post-op, decreases as the patient recovers, then rises slowly again as the patient ages. Or, recurrence rate of different cancers varies highly over time, and depends on tumor genetics, treatment, and other environmental factors.\n\n\nSurvival analysis lets you analyze the rates of occurrence of events over time, without assuming the rates are constant. Generally, survival analysis lets you model the time until an event occurs,1 or compare the time-to-event between different groups, or how time-to-event correlates with quantitative variables.\nThe hazard is the instantaneous event (death) rate at a particular time point t. Survival analysis doesn’t assume the hazard is constant over time. The cumulative hazard is the total hazard experienced up to time t.\nThe survival function, is the probability an individual survives (or, the probability that the event of interest does not occur) up to and including time t. It’s the probability that the event (e.g., death) hasn’t occured yet. It looks like this, where TT is the time of death, and Pr(T&gt;t)Pr(T&gt;t) is the probability that the time of death is greater than some time tt. SS is a probability, so 0≤S(t)≤10≤S(t)≤1, since survival times are always positive (T≥0T≥0).\nS(t)=Pr(T&gt;t)S(t)=Pr(T&gt;t)\nThe Kaplan-Meier curve illustrates the survival function. It’s a step function illustrating the cumulative survival probability over time. The curve is horizontal over periods where no event occurs, then drops vertically corresponding to a change in the survival function at each time an event occurs.\nCensoring is a type of missing data problem unique to survival analysis. This happens when you track the sample/subject through the end of the study and the event never occurs. This could also happen due to the sample/subject dropping out of the study for reasons other than death, or some other loss to followup. The sample is censored in that you only know that the individual survived up to the loss to followup, but you don’t know anything about survival after that.\nProportional hazards assumption: The main goal of survival analysis is to compare the survival functions in different groups, e.g., leukemia patients as compared to cancer-free controls. If you followed both groups until everyone died, both survival curves would end at 0%, but one group might have survived on average a lot longer than the other group. Survival analysis does this by comparing the hazard at different times over the observation period. Survival analysis doesn’t assume that the hazard is constant, but does assume that the ratio of hazards between groups is constant over time. This class does not cover methods to deal with non-proportional hazards, or interactions of covariates with the time to event.\nProportional hazards regression a.k.a. Cox regression is the most common approach to assess the effect of different variables on survival.\n\n\n\nKaplan-Meier curves are good for visualizing differences in survival between two categorical groups, but they don’t work well for assessing the effect of quantitative variables like age, gene expression, leukocyte count, etc. Cox PH regression can assess the effect of both categorical and continuous variables, and can model the effect of multiple variables at once.\nCox PH regression models the natural log of the hazard at time t, denoted h(t)h(t), as a function of the baseline hazard (h0(t)h0(t)) (the hazard for an individual where all exposure variables are 0) and multiple exposure variables x1x1, x1x1, ……, xpxp. The form of the Cox PH model is:\nlog(h(t))=log(h0(t))+β1x1+β2x2+…+βpxplog(h(t))=log(h0(t))+β1x1+β2x2+…+βpxp\nIf you exponentiate both sides of the equation, and limit the right hand side to just a single categorical exposure variable (x1x1) with two groups (x1=1x1=1 for exposed and x1=0x1=0 for unexposed), the equation becomes:\nh1(t)=h0(t)×eβ1x1h1(t)=h0(t)×eβ1x1\nRearranging that equation lets you estimate the hazard ratio, comparing the exposed to the unexposed individuals at time t:\nHR(t)=h1(t)h0(t)=eβ1HR(t)=h1(t)h0(t)=eβ1\nThis model shows that the hazard ratio is eβ1eβ1, and remains constant over time t (hence the name proportional hazards regression). The ββ values are the regression coefficients that are estimated from the model, and represent the log(HazardRatio)log(HazardRatio) for each unit increase in the corresponding predictor variable. The interpretation of the hazards ratio depends on the measurement scale of the predictor variable, but in simple terms, a positive coefficient indicates worse survival and a negative coefficient indicates better survival for the variable in question.\n\n\n\n\nThe core survival analysis functions are in the survival package. The survival package is one of the few “core” packages that comes bundled with your basic R installation, so you probably didn’t need to install.packages() it. But, you’ll need to load it like any other library when you want to use it. We’ll also be using the dplyr package, so let’s load that too. Finally, we’ll also want to load the survminer package, which provides much nicer Kaplan-Meier plots out-of-the-box than what you get out of base graphics.\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttachement du package : 'dplyr'\n\n\nLes objets suivants sont masqués depuis 'package:stats':\n\n    filter, lag\n\n\nLes objets suivants sont masqués depuis 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(survival)\nlibrary(survminer)\n\n\nLe chargement a nécessité le package : ggplot2\n\n\nLe chargement a nécessité le package : ggpubr\n\n\n\nAttachement du package : 'survminer'\n\n\nL'objet suivant est masqué depuis 'package:survival':\n\n    myeloma\n\n\nThe core functions we’ll use out of the survival package include:\n\nSurv(): Creates a survival object.\nsurvfit(): Fits a survival curve using either a formula, of from a previously fitted Cox model.\ncoxph(): Fits a Cox proportional hazards regression model.\n\nOther optional functions you might use include:\n\ncox.zph(): Tests the proportional hazards assumption of a Cox regression model.\nsurvdiff(): Tests for differences in survival between two groups using a log-rank / Mantel-Haenszel test.\n\nSurv() creates the response variable, and typical usage takes the time to event, and whether or not the event occured (i.e., death vs censored). survfit() creates a survival curve that you could then display or plot. coxph() implements the regression analysis, and models specified the same way as in regular linear models, but using the coxph() function.\n\n\nWe’re going to be using the built-in lung cancer dataset that ships with the survival package. You can get some more information about the dataset by running ?lung. The help tells us there are 10 variables in this data:\n\n\nCode\nlibrary(survival)\n?lung\n\n\ndémarrage du serveur d'aide httpd ... fini\n\n\n\ninst: Institution code\ntime: Survival time in days\nstatus: censoring status 1=censored, 2=dead\nage: Age in years\nsex: Male=1 Female=2\nph.ecog: ECOG performance score (0=good 5=dead)\nph.karno: Karnofsky performance score as rated by physician\npat.karno: Karnofsky performance score as rated by patient\nmeal.cal: Calories consumed at meals\nwt.loss: Weight loss in last six months\n\nYou can access the data just by running lung, as if you had read in a dataset and called it lung. You can operate on it just like any other data frame.\n\n\nCode\nhead(lung)\n\n\n  inst time status age sex ph.ecog ph.karno pat.karno meal.cal wt.loss\n1    3  306      2  74   1       1       90       100     1175      NA\n2    3  455      2  68   1       0       90        90     1225      15\n3    3 1010      1  56   1       0       90        90       NA      15\n4    5  210      2  57   1       1       90        60     1150      11\n5    1  883      2  60   1       0      100        90       NA       0\n6   12 1022      1  74   1       1       50        80      513       0\n\n\nCode\nclass(lung)\n\n\n[1] \"data.frame\"\n\n\nCode\ndim(lung)\n\n\n[1] 228  10\n\n\nCode\nView(lung)\n\n\nNotice that lung is a plain data.frame object. You could see what it looks like as a tibble (prints nicely, tells you the type of variable each column is). You could then reassign lung to the as_tibble()-ified version.\n\n\nCode\nas_tibble(lung)\n\n\n# A tibble: 228 × 10\n    inst  time status   age   sex ph.ecog ph.karno pat.karno meal.cal wt.loss\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1     3   306      2    74     1       1       90       100     1175      NA\n 2     3   455      2    68     1       0       90        90     1225      15\n 3     3  1010      1    56     1       0       90        90       NA      15\n 4     5   210      2    57     1       1       90        60     1150      11\n 5     1   883      2    60     1       0      100        90       NA       0\n 6    12  1022      1    74     1       1       50        80      513       0\n 7     7   310      2    68     2       2       70        60      384      10\n 8    11   361      2    71     2       2       60        80      538       1\n 9     1   218      2    53     1       1       70        80      825      16\n10     7   166      2    61     1       2       70        70      271      34\n# ℹ 218 more rows\n\n\nCode\nlung &lt;- as_tibble(lung)\nlung\n\n\n# A tibble: 228 × 10\n    inst  time status   age   sex ph.ecog ph.karno pat.karno meal.cal wt.loss\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1     3   306      2    74     1       1       90       100     1175      NA\n 2     3   455      2    68     1       0       90        90     1225      15\n 3     3  1010      1    56     1       0       90        90       NA      15\n 4     5   210      2    57     1       1       90        60     1150      11\n 5     1   883      2    60     1       0      100        90       NA       0\n 6    12  1022      1    74     1       1       50        80      513       0\n 7     7   310      2    68     2       2       70        60      384      10\n 8    11   361      2    71     2       2       60        80      538       1\n 9     1   218      2    53     1       1       70        80      825      16\n10     7   166      2    61     1       2       70        70      271      34\n# ℹ 218 more rows\n\n\n\n\n\nCheck out the help for ?Surv. This is the main function we’ll use to create the survival object. You can play fast and loose with how you specify the arguments to Surv. The help tells you that when there are two unnamed arguments, they will match time and event in that order. This is the common shorthand you’ll often see for right-censored data. The alternative lets you specify interval data, where you give it the start and end times (time and time2). If you keep reading you’ll see how Surv tries to guess how you’re coding the status variable. It will try to guess whether you’re using 0/1 or 1/2 to represent censored vs “dead”, respectively.\nTry creating a survival object called s, then display it. If you go back and head(lung) the data, you can see how these are related. It’s a special type of vector that tells you both how long the subject was tracked for, and whether or not the event occured or the sample was censored (shown by the +).\n\n\nCode\ns &lt;- Surv(lung$time, lung$status)\nclass(s)\n\n\n[1] \"Surv\"\n\n\nCode\ns\n\n\n  [1]  306   455  1010+  210   883  1022+  310   361   218   166   170   654 \n [13]  728    71   567   144   613   707    61    88   301    81   624   371 \n [25]  394   520   574   118   390    12   473    26   533   107    53   122 \n [37]  814   965+   93   731   460   153   433   145   583    95   303   519 \n [49]  643   765   735   189    53   246   689    65     5   132   687   345 \n [61]  444   223   175    60   163    65   208   821+  428   230   840+  305 \n [73]   11   132   226   426   705   363    11   176   791    95   196+  167 \n [85]  806+  284   641   147   740+  163   655   239    88   245   588+   30 \n [97]  179   310   477   166   559+  450   364   107   177   156   529+   11 \n[109]  429   351    15   181   283   201   524    13   212   524   288   363 \n[121]  442   199   550    54   558   207    92    60   551+  543+  293   202 \n[133]  353   511+  267   511+  371   387   457   337   201   404+  222    62 \n[145]  458+  356+  353   163    31   340   229   444+  315+  182   156   329 \n[157]  364+  291   179   376+  384+  268   292+  142   413+  266+  194   320 \n[169]  181   285   301+  348   197   382+  303+  296+  180   186   145   269+\n[181]  300+  284+  350   272+  292+  332+  285   259+  110   286   270    81 \n[193]  131   225+  269   225+  243+  279+  276+  135    79    59   240+  202+\n[205]  235+  105   224+  239   237+  173+  252+  221+  185+   92+   13   222+\n[217]  192+  183   211+  175+  197+  203+  116   188+  191+  105+  174+  177+\n\n\nCode\nhead(lung)\n\n\n# A tibble: 6 × 10\n   inst  time status   age   sex ph.ecog ph.karno pat.karno meal.cal wt.loss\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     3   306      2    74     1       1       90       100     1175      NA\n2     3   455      2    68     1       0       90        90     1225      15\n3     3  1010      1    56     1       0       90        90       NA      15\n4     5   210      2    57     1       1       90        60     1150      11\n5     1   883      2    60     1       0      100        90       NA       0\n6    12  1022      1    74     1       1       50        80      513       0\n\n\nNow, let’s fit a survival curve with the survfit() function. See the help for ?survfit. Here we’ll create a simple survival curve that doesn’t consider any different groupings, so we’ll specify just an intercept (e.g., ~1) in the formula that survfit expects. We can do what we just did by “modeling” the survival object s we just created against an intercept only, but from here out, we’ll just do this in one step by nesting the Surv() call within the survfit() call, and similar to how we specify data for linear models with lm(), we’ll use the data= argument to specify which data we’re using. Similarly, we can assign that to another object called sfit (or whatever we wanted to call it).\n\n\nCode\nsurvfit(s~1)\n\n\nCall: survfit(formula = s ~ 1)\n\n       n events median 0.95LCL 0.95UCL\n[1,] 228    165    310     285     363\n\n\nCode\nsurvfit(Surv(time, status)~1, data=lung)\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung)\n\n       n events median 0.95LCL 0.95UCL\n[1,] 228    165    310     285     363\n\n\nCode\nsfit &lt;- survfit(Surv(time, status)~1, data=lung)\nsfit\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung)\n\n       n events median 0.95LCL 0.95UCL\n[1,] 228    165    310     285     363\n\n\nNow, that object itself isn’t very interesting. It’s more interesting to run summary on what it creates. This will show a life table.\n\n\nCode\nsummary(sfit)\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    5    228       1   0.9956 0.00438       0.9871        1.000\n   11    227       3   0.9825 0.00869       0.9656        1.000\n   12    224       1   0.9781 0.00970       0.9592        0.997\n   13    223       2   0.9693 0.01142       0.9472        0.992\n   15    221       1   0.9649 0.01219       0.9413        0.989\n   26    220       1   0.9605 0.01290       0.9356        0.986\n   30    219       1   0.9561 0.01356       0.9299        0.983\n   31    218       1   0.9518 0.01419       0.9243        0.980\n   53    217       2   0.9430 0.01536       0.9134        0.974\n   54    215       1   0.9386 0.01590       0.9079        0.970\n   59    214       1   0.9342 0.01642       0.9026        0.967\n   60    213       2   0.9254 0.01740       0.8920        0.960\n   61    211       1   0.9211 0.01786       0.8867        0.957\n   62    210       1   0.9167 0.01830       0.8815        0.953\n   65    209       2   0.9079 0.01915       0.8711        0.946\n   71    207       1   0.9035 0.01955       0.8660        0.943\n   79    206       1   0.8991 0.01995       0.8609        0.939\n   81    205       2   0.8904 0.02069       0.8507        0.932\n   88    203       2   0.8816 0.02140       0.8406        0.925\n   92    201       1   0.8772 0.02174       0.8356        0.921\n   93    199       1   0.8728 0.02207       0.8306        0.917\n   95    198       2   0.8640 0.02271       0.8206        0.910\n  105    196       1   0.8596 0.02302       0.8156        0.906\n  107    194       2   0.8507 0.02362       0.8056        0.898\n  110    192       1   0.8463 0.02391       0.8007        0.894\n  116    191       1   0.8418 0.02419       0.7957        0.891\n  118    190       1   0.8374 0.02446       0.7908        0.887\n  122    189       1   0.8330 0.02473       0.7859        0.883\n  131    188       1   0.8285 0.02500       0.7810        0.879\n  132    187       2   0.8197 0.02550       0.7712        0.871\n  135    185       1   0.8153 0.02575       0.7663        0.867\n  142    184       1   0.8108 0.02598       0.7615        0.863\n  144    183       1   0.8064 0.02622       0.7566        0.859\n  145    182       2   0.7975 0.02667       0.7469        0.852\n  147    180       1   0.7931 0.02688       0.7421        0.848\n  153    179       1   0.7887 0.02710       0.7373        0.844\n  156    178       2   0.7798 0.02751       0.7277        0.836\n  163    176       3   0.7665 0.02809       0.7134        0.824\n  166    173       2   0.7577 0.02845       0.7039        0.816\n  167    171       1   0.7532 0.02863       0.6991        0.811\n  170    170       1   0.7488 0.02880       0.6944        0.807\n  175    167       1   0.7443 0.02898       0.6896        0.803\n  176    165       1   0.7398 0.02915       0.6848        0.799\n  177    164       1   0.7353 0.02932       0.6800        0.795\n  179    162       2   0.7262 0.02965       0.6704        0.787\n  180    160       1   0.7217 0.02981       0.6655        0.783\n  181    159       2   0.7126 0.03012       0.6559        0.774\n  182    157       1   0.7081 0.03027       0.6511        0.770\n  183    156       1   0.7035 0.03041       0.6464        0.766\n  186    154       1   0.6989 0.03056       0.6416        0.761\n  189    152       1   0.6943 0.03070       0.6367        0.757\n  194    149       1   0.6897 0.03085       0.6318        0.753\n  197    147       1   0.6850 0.03099       0.6269        0.749\n  199    145       1   0.6803 0.03113       0.6219        0.744\n  201    144       2   0.6708 0.03141       0.6120        0.735\n  202    142       1   0.6661 0.03154       0.6071        0.731\n  207    139       1   0.6613 0.03168       0.6020        0.726\n  208    138       1   0.6565 0.03181       0.5970        0.722\n  210    137       1   0.6517 0.03194       0.5920        0.717\n  212    135       1   0.6469 0.03206       0.5870        0.713\n  218    134       1   0.6421 0.03218       0.5820        0.708\n  222    132       1   0.6372 0.03231       0.5769        0.704\n  223    130       1   0.6323 0.03243       0.5718        0.699\n  226    126       1   0.6273 0.03256       0.5666        0.694\n  229    125       1   0.6223 0.03268       0.5614        0.690\n  230    124       1   0.6172 0.03280       0.5562        0.685\n  239    121       2   0.6070 0.03304       0.5456        0.675\n  245    117       1   0.6019 0.03316       0.5402        0.670\n  246    116       1   0.5967 0.03328       0.5349        0.666\n  267    112       1   0.5913 0.03341       0.5294        0.661\n  268    111       1   0.5860 0.03353       0.5239        0.656\n  269    110       1   0.5807 0.03364       0.5184        0.651\n  270    108       1   0.5753 0.03376       0.5128        0.645\n  283    104       1   0.5698 0.03388       0.5071        0.640\n  284    103       1   0.5642 0.03400       0.5014        0.635\n  285    101       2   0.5531 0.03424       0.4899        0.624\n  286     99       1   0.5475 0.03434       0.4841        0.619\n  288     98       1   0.5419 0.03444       0.4784        0.614\n  291     97       1   0.5363 0.03454       0.4727        0.608\n  293     94       1   0.5306 0.03464       0.4669        0.603\n  301     91       1   0.5248 0.03475       0.4609        0.597\n  303     89       1   0.5189 0.03485       0.4549        0.592\n  305     87       1   0.5129 0.03496       0.4488        0.586\n  306     86       1   0.5070 0.03506       0.4427        0.581\n  310     85       2   0.4950 0.03523       0.4306        0.569\n  320     82       1   0.4890 0.03532       0.4244        0.563\n  329     81       1   0.4830 0.03539       0.4183        0.558\n  337     79       1   0.4768 0.03547       0.4121        0.552\n  340     78       1   0.4707 0.03554       0.4060        0.546\n  345     77       1   0.4646 0.03560       0.3998        0.540\n  348     76       1   0.4585 0.03565       0.3937        0.534\n  350     75       1   0.4524 0.03569       0.3876        0.528\n  351     74       1   0.4463 0.03573       0.3815        0.522\n  353     73       2   0.4340 0.03578       0.3693        0.510\n  361     70       1   0.4278 0.03581       0.3631        0.504\n  363     69       2   0.4154 0.03583       0.3508        0.492\n  364     67       1   0.4092 0.03582       0.3447        0.486\n  371     65       2   0.3966 0.03581       0.3323        0.473\n  387     60       1   0.3900 0.03582       0.3258        0.467\n  390     59       1   0.3834 0.03582       0.3193        0.460\n  394     58       1   0.3768 0.03580       0.3128        0.454\n  426     55       1   0.3700 0.03580       0.3060        0.447\n  428     54       1   0.3631 0.03579       0.2993        0.440\n  429     53       1   0.3563 0.03576       0.2926        0.434\n  433     52       1   0.3494 0.03573       0.2860        0.427\n  442     51       1   0.3426 0.03568       0.2793        0.420\n  444     50       1   0.3357 0.03561       0.2727        0.413\n  450     48       1   0.3287 0.03555       0.2659        0.406\n  455     47       1   0.3217 0.03548       0.2592        0.399\n  457     46       1   0.3147 0.03539       0.2525        0.392\n  460     44       1   0.3076 0.03530       0.2456        0.385\n  473     43       1   0.3004 0.03520       0.2388        0.378\n  477     42       1   0.2933 0.03508       0.2320        0.371\n  519     39       1   0.2857 0.03498       0.2248        0.363\n  520     38       1   0.2782 0.03485       0.2177        0.356\n  524     37       2   0.2632 0.03455       0.2035        0.340\n  533     34       1   0.2554 0.03439       0.1962        0.333\n  550     32       1   0.2475 0.03423       0.1887        0.325\n  558     30       1   0.2392 0.03407       0.1810        0.316\n  567     28       1   0.2307 0.03391       0.1729        0.308\n  574     27       1   0.2221 0.03371       0.1650        0.299\n  583     26       1   0.2136 0.03348       0.1571        0.290\n  613     24       1   0.2047 0.03325       0.1489        0.281\n  624     23       1   0.1958 0.03297       0.1407        0.272\n  641     22       1   0.1869 0.03265       0.1327        0.263\n  643     21       1   0.1780 0.03229       0.1247        0.254\n  654     20       1   0.1691 0.03188       0.1169        0.245\n  655     19       1   0.1602 0.03142       0.1091        0.235\n  687     18       1   0.1513 0.03090       0.1014        0.226\n  689     17       1   0.1424 0.03034       0.0938        0.216\n  705     16       1   0.1335 0.02972       0.0863        0.207\n  707     15       1   0.1246 0.02904       0.0789        0.197\n  728     14       1   0.1157 0.02830       0.0716        0.187\n  731     13       1   0.1068 0.02749       0.0645        0.177\n  735     12       1   0.0979 0.02660       0.0575        0.167\n  765     10       1   0.0881 0.02568       0.0498        0.156\n  791      9       1   0.0783 0.02462       0.0423        0.145\n  814      7       1   0.0671 0.02351       0.0338        0.133\n  883      4       1   0.0503 0.02285       0.0207        0.123\n\n\nThese tables show a row for each time point where either the event occured or a sample was censored. It shows the number at risk (number still remaining), and the cumulative survival at that instant.\nWhat’s more interesting though is if we model something besides just an intercept. Let’s fit survival curves separately by sex.\n\n\nCode\nsfit &lt;- survfit(Surv(time, status)~sex, data=lung)\nsfit\n\n\nCall: survfit(formula = Surv(time, status) ~ sex, data = lung)\n\n        n events median 0.95LCL 0.95UCL\nsex=1 138    112    270     212     310\nsex=2  90     53    426     348     550\n\n\nCode\nsummary(sfit)\n\n\nCall: survfit(formula = Surv(time, status) ~ sex, data = lung)\n\n                sex=1 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n   11    138       3   0.9783  0.0124       0.9542        1.000\n   12    135       1   0.9710  0.0143       0.9434        0.999\n   13    134       2   0.9565  0.0174       0.9231        0.991\n   15    132       1   0.9493  0.0187       0.9134        0.987\n   26    131       1   0.9420  0.0199       0.9038        0.982\n   30    130       1   0.9348  0.0210       0.8945        0.977\n   31    129       1   0.9275  0.0221       0.8853        0.972\n   53    128       2   0.9130  0.0240       0.8672        0.961\n   54    126       1   0.9058  0.0249       0.8583        0.956\n   59    125       1   0.8986  0.0257       0.8496        0.950\n   60    124       1   0.8913  0.0265       0.8409        0.945\n   65    123       2   0.8768  0.0280       0.8237        0.933\n   71    121       1   0.8696  0.0287       0.8152        0.928\n   81    120       1   0.8623  0.0293       0.8067        0.922\n   88    119       2   0.8478  0.0306       0.7900        0.910\n   92    117       1   0.8406  0.0312       0.7817        0.904\n   93    116       1   0.8333  0.0317       0.7734        0.898\n   95    115       1   0.8261  0.0323       0.7652        0.892\n  105    114       1   0.8188  0.0328       0.7570        0.886\n  107    113       1   0.8116  0.0333       0.7489        0.880\n  110    112       1   0.8043  0.0338       0.7408        0.873\n  116    111       1   0.7971  0.0342       0.7328        0.867\n  118    110       1   0.7899  0.0347       0.7247        0.861\n  131    109       1   0.7826  0.0351       0.7167        0.855\n  132    108       2   0.7681  0.0359       0.7008        0.842\n  135    106       1   0.7609  0.0363       0.6929        0.835\n  142    105       1   0.7536  0.0367       0.6851        0.829\n  144    104       1   0.7464  0.0370       0.6772        0.823\n  147    103       1   0.7391  0.0374       0.6694        0.816\n  156    102       2   0.7246  0.0380       0.6538        0.803\n  163    100       3   0.7029  0.0389       0.6306        0.783\n  166     97       1   0.6957  0.0392       0.6230        0.777\n  170     96       1   0.6884  0.0394       0.6153        0.770\n  175     94       1   0.6811  0.0397       0.6076        0.763\n  176     93       1   0.6738  0.0399       0.5999        0.757\n  177     92       1   0.6664  0.0402       0.5922        0.750\n  179     91       2   0.6518  0.0406       0.5769        0.736\n  180     89       1   0.6445  0.0408       0.5693        0.730\n  181     88       2   0.6298  0.0412       0.5541        0.716\n  183     86       1   0.6225  0.0413       0.5466        0.709\n  189     83       1   0.6150  0.0415       0.5388        0.702\n  197     80       1   0.6073  0.0417       0.5309        0.695\n  202     78       1   0.5995  0.0419       0.5228        0.687\n  207     77       1   0.5917  0.0420       0.5148        0.680\n  210     76       1   0.5839  0.0422       0.5068        0.673\n  212     75       1   0.5762  0.0424       0.4988        0.665\n  218     74       1   0.5684  0.0425       0.4909        0.658\n  222     72       1   0.5605  0.0426       0.4829        0.651\n  223     70       1   0.5525  0.0428       0.4747        0.643\n  229     67       1   0.5442  0.0429       0.4663        0.635\n  230     66       1   0.5360  0.0431       0.4579        0.627\n  239     64       1   0.5276  0.0432       0.4494        0.619\n  246     63       1   0.5192  0.0433       0.4409        0.611\n  267     61       1   0.5107  0.0434       0.4323        0.603\n  269     60       1   0.5022  0.0435       0.4238        0.595\n  270     59       1   0.4937  0.0436       0.4152        0.587\n  283     57       1   0.4850  0.0437       0.4065        0.579\n  284     56       1   0.4764  0.0438       0.3979        0.570\n  285     54       1   0.4676  0.0438       0.3891        0.562\n  286     53       1   0.4587  0.0439       0.3803        0.553\n  288     52       1   0.4499  0.0439       0.3716        0.545\n  291     51       1   0.4411  0.0439       0.3629        0.536\n  301     48       1   0.4319  0.0440       0.3538        0.527\n  303     46       1   0.4225  0.0440       0.3445        0.518\n  306     44       1   0.4129  0.0440       0.3350        0.509\n  310     43       1   0.4033  0.0441       0.3256        0.500\n  320     42       1   0.3937  0.0440       0.3162        0.490\n  329     41       1   0.3841  0.0440       0.3069        0.481\n  337     40       1   0.3745  0.0439       0.2976        0.471\n  353     39       2   0.3553  0.0437       0.2791        0.452\n  363     37       1   0.3457  0.0436       0.2700        0.443\n  364     36       1   0.3361  0.0434       0.2609        0.433\n  371     35       1   0.3265  0.0432       0.2519        0.423\n  387     34       1   0.3169  0.0430       0.2429        0.413\n  390     33       1   0.3073  0.0428       0.2339        0.404\n  394     32       1   0.2977  0.0425       0.2250        0.394\n  428     29       1   0.2874  0.0423       0.2155        0.383\n  429     28       1   0.2771  0.0420       0.2060        0.373\n  442     27       1   0.2669  0.0417       0.1965        0.362\n  455     25       1   0.2562  0.0413       0.1868        0.351\n  457     24       1   0.2455  0.0410       0.1770        0.341\n  460     22       1   0.2344  0.0406       0.1669        0.329\n  477     21       1   0.2232  0.0402       0.1569        0.318\n  519     20       1   0.2121  0.0397       0.1469        0.306\n  524     19       1   0.2009  0.0391       0.1371        0.294\n  533     18       1   0.1897  0.0385       0.1275        0.282\n  558     17       1   0.1786  0.0378       0.1179        0.270\n  567     16       1   0.1674  0.0371       0.1085        0.258\n  574     15       1   0.1562  0.0362       0.0992        0.246\n  583     14       1   0.1451  0.0353       0.0900        0.234\n  613     13       1   0.1339  0.0343       0.0810        0.221\n  624     12       1   0.1228  0.0332       0.0722        0.209\n  643     11       1   0.1116  0.0320       0.0636        0.196\n  655     10       1   0.1004  0.0307       0.0552        0.183\n  689      9       1   0.0893  0.0293       0.0470        0.170\n  707      8       1   0.0781  0.0276       0.0390        0.156\n  791      7       1   0.0670  0.0259       0.0314        0.143\n  814      5       1   0.0536  0.0239       0.0223        0.128\n  883      3       1   0.0357  0.0216       0.0109        0.117\n\n                sex=2 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    5     90       1   0.9889  0.0110       0.9675        1.000\n   60     89       1   0.9778  0.0155       0.9478        1.000\n   61     88       1   0.9667  0.0189       0.9303        1.000\n   62     87       1   0.9556  0.0217       0.9139        0.999\n   79     86       1   0.9444  0.0241       0.8983        0.993\n   81     85       1   0.9333  0.0263       0.8832        0.986\n   95     83       1   0.9221  0.0283       0.8683        0.979\n  107     81       1   0.9107  0.0301       0.8535        0.972\n  122     80       1   0.8993  0.0318       0.8390        0.964\n  145     79       2   0.8766  0.0349       0.8108        0.948\n  153     77       1   0.8652  0.0362       0.7970        0.939\n  166     76       1   0.8538  0.0375       0.7834        0.931\n  167     75       1   0.8424  0.0387       0.7699        0.922\n  182     71       1   0.8305  0.0399       0.7559        0.913\n  186     70       1   0.8187  0.0411       0.7420        0.903\n  194     68       1   0.8066  0.0422       0.7280        0.894\n  199     67       1   0.7946  0.0432       0.7142        0.884\n  201     66       2   0.7705  0.0452       0.6869        0.864\n  208     62       1   0.7581  0.0461       0.6729        0.854\n  226     59       1   0.7452  0.0471       0.6584        0.843\n  239     57       1   0.7322  0.0480       0.6438        0.833\n  245     54       1   0.7186  0.0490       0.6287        0.821\n  268     51       1   0.7045  0.0501       0.6129        0.810\n  285     47       1   0.6895  0.0512       0.5962        0.798\n  293     45       1   0.6742  0.0523       0.5791        0.785\n  305     43       1   0.6585  0.0534       0.5618        0.772\n  310     42       1   0.6428  0.0544       0.5447        0.759\n  340     39       1   0.6264  0.0554       0.5267        0.745\n  345     38       1   0.6099  0.0563       0.5089        0.731\n  348     37       1   0.5934  0.0572       0.4913        0.717\n  350     36       1   0.5769  0.0579       0.4739        0.702\n  351     35       1   0.5604  0.0586       0.4566        0.688\n  361     33       1   0.5434  0.0592       0.4390        0.673\n  363     32       1   0.5265  0.0597       0.4215        0.658\n  371     30       1   0.5089  0.0603       0.4035        0.642\n  426     26       1   0.4893  0.0610       0.3832        0.625\n  433     25       1   0.4698  0.0617       0.3632        0.608\n  444     24       1   0.4502  0.0621       0.3435        0.590\n  450     23       1   0.4306  0.0624       0.3241        0.572\n  473     22       1   0.4110  0.0626       0.3050        0.554\n  520     19       1   0.3894  0.0629       0.2837        0.534\n  524     18       1   0.3678  0.0630       0.2628        0.515\n  550     15       1   0.3433  0.0634       0.2390        0.493\n  641     11       1   0.3121  0.0649       0.2076        0.469\n  654     10       1   0.2808  0.0655       0.1778        0.443\n  687      9       1   0.2496  0.0652       0.1496        0.417\n  705      8       1   0.2184  0.0641       0.1229        0.388\n  728      7       1   0.1872  0.0621       0.0978        0.359\n  731      6       1   0.1560  0.0590       0.0743        0.328\n  735      5       1   0.1248  0.0549       0.0527        0.295\n  765      3       1   0.0832  0.0499       0.0257        0.270\n\n\nNow, check out the help for ?summary.survfit. You can give the summary() function an option for what times you want to show in the results. Look at the range of followup times in the lung dataset with range(). You can create a sequence of numbers going from one number to another number by increments of yet another number with the seq() function.\n\n\nCode\n# ?summary.survfit\nrange(lung$time)\n\n\n[1]    5 1022\n\n\nCode\nseq(0, 1100, 100)\n\n\n [1]    0  100  200  300  400  500  600  700  800  900 1000 1100\n\n\nAnd we can use that sequence vector with a summary call on sfit to get life tables at those intervals separately for both males (1) and females (2). From these tables we can start to see that males tend to have worse survival than females.\n\n\nCode\nsummary(sfit, times=seq(0, 1000, 100))\n\n\nCall: survfit(formula = Surv(time, status) ~ sex, data = lung)\n\n                sex=1 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0    138       0   1.0000  0.0000       1.0000        1.000\n  100    114      24   0.8261  0.0323       0.7652        0.892\n  200     78      30   0.6073  0.0417       0.5309        0.695\n  300     49      20   0.4411  0.0439       0.3629        0.536\n  400     31      15   0.2977  0.0425       0.2250        0.394\n  500     20       7   0.2232  0.0402       0.1569        0.318\n  600     13       7   0.1451  0.0353       0.0900        0.234\n  700      8       5   0.0893  0.0293       0.0470        0.170\n  800      6       2   0.0670  0.0259       0.0314        0.143\n  900      2       2   0.0357  0.0216       0.0109        0.117\n 1000      2       0   0.0357  0.0216       0.0109        0.117\n\n                sex=2 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     90       0   1.0000  0.0000       1.0000        1.000\n  100     82       7   0.9221  0.0283       0.8683        0.979\n  200     66      11   0.7946  0.0432       0.7142        0.884\n  300     43       9   0.6742  0.0523       0.5791        0.785\n  400     26      10   0.5089  0.0603       0.4035        0.642\n  500     21       5   0.4110  0.0626       0.3050        0.554\n  600     11       3   0.3433  0.0634       0.2390        0.493\n  700      8       3   0.2496  0.0652       0.1496        0.417\n  800      2       5   0.0832  0.0499       0.0257        0.270\n  900      1       0   0.0832  0.0499       0.0257        0.270\n\n\n\n\n\nNow that we’ve fit a survival curve to the data it’s pretty easy to visualize it with a Kaplan-Meier plot. Create the survival object if you don’t have it yet, and instead of using summary(), use plot() instead.\n\n\nCode\nsfit &lt;- survfit(Surv(time, status)~sex, data=lung)\nplot(sfit)\n\n\n\n\n\n\n\n\n\nThere are lots of ways to modify the plot produced by base R’s plot() function. You can see more options with the help for ?plot.survfit. We’re not going to go into any more detail here, because there’s another package called survminer that provides a function called ggsurvplot() that makes it much easier to produce publication-ready survival plots, and if you’re familiar with ggplot2 syntax it’s pretty easy to modify. So, let’s load the package and try it out.\n\n\nCode\nlibrary(survminer)\nggsurvplot(sfit)\n\n\n\n\n\n\n\n\n\nThis plot is substantially more informative by default, just because it automatically color codes the different groups, adds axis labels, and creates and automatic legend. But there’s a lot more you can do pretty easily here. Let’s add confidence intervals, show the p-value for the log-rank test, show a risk table below the plot, and change the colors and the group labels.\n\n\nCode\nggsurvplot(sfit, conf.int=TRUE, pval=TRUE, risk.table=TRUE, \n           legend.labs=c(\"Male\", \"Female\"), legend.title=\"Sex\",  \n           palette=c(\"dodgerblue2\", \"orchid2\"), \n           title=\"Kaplan-Meier Curve for Lung Cancer Survival\", \n           risk.table.height=.15)"
  },
  {
    "objectID": "projects/Project7/index.html#background",
    "href": "projects/Project7/index.html#background",
    "title": "Survival Analysis using R",
    "section": "",
    "text": "In the class on essential statistics we covered basic categorical data analysis – comparing proportions (risks, rates, etc) between different groups using a chi-square or fisher exact test, or logistic regression. For example, we looked at how the diabetes rate differed between males and females. In this kind of analysis you implicitly assume that the rates are constant over the period of the study, or as defined by the different groups you defined.\nBut, in longitudinal studies where you track samples or subjects from one time point (e.g., entry into a study, diagnosis, start of a treatment) until you observe some outcome event (e.g., death, onset of disease, relapse), it doesn’t make sense to assume the rates are constant. For example: the risk of death after heart surgery is highest immediately post-op, decreases as the patient recovers, then rises slowly again as the patient ages. Or, recurrence rate of different cancers varies highly over time, and depends on tumor genetics, treatment, and other environmental factors.\n\n\nSurvival analysis lets you analyze the rates of occurrence of events over time, without assuming the rates are constant. Generally, survival analysis lets you model the time until an event occurs,1 or compare the time-to-event between different groups, or how time-to-event correlates with quantitative variables.\nThe hazard is the instantaneous event (death) rate at a particular time point t. Survival analysis doesn’t assume the hazard is constant over time. The cumulative hazard is the total hazard experienced up to time t.\nThe survival function, is the probability an individual survives (or, the probability that the event of interest does not occur) up to and including time t. It’s the probability that the event (e.g., death) hasn’t occured yet. It looks like this, where TT is the time of death, and Pr(T&gt;t)Pr(T&gt;t) is the probability that the time of death is greater than some time tt. SS is a probability, so 0≤S(t)≤10≤S(t)≤1, since survival times are always positive (T≥0T≥0).\nS(t)=Pr(T&gt;t)S(t)=Pr(T&gt;t)\nThe Kaplan-Meier curve illustrates the survival function. It’s a step function illustrating the cumulative survival probability over time. The curve is horizontal over periods where no event occurs, then drops vertically corresponding to a change in the survival function at each time an event occurs.\nCensoring is a type of missing data problem unique to survival analysis. This happens when you track the sample/subject through the end of the study and the event never occurs. This could also happen due to the sample/subject dropping out of the study for reasons other than death, or some other loss to followup. The sample is censored in that you only know that the individual survived up to the loss to followup, but you don’t know anything about survival after that.\nProportional hazards assumption: The main goal of survival analysis is to compare the survival functions in different groups, e.g., leukemia patients as compared to cancer-free controls. If you followed both groups until everyone died, both survival curves would end at 0%, but one group might have survived on average a lot longer than the other group. Survival analysis does this by comparing the hazard at different times over the observation period. Survival analysis doesn’t assume that the hazard is constant, but does assume that the ratio of hazards between groups is constant over time. This class does not cover methods to deal with non-proportional hazards, or interactions of covariates with the time to event.\nProportional hazards regression a.k.a. Cox regression is the most common approach to assess the effect of different variables on survival.\n\n\n\nKaplan-Meier curves are good for visualizing differences in survival between two categorical groups, but they don’t work well for assessing the effect of quantitative variables like age, gene expression, leukocyte count, etc. Cox PH regression can assess the effect of both categorical and continuous variables, and can model the effect of multiple variables at once.\nCox PH regression models the natural log of the hazard at time t, denoted h(t)h(t), as a function of the baseline hazard (h0(t)h0(t)) (the hazard for an individual where all exposure variables are 0) and multiple exposure variables x1x1, x1x1, ……, xpxp. The form of the Cox PH model is:\nlog(h(t))=log(h0(t))+β1x1+β2x2+…+βpxplog(h(t))=log(h0(t))+β1x1+β2x2+…+βpxp\nIf you exponentiate both sides of the equation, and limit the right hand side to just a single categorical exposure variable (x1x1) with two groups (x1=1x1=1 for exposed and x1=0x1=0 for unexposed), the equation becomes:\nh1(t)=h0(t)×eβ1x1h1(t)=h0(t)×eβ1x1\nRearranging that equation lets you estimate the hazard ratio, comparing the exposed to the unexposed individuals at time t:\nHR(t)=h1(t)h0(t)=eβ1HR(t)=h1(t)h0(t)=eβ1\nThis model shows that the hazard ratio is eβ1eβ1, and remains constant over time t (hence the name proportional hazards regression). The ββ values are the regression coefficients that are estimated from the model, and represent the log(HazardRatio)log(HazardRatio) for each unit increase in the corresponding predictor variable. The interpretation of the hazards ratio depends on the measurement scale of the predictor variable, but in simple terms, a positive coefficient indicates worse survival and a negative coefficient indicates better survival for the variable in question."
  },
  {
    "objectID": "projects/Project7/index.html#survival-analysis-in-r",
    "href": "projects/Project7/index.html#survival-analysis-in-r",
    "title": "Survival Analysis using R",
    "section": "",
    "text": "The core survival analysis functions are in the survival package. The survival package is one of the few “core” packages that comes bundled with your basic R installation, so you probably didn’t need to install.packages() it. But, you’ll need to load it like any other library when you want to use it. We’ll also be using the dplyr package, so let’s load that too. Finally, we’ll also want to load the survminer package, which provides much nicer Kaplan-Meier plots out-of-the-box than what you get out of base graphics.\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttachement du package : 'dplyr'\n\n\nLes objets suivants sont masqués depuis 'package:stats':\n\n    filter, lag\n\n\nLes objets suivants sont masqués depuis 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(survival)\nlibrary(survminer)\n\n\nLe chargement a nécessité le package : ggplot2\n\n\nLe chargement a nécessité le package : ggpubr\n\n\n\nAttachement du package : 'survminer'\n\n\nL'objet suivant est masqué depuis 'package:survival':\n\n    myeloma\n\n\nThe core functions we’ll use out of the survival package include:\n\nSurv(): Creates a survival object.\nsurvfit(): Fits a survival curve using either a formula, of from a previously fitted Cox model.\ncoxph(): Fits a Cox proportional hazards regression model.\n\nOther optional functions you might use include:\n\ncox.zph(): Tests the proportional hazards assumption of a Cox regression model.\nsurvdiff(): Tests for differences in survival between two groups using a log-rank / Mantel-Haenszel test.\n\nSurv() creates the response variable, and typical usage takes the time to event, and whether or not the event occured (i.e., death vs censored). survfit() creates a survival curve that you could then display or plot. coxph() implements the regression analysis, and models specified the same way as in regular linear models, but using the coxph() function.\n\n\nWe’re going to be using the built-in lung cancer dataset that ships with the survival package. You can get some more information about the dataset by running ?lung. The help tells us there are 10 variables in this data:\n\n\nCode\nlibrary(survival)\n?lung\n\n\ndémarrage du serveur d'aide httpd ... fini\n\n\n\ninst: Institution code\ntime: Survival time in days\nstatus: censoring status 1=censored, 2=dead\nage: Age in years\nsex: Male=1 Female=2\nph.ecog: ECOG performance score (0=good 5=dead)\nph.karno: Karnofsky performance score as rated by physician\npat.karno: Karnofsky performance score as rated by patient\nmeal.cal: Calories consumed at meals\nwt.loss: Weight loss in last six months\n\nYou can access the data just by running lung, as if you had read in a dataset and called it lung. You can operate on it just like any other data frame.\n\n\nCode\nhead(lung)\n\n\n  inst time status age sex ph.ecog ph.karno pat.karno meal.cal wt.loss\n1    3  306      2  74   1       1       90       100     1175      NA\n2    3  455      2  68   1       0       90        90     1225      15\n3    3 1010      1  56   1       0       90        90       NA      15\n4    5  210      2  57   1       1       90        60     1150      11\n5    1  883      2  60   1       0      100        90       NA       0\n6   12 1022      1  74   1       1       50        80      513       0\n\n\nCode\nclass(lung)\n\n\n[1] \"data.frame\"\n\n\nCode\ndim(lung)\n\n\n[1] 228  10\n\n\nCode\nView(lung)\n\n\nNotice that lung is a plain data.frame object. You could see what it looks like as a tibble (prints nicely, tells you the type of variable each column is). You could then reassign lung to the as_tibble()-ified version.\n\n\nCode\nas_tibble(lung)\n\n\n# A tibble: 228 × 10\n    inst  time status   age   sex ph.ecog ph.karno pat.karno meal.cal wt.loss\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1     3   306      2    74     1       1       90       100     1175      NA\n 2     3   455      2    68     1       0       90        90     1225      15\n 3     3  1010      1    56     1       0       90        90       NA      15\n 4     5   210      2    57     1       1       90        60     1150      11\n 5     1   883      2    60     1       0      100        90       NA       0\n 6    12  1022      1    74     1       1       50        80      513       0\n 7     7   310      2    68     2       2       70        60      384      10\n 8    11   361      2    71     2       2       60        80      538       1\n 9     1   218      2    53     1       1       70        80      825      16\n10     7   166      2    61     1       2       70        70      271      34\n# ℹ 218 more rows\n\n\nCode\nlung &lt;- as_tibble(lung)\nlung\n\n\n# A tibble: 228 × 10\n    inst  time status   age   sex ph.ecog ph.karno pat.karno meal.cal wt.loss\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1     3   306      2    74     1       1       90       100     1175      NA\n 2     3   455      2    68     1       0       90        90     1225      15\n 3     3  1010      1    56     1       0       90        90       NA      15\n 4     5   210      2    57     1       1       90        60     1150      11\n 5     1   883      2    60     1       0      100        90       NA       0\n 6    12  1022      1    74     1       1       50        80      513       0\n 7     7   310      2    68     2       2       70        60      384      10\n 8    11   361      2    71     2       2       60        80      538       1\n 9     1   218      2    53     1       1       70        80      825      16\n10     7   166      2    61     1       2       70        70      271      34\n# ℹ 218 more rows\n\n\n\n\n\nCheck out the help for ?Surv. This is the main function we’ll use to create the survival object. You can play fast and loose with how you specify the arguments to Surv. The help tells you that when there are two unnamed arguments, they will match time and event in that order. This is the common shorthand you’ll often see for right-censored data. The alternative lets you specify interval data, where you give it the start and end times (time and time2). If you keep reading you’ll see how Surv tries to guess how you’re coding the status variable. It will try to guess whether you’re using 0/1 or 1/2 to represent censored vs “dead”, respectively.\nTry creating a survival object called s, then display it. If you go back and head(lung) the data, you can see how these are related. It’s a special type of vector that tells you both how long the subject was tracked for, and whether or not the event occured or the sample was censored (shown by the +).\n\n\nCode\ns &lt;- Surv(lung$time, lung$status)\nclass(s)\n\n\n[1] \"Surv\"\n\n\nCode\ns\n\n\n  [1]  306   455  1010+  210   883  1022+  310   361   218   166   170   654 \n [13]  728    71   567   144   613   707    61    88   301    81   624   371 \n [25]  394   520   574   118   390    12   473    26   533   107    53   122 \n [37]  814   965+   93   731   460   153   433   145   583    95   303   519 \n [49]  643   765   735   189    53   246   689    65     5   132   687   345 \n [61]  444   223   175    60   163    65   208   821+  428   230   840+  305 \n [73]   11   132   226   426   705   363    11   176   791    95   196+  167 \n [85]  806+  284   641   147   740+  163   655   239    88   245   588+   30 \n [97]  179   310   477   166   559+  450   364   107   177   156   529+   11 \n[109]  429   351    15   181   283   201   524    13   212   524   288   363 \n[121]  442   199   550    54   558   207    92    60   551+  543+  293   202 \n[133]  353   511+  267   511+  371   387   457   337   201   404+  222    62 \n[145]  458+  356+  353   163    31   340   229   444+  315+  182   156   329 \n[157]  364+  291   179   376+  384+  268   292+  142   413+  266+  194   320 \n[169]  181   285   301+  348   197   382+  303+  296+  180   186   145   269+\n[181]  300+  284+  350   272+  292+  332+  285   259+  110   286   270    81 \n[193]  131   225+  269   225+  243+  279+  276+  135    79    59   240+  202+\n[205]  235+  105   224+  239   237+  173+  252+  221+  185+   92+   13   222+\n[217]  192+  183   211+  175+  197+  203+  116   188+  191+  105+  174+  177+\n\n\nCode\nhead(lung)\n\n\n# A tibble: 6 × 10\n   inst  time status   age   sex ph.ecog ph.karno pat.karno meal.cal wt.loss\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     3   306      2    74     1       1       90       100     1175      NA\n2     3   455      2    68     1       0       90        90     1225      15\n3     3  1010      1    56     1       0       90        90       NA      15\n4     5   210      2    57     1       1       90        60     1150      11\n5     1   883      2    60     1       0      100        90       NA       0\n6    12  1022      1    74     1       1       50        80      513       0\n\n\nNow, let’s fit a survival curve with the survfit() function. See the help for ?survfit. Here we’ll create a simple survival curve that doesn’t consider any different groupings, so we’ll specify just an intercept (e.g., ~1) in the formula that survfit expects. We can do what we just did by “modeling” the survival object s we just created against an intercept only, but from here out, we’ll just do this in one step by nesting the Surv() call within the survfit() call, and similar to how we specify data for linear models with lm(), we’ll use the data= argument to specify which data we’re using. Similarly, we can assign that to another object called sfit (or whatever we wanted to call it).\n\n\nCode\nsurvfit(s~1)\n\n\nCall: survfit(formula = s ~ 1)\n\n       n events median 0.95LCL 0.95UCL\n[1,] 228    165    310     285     363\n\n\nCode\nsurvfit(Surv(time, status)~1, data=lung)\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung)\n\n       n events median 0.95LCL 0.95UCL\n[1,] 228    165    310     285     363\n\n\nCode\nsfit &lt;- survfit(Surv(time, status)~1, data=lung)\nsfit\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung)\n\n       n events median 0.95LCL 0.95UCL\n[1,] 228    165    310     285     363\n\n\nNow, that object itself isn’t very interesting. It’s more interesting to run summary on what it creates. This will show a life table.\n\n\nCode\nsummary(sfit)\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    5    228       1   0.9956 0.00438       0.9871        1.000\n   11    227       3   0.9825 0.00869       0.9656        1.000\n   12    224       1   0.9781 0.00970       0.9592        0.997\n   13    223       2   0.9693 0.01142       0.9472        0.992\n   15    221       1   0.9649 0.01219       0.9413        0.989\n   26    220       1   0.9605 0.01290       0.9356        0.986\n   30    219       1   0.9561 0.01356       0.9299        0.983\n   31    218       1   0.9518 0.01419       0.9243        0.980\n   53    217       2   0.9430 0.01536       0.9134        0.974\n   54    215       1   0.9386 0.01590       0.9079        0.970\n   59    214       1   0.9342 0.01642       0.9026        0.967\n   60    213       2   0.9254 0.01740       0.8920        0.960\n   61    211       1   0.9211 0.01786       0.8867        0.957\n   62    210       1   0.9167 0.01830       0.8815        0.953\n   65    209       2   0.9079 0.01915       0.8711        0.946\n   71    207       1   0.9035 0.01955       0.8660        0.943\n   79    206       1   0.8991 0.01995       0.8609        0.939\n   81    205       2   0.8904 0.02069       0.8507        0.932\n   88    203       2   0.8816 0.02140       0.8406        0.925\n   92    201       1   0.8772 0.02174       0.8356        0.921\n   93    199       1   0.8728 0.02207       0.8306        0.917\n   95    198       2   0.8640 0.02271       0.8206        0.910\n  105    196       1   0.8596 0.02302       0.8156        0.906\n  107    194       2   0.8507 0.02362       0.8056        0.898\n  110    192       1   0.8463 0.02391       0.8007        0.894\n  116    191       1   0.8418 0.02419       0.7957        0.891\n  118    190       1   0.8374 0.02446       0.7908        0.887\n  122    189       1   0.8330 0.02473       0.7859        0.883\n  131    188       1   0.8285 0.02500       0.7810        0.879\n  132    187       2   0.8197 0.02550       0.7712        0.871\n  135    185       1   0.8153 0.02575       0.7663        0.867\n  142    184       1   0.8108 0.02598       0.7615        0.863\n  144    183       1   0.8064 0.02622       0.7566        0.859\n  145    182       2   0.7975 0.02667       0.7469        0.852\n  147    180       1   0.7931 0.02688       0.7421        0.848\n  153    179       1   0.7887 0.02710       0.7373        0.844\n  156    178       2   0.7798 0.02751       0.7277        0.836\n  163    176       3   0.7665 0.02809       0.7134        0.824\n  166    173       2   0.7577 0.02845       0.7039        0.816\n  167    171       1   0.7532 0.02863       0.6991        0.811\n  170    170       1   0.7488 0.02880       0.6944        0.807\n  175    167       1   0.7443 0.02898       0.6896        0.803\n  176    165       1   0.7398 0.02915       0.6848        0.799\n  177    164       1   0.7353 0.02932       0.6800        0.795\n  179    162       2   0.7262 0.02965       0.6704        0.787\n  180    160       1   0.7217 0.02981       0.6655        0.783\n  181    159       2   0.7126 0.03012       0.6559        0.774\n  182    157       1   0.7081 0.03027       0.6511        0.770\n  183    156       1   0.7035 0.03041       0.6464        0.766\n  186    154       1   0.6989 0.03056       0.6416        0.761\n  189    152       1   0.6943 0.03070       0.6367        0.757\n  194    149       1   0.6897 0.03085       0.6318        0.753\n  197    147       1   0.6850 0.03099       0.6269        0.749\n  199    145       1   0.6803 0.03113       0.6219        0.744\n  201    144       2   0.6708 0.03141       0.6120        0.735\n  202    142       1   0.6661 0.03154       0.6071        0.731\n  207    139       1   0.6613 0.03168       0.6020        0.726\n  208    138       1   0.6565 0.03181       0.5970        0.722\n  210    137       1   0.6517 0.03194       0.5920        0.717\n  212    135       1   0.6469 0.03206       0.5870        0.713\n  218    134       1   0.6421 0.03218       0.5820        0.708\n  222    132       1   0.6372 0.03231       0.5769        0.704\n  223    130       1   0.6323 0.03243       0.5718        0.699\n  226    126       1   0.6273 0.03256       0.5666        0.694\n  229    125       1   0.6223 0.03268       0.5614        0.690\n  230    124       1   0.6172 0.03280       0.5562        0.685\n  239    121       2   0.6070 0.03304       0.5456        0.675\n  245    117       1   0.6019 0.03316       0.5402        0.670\n  246    116       1   0.5967 0.03328       0.5349        0.666\n  267    112       1   0.5913 0.03341       0.5294        0.661\n  268    111       1   0.5860 0.03353       0.5239        0.656\n  269    110       1   0.5807 0.03364       0.5184        0.651\n  270    108       1   0.5753 0.03376       0.5128        0.645\n  283    104       1   0.5698 0.03388       0.5071        0.640\n  284    103       1   0.5642 0.03400       0.5014        0.635\n  285    101       2   0.5531 0.03424       0.4899        0.624\n  286     99       1   0.5475 0.03434       0.4841        0.619\n  288     98       1   0.5419 0.03444       0.4784        0.614\n  291     97       1   0.5363 0.03454       0.4727        0.608\n  293     94       1   0.5306 0.03464       0.4669        0.603\n  301     91       1   0.5248 0.03475       0.4609        0.597\n  303     89       1   0.5189 0.03485       0.4549        0.592\n  305     87       1   0.5129 0.03496       0.4488        0.586\n  306     86       1   0.5070 0.03506       0.4427        0.581\n  310     85       2   0.4950 0.03523       0.4306        0.569\n  320     82       1   0.4890 0.03532       0.4244        0.563\n  329     81       1   0.4830 0.03539       0.4183        0.558\n  337     79       1   0.4768 0.03547       0.4121        0.552\n  340     78       1   0.4707 0.03554       0.4060        0.546\n  345     77       1   0.4646 0.03560       0.3998        0.540\n  348     76       1   0.4585 0.03565       0.3937        0.534\n  350     75       1   0.4524 0.03569       0.3876        0.528\n  351     74       1   0.4463 0.03573       0.3815        0.522\n  353     73       2   0.4340 0.03578       0.3693        0.510\n  361     70       1   0.4278 0.03581       0.3631        0.504\n  363     69       2   0.4154 0.03583       0.3508        0.492\n  364     67       1   0.4092 0.03582       0.3447        0.486\n  371     65       2   0.3966 0.03581       0.3323        0.473\n  387     60       1   0.3900 0.03582       0.3258        0.467\n  390     59       1   0.3834 0.03582       0.3193        0.460\n  394     58       1   0.3768 0.03580       0.3128        0.454\n  426     55       1   0.3700 0.03580       0.3060        0.447\n  428     54       1   0.3631 0.03579       0.2993        0.440\n  429     53       1   0.3563 0.03576       0.2926        0.434\n  433     52       1   0.3494 0.03573       0.2860        0.427\n  442     51       1   0.3426 0.03568       0.2793        0.420\n  444     50       1   0.3357 0.03561       0.2727        0.413\n  450     48       1   0.3287 0.03555       0.2659        0.406\n  455     47       1   0.3217 0.03548       0.2592        0.399\n  457     46       1   0.3147 0.03539       0.2525        0.392\n  460     44       1   0.3076 0.03530       0.2456        0.385\n  473     43       1   0.3004 0.03520       0.2388        0.378\n  477     42       1   0.2933 0.03508       0.2320        0.371\n  519     39       1   0.2857 0.03498       0.2248        0.363\n  520     38       1   0.2782 0.03485       0.2177        0.356\n  524     37       2   0.2632 0.03455       0.2035        0.340\n  533     34       1   0.2554 0.03439       0.1962        0.333\n  550     32       1   0.2475 0.03423       0.1887        0.325\n  558     30       1   0.2392 0.03407       0.1810        0.316\n  567     28       1   0.2307 0.03391       0.1729        0.308\n  574     27       1   0.2221 0.03371       0.1650        0.299\n  583     26       1   0.2136 0.03348       0.1571        0.290\n  613     24       1   0.2047 0.03325       0.1489        0.281\n  624     23       1   0.1958 0.03297       0.1407        0.272\n  641     22       1   0.1869 0.03265       0.1327        0.263\n  643     21       1   0.1780 0.03229       0.1247        0.254\n  654     20       1   0.1691 0.03188       0.1169        0.245\n  655     19       1   0.1602 0.03142       0.1091        0.235\n  687     18       1   0.1513 0.03090       0.1014        0.226\n  689     17       1   0.1424 0.03034       0.0938        0.216\n  705     16       1   0.1335 0.02972       0.0863        0.207\n  707     15       1   0.1246 0.02904       0.0789        0.197\n  728     14       1   0.1157 0.02830       0.0716        0.187\n  731     13       1   0.1068 0.02749       0.0645        0.177\n  735     12       1   0.0979 0.02660       0.0575        0.167\n  765     10       1   0.0881 0.02568       0.0498        0.156\n  791      9       1   0.0783 0.02462       0.0423        0.145\n  814      7       1   0.0671 0.02351       0.0338        0.133\n  883      4       1   0.0503 0.02285       0.0207        0.123\n\n\nThese tables show a row for each time point where either the event occured or a sample was censored. It shows the number at risk (number still remaining), and the cumulative survival at that instant.\nWhat’s more interesting though is if we model something besides just an intercept. Let’s fit survival curves separately by sex.\n\n\nCode\nsfit &lt;- survfit(Surv(time, status)~sex, data=lung)\nsfit\n\n\nCall: survfit(formula = Surv(time, status) ~ sex, data = lung)\n\n        n events median 0.95LCL 0.95UCL\nsex=1 138    112    270     212     310\nsex=2  90     53    426     348     550\n\n\nCode\nsummary(sfit)\n\n\nCall: survfit(formula = Surv(time, status) ~ sex, data = lung)\n\n                sex=1 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n   11    138       3   0.9783  0.0124       0.9542        1.000\n   12    135       1   0.9710  0.0143       0.9434        0.999\n   13    134       2   0.9565  0.0174       0.9231        0.991\n   15    132       1   0.9493  0.0187       0.9134        0.987\n   26    131       1   0.9420  0.0199       0.9038        0.982\n   30    130       1   0.9348  0.0210       0.8945        0.977\n   31    129       1   0.9275  0.0221       0.8853        0.972\n   53    128       2   0.9130  0.0240       0.8672        0.961\n   54    126       1   0.9058  0.0249       0.8583        0.956\n   59    125       1   0.8986  0.0257       0.8496        0.950\n   60    124       1   0.8913  0.0265       0.8409        0.945\n   65    123       2   0.8768  0.0280       0.8237        0.933\n   71    121       1   0.8696  0.0287       0.8152        0.928\n   81    120       1   0.8623  0.0293       0.8067        0.922\n   88    119       2   0.8478  0.0306       0.7900        0.910\n   92    117       1   0.8406  0.0312       0.7817        0.904\n   93    116       1   0.8333  0.0317       0.7734        0.898\n   95    115       1   0.8261  0.0323       0.7652        0.892\n  105    114       1   0.8188  0.0328       0.7570        0.886\n  107    113       1   0.8116  0.0333       0.7489        0.880\n  110    112       1   0.8043  0.0338       0.7408        0.873\n  116    111       1   0.7971  0.0342       0.7328        0.867\n  118    110       1   0.7899  0.0347       0.7247        0.861\n  131    109       1   0.7826  0.0351       0.7167        0.855\n  132    108       2   0.7681  0.0359       0.7008        0.842\n  135    106       1   0.7609  0.0363       0.6929        0.835\n  142    105       1   0.7536  0.0367       0.6851        0.829\n  144    104       1   0.7464  0.0370       0.6772        0.823\n  147    103       1   0.7391  0.0374       0.6694        0.816\n  156    102       2   0.7246  0.0380       0.6538        0.803\n  163    100       3   0.7029  0.0389       0.6306        0.783\n  166     97       1   0.6957  0.0392       0.6230        0.777\n  170     96       1   0.6884  0.0394       0.6153        0.770\n  175     94       1   0.6811  0.0397       0.6076        0.763\n  176     93       1   0.6738  0.0399       0.5999        0.757\n  177     92       1   0.6664  0.0402       0.5922        0.750\n  179     91       2   0.6518  0.0406       0.5769        0.736\n  180     89       1   0.6445  0.0408       0.5693        0.730\n  181     88       2   0.6298  0.0412       0.5541        0.716\n  183     86       1   0.6225  0.0413       0.5466        0.709\n  189     83       1   0.6150  0.0415       0.5388        0.702\n  197     80       1   0.6073  0.0417       0.5309        0.695\n  202     78       1   0.5995  0.0419       0.5228        0.687\n  207     77       1   0.5917  0.0420       0.5148        0.680\n  210     76       1   0.5839  0.0422       0.5068        0.673\n  212     75       1   0.5762  0.0424       0.4988        0.665\n  218     74       1   0.5684  0.0425       0.4909        0.658\n  222     72       1   0.5605  0.0426       0.4829        0.651\n  223     70       1   0.5525  0.0428       0.4747        0.643\n  229     67       1   0.5442  0.0429       0.4663        0.635\n  230     66       1   0.5360  0.0431       0.4579        0.627\n  239     64       1   0.5276  0.0432       0.4494        0.619\n  246     63       1   0.5192  0.0433       0.4409        0.611\n  267     61       1   0.5107  0.0434       0.4323        0.603\n  269     60       1   0.5022  0.0435       0.4238        0.595\n  270     59       1   0.4937  0.0436       0.4152        0.587\n  283     57       1   0.4850  0.0437       0.4065        0.579\n  284     56       1   0.4764  0.0438       0.3979        0.570\n  285     54       1   0.4676  0.0438       0.3891        0.562\n  286     53       1   0.4587  0.0439       0.3803        0.553\n  288     52       1   0.4499  0.0439       0.3716        0.545\n  291     51       1   0.4411  0.0439       0.3629        0.536\n  301     48       1   0.4319  0.0440       0.3538        0.527\n  303     46       1   0.4225  0.0440       0.3445        0.518\n  306     44       1   0.4129  0.0440       0.3350        0.509\n  310     43       1   0.4033  0.0441       0.3256        0.500\n  320     42       1   0.3937  0.0440       0.3162        0.490\n  329     41       1   0.3841  0.0440       0.3069        0.481\n  337     40       1   0.3745  0.0439       0.2976        0.471\n  353     39       2   0.3553  0.0437       0.2791        0.452\n  363     37       1   0.3457  0.0436       0.2700        0.443\n  364     36       1   0.3361  0.0434       0.2609        0.433\n  371     35       1   0.3265  0.0432       0.2519        0.423\n  387     34       1   0.3169  0.0430       0.2429        0.413\n  390     33       1   0.3073  0.0428       0.2339        0.404\n  394     32       1   0.2977  0.0425       0.2250        0.394\n  428     29       1   0.2874  0.0423       0.2155        0.383\n  429     28       1   0.2771  0.0420       0.2060        0.373\n  442     27       1   0.2669  0.0417       0.1965        0.362\n  455     25       1   0.2562  0.0413       0.1868        0.351\n  457     24       1   0.2455  0.0410       0.1770        0.341\n  460     22       1   0.2344  0.0406       0.1669        0.329\n  477     21       1   0.2232  0.0402       0.1569        0.318\n  519     20       1   0.2121  0.0397       0.1469        0.306\n  524     19       1   0.2009  0.0391       0.1371        0.294\n  533     18       1   0.1897  0.0385       0.1275        0.282\n  558     17       1   0.1786  0.0378       0.1179        0.270\n  567     16       1   0.1674  0.0371       0.1085        0.258\n  574     15       1   0.1562  0.0362       0.0992        0.246\n  583     14       1   0.1451  0.0353       0.0900        0.234\n  613     13       1   0.1339  0.0343       0.0810        0.221\n  624     12       1   0.1228  0.0332       0.0722        0.209\n  643     11       1   0.1116  0.0320       0.0636        0.196\n  655     10       1   0.1004  0.0307       0.0552        0.183\n  689      9       1   0.0893  0.0293       0.0470        0.170\n  707      8       1   0.0781  0.0276       0.0390        0.156\n  791      7       1   0.0670  0.0259       0.0314        0.143\n  814      5       1   0.0536  0.0239       0.0223        0.128\n  883      3       1   0.0357  0.0216       0.0109        0.117\n\n                sex=2 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    5     90       1   0.9889  0.0110       0.9675        1.000\n   60     89       1   0.9778  0.0155       0.9478        1.000\n   61     88       1   0.9667  0.0189       0.9303        1.000\n   62     87       1   0.9556  0.0217       0.9139        0.999\n   79     86       1   0.9444  0.0241       0.8983        0.993\n   81     85       1   0.9333  0.0263       0.8832        0.986\n   95     83       1   0.9221  0.0283       0.8683        0.979\n  107     81       1   0.9107  0.0301       0.8535        0.972\n  122     80       1   0.8993  0.0318       0.8390        0.964\n  145     79       2   0.8766  0.0349       0.8108        0.948\n  153     77       1   0.8652  0.0362       0.7970        0.939\n  166     76       1   0.8538  0.0375       0.7834        0.931\n  167     75       1   0.8424  0.0387       0.7699        0.922\n  182     71       1   0.8305  0.0399       0.7559        0.913\n  186     70       1   0.8187  0.0411       0.7420        0.903\n  194     68       1   0.8066  0.0422       0.7280        0.894\n  199     67       1   0.7946  0.0432       0.7142        0.884\n  201     66       2   0.7705  0.0452       0.6869        0.864\n  208     62       1   0.7581  0.0461       0.6729        0.854\n  226     59       1   0.7452  0.0471       0.6584        0.843\n  239     57       1   0.7322  0.0480       0.6438        0.833\n  245     54       1   0.7186  0.0490       0.6287        0.821\n  268     51       1   0.7045  0.0501       0.6129        0.810\n  285     47       1   0.6895  0.0512       0.5962        0.798\n  293     45       1   0.6742  0.0523       0.5791        0.785\n  305     43       1   0.6585  0.0534       0.5618        0.772\n  310     42       1   0.6428  0.0544       0.5447        0.759\n  340     39       1   0.6264  0.0554       0.5267        0.745\n  345     38       1   0.6099  0.0563       0.5089        0.731\n  348     37       1   0.5934  0.0572       0.4913        0.717\n  350     36       1   0.5769  0.0579       0.4739        0.702\n  351     35       1   0.5604  0.0586       0.4566        0.688\n  361     33       1   0.5434  0.0592       0.4390        0.673\n  363     32       1   0.5265  0.0597       0.4215        0.658\n  371     30       1   0.5089  0.0603       0.4035        0.642\n  426     26       1   0.4893  0.0610       0.3832        0.625\n  433     25       1   0.4698  0.0617       0.3632        0.608\n  444     24       1   0.4502  0.0621       0.3435        0.590\n  450     23       1   0.4306  0.0624       0.3241        0.572\n  473     22       1   0.4110  0.0626       0.3050        0.554\n  520     19       1   0.3894  0.0629       0.2837        0.534\n  524     18       1   0.3678  0.0630       0.2628        0.515\n  550     15       1   0.3433  0.0634       0.2390        0.493\n  641     11       1   0.3121  0.0649       0.2076        0.469\n  654     10       1   0.2808  0.0655       0.1778        0.443\n  687      9       1   0.2496  0.0652       0.1496        0.417\n  705      8       1   0.2184  0.0641       0.1229        0.388\n  728      7       1   0.1872  0.0621       0.0978        0.359\n  731      6       1   0.1560  0.0590       0.0743        0.328\n  735      5       1   0.1248  0.0549       0.0527        0.295\n  765      3       1   0.0832  0.0499       0.0257        0.270\n\n\nNow, check out the help for ?summary.survfit. You can give the summary() function an option for what times you want to show in the results. Look at the range of followup times in the lung dataset with range(). You can create a sequence of numbers going from one number to another number by increments of yet another number with the seq() function.\n\n\nCode\n# ?summary.survfit\nrange(lung$time)\n\n\n[1]    5 1022\n\n\nCode\nseq(0, 1100, 100)\n\n\n [1]    0  100  200  300  400  500  600  700  800  900 1000 1100\n\n\nAnd we can use that sequence vector with a summary call on sfit to get life tables at those intervals separately for both males (1) and females (2). From these tables we can start to see that males tend to have worse survival than females.\n\n\nCode\nsummary(sfit, times=seq(0, 1000, 100))\n\n\nCall: survfit(formula = Surv(time, status) ~ sex, data = lung)\n\n                sex=1 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0    138       0   1.0000  0.0000       1.0000        1.000\n  100    114      24   0.8261  0.0323       0.7652        0.892\n  200     78      30   0.6073  0.0417       0.5309        0.695\n  300     49      20   0.4411  0.0439       0.3629        0.536\n  400     31      15   0.2977  0.0425       0.2250        0.394\n  500     20       7   0.2232  0.0402       0.1569        0.318\n  600     13       7   0.1451  0.0353       0.0900        0.234\n  700      8       5   0.0893  0.0293       0.0470        0.170\n  800      6       2   0.0670  0.0259       0.0314        0.143\n  900      2       2   0.0357  0.0216       0.0109        0.117\n 1000      2       0   0.0357  0.0216       0.0109        0.117\n\n                sex=2 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     90       0   1.0000  0.0000       1.0000        1.000\n  100     82       7   0.9221  0.0283       0.8683        0.979\n  200     66      11   0.7946  0.0432       0.7142        0.884\n  300     43       9   0.6742  0.0523       0.5791        0.785\n  400     26      10   0.5089  0.0603       0.4035        0.642\n  500     21       5   0.4110  0.0626       0.3050        0.554\n  600     11       3   0.3433  0.0634       0.2390        0.493\n  700      8       3   0.2496  0.0652       0.1496        0.417\n  800      2       5   0.0832  0.0499       0.0257        0.270\n  900      1       0   0.0832  0.0499       0.0257        0.270\n\n\n\n\n\nNow that we’ve fit a survival curve to the data it’s pretty easy to visualize it with a Kaplan-Meier plot. Create the survival object if you don’t have it yet, and instead of using summary(), use plot() instead.\n\n\nCode\nsfit &lt;- survfit(Surv(time, status)~sex, data=lung)\nplot(sfit)\n\n\n\n\n\n\n\n\n\nThere are lots of ways to modify the plot produced by base R’s plot() function. You can see more options with the help for ?plot.survfit. We’re not going to go into any more detail here, because there’s another package called survminer that provides a function called ggsurvplot() that makes it much easier to produce publication-ready survival plots, and if you’re familiar with ggplot2 syntax it’s pretty easy to modify. So, let’s load the package and try it out.\n\n\nCode\nlibrary(survminer)\nggsurvplot(sfit)\n\n\n\n\n\n\n\n\n\nThis plot is substantially more informative by default, just because it automatically color codes the different groups, adds axis labels, and creates and automatic legend. But there’s a lot more you can do pretty easily here. Let’s add confidence intervals, show the p-value for the log-rank test, show a risk table below the plot, and change the colors and the group labels.\n\n\nCode\nggsurvplot(sfit, conf.int=TRUE, pval=TRUE, risk.table=TRUE, \n           legend.labs=c(\"Male\", \"Female\"), legend.title=\"Sex\",  \n           palette=c(\"dodgerblue2\", \"orchid2\"), \n           title=\"Kaplan-Meier Curve for Lung Cancer Survival\", \n           risk.table.height=.15)"
  }
]